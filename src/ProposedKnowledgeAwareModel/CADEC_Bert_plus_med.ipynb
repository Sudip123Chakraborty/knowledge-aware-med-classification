{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CADEC Ablation best.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqx9cKKKkqBr",
        "outputId": "2fbf2d1a-5d7d-4c80-bf58-c73951dcb145",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers.git\n",
        "%cd transformers/\n",
        "!git checkout 896a0eb1fd861bc37097a9b669ebf4cb8d523de7\n",
        "%cd ..\n",
        "!git clone https://github.com/joyousprakhar/ICHI-dataset.git\n",
        "!cp -a ICHI-dataset/. transformers/\n",
        "%cd transformers/\n",
        "!mkdir examples/cadec/\n",
        "!pip install -r requirements.txt\n",
        "!pip install .\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 8, done.\u001b[K\n",
            "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 47193 (delta 0), reused 2 (delta 0), pack-reused 47185\u001b[K\n",
            "Receiving objects: 100% (47193/47193), 33.80 MiB | 27.67 MiB/s, done.\n",
            "Resolving deltas: 100% (32817/32817), done.\n",
            "/content/transformers\n",
            "Note: checking out '896a0eb1fd861bc37097a9b669ebf4cb8d523de7'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n",
            "HEAD is now at 896a0eb1 Merge pull request #2459 from Perseus14/patch-4\n",
            "/content\n",
            "Cloning into 'ICHI-dataset'...\n",
            "remote: Enumerating objects: 54, done.\u001b[K\n",
            "remote: Counting objects: 100% (54/54), done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 54 (delta 21), reused 42 (delta 17), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (54/54), done.\n",
            "/content/transformers\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (1.18.5)\n",
            "Collecting tokenizers==0.0.11\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/36/7af38d572c935f8e0462ec7b4f7a46d73a2b3b1a938f50a5e8132d5b2dc5/tokenizers-0.0.11-cp36-cp36m-manylinux1_x86_64.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 4.6MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2f/08/f1ff665147a5d75b871bbe5ba76916f6490419c52a33e588385c4b69281b/boto3-1.15.18-py2.py3-none-any.whl (129kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 36.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 6)) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 7)) (2019.12.20)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 41.6MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 51.1MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.3MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting botocore<1.19.0,>=1.18.18\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2d/72/984ac8f33b5c8df5ff63f323a8724f65b4d0f8956968b942b77d35d3a1ef/botocore-1.18.18-py2.py3-none-any.whl (6.7MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7MB 51.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->-r requirements.txt (line 5)) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->-r requirements.txt (line 5)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->-r requirements.txt (line 5)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->-r requirements.txt (line 5)) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->-r requirements.txt (line 9)) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->-r requirements.txt (line 9)) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->-r requirements.txt (line 9)) (0.16.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.19.0,>=1.18.18->boto3->-r requirements.txt (line 3)) (2.8.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=3413c38d850d4f5b147c53fe4a39196e707dbe0fa48521024eaf19324b391f24\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, jmespath, botocore, s3transfer, boto3, sentencepiece, sacremoses\n",
            "Successfully installed boto3-1.15.18 botocore-1.18.18 jmespath-0.10.0 s3transfer-0.3.3 sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.0.11\n",
            "Processing /content/transformers\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0) (1.18.5)\n",
            "Requirement already satisfied: tokenizers==0.0.11 in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0) (0.0.11)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0) (1.15.18)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0) (0.1.91)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0) (0.0.43)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.19.0,>=1.18.18 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0) (1.18.18)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.3.0) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.3.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.3.0) (7.1.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.19.0,>=1.18.18->boto3->transformers==2.3.0) (2.8.1)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-2.3.0-cp36-none-any.whl size=451157 sha256=9366d54e252f57b528ff3578d50a2362d9907c3f67e01bbc46bfd33d1a00c905\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-6eq40cnw/wheels/23/19/dd/2561a4e47240cf6b307729d58e56f8077dd0c698f5992216cf\n",
            "Successfully built transformers\n",
            "Installing collected packages: transformers\n",
            "Successfully installed transformers-2.3.0\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-D79M5NpS8vh"
      },
      "source": [
        "# ########### Uncomment it to download Glove Vectors\n",
        "\n",
        "# !curl -O -J -L http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
        "# !unzip glove.840B.300d.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-EzqBB_PvmT"
      },
      "source": [
        "# ########### Uncomment it to download BioASQ Embeddings\n",
        "\n",
        "# !curl -O -J -L http://bioasq.lip6.fr/tools/BioASQword2vec/\n",
        "# !tar -xvzf biomedicalWordVectors.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SL0KtyeJkv6Z"
      },
      "source": [
        "# class ICHIDataset(Dataset):\n",
        "#     def __init__(self, fname, tokenizer):\n",
        "#         fin = open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
        "#         lines = fin.readlines()\n",
        "#         fin.close()\n",
        "\n",
        "#         all_data = []\n",
        "#         for i in range(0, len(lines), 3):\n",
        "#             text_left, _, text_right = [s.lower().strip() for s in lines[i].partition(\"$T$\")]\n",
        "#             aspect = lines[i + 1].lower().strip()\n",
        "#             polarity = lines[i + 2].strip()\n",
        "\n",
        "#             text_raw_indices = tokenizer.text_to_sequence(text_left + \" \" + aspect + \" \" + text_right)\n",
        "#             text_raw_without_aspect_indices = tokenizer.text_to_sequence(text_left + \" \" + text_right)\n",
        "#             text_left_indices = tokenizer.text_to_sequence(text_left)\n",
        "#             text_left_with_aspect_indices = tokenizer.text_to_sequence(text_left + \" \" + aspect)\n",
        "#             text_right_indices = tokenizer.text_to_sequence(text_right, reverse=True)\n",
        "#             text_right_with_aspect_indices = tokenizer.text_to_sequence(\" \" + aspect + \" \" + text_right, reverse=True)\n",
        "#             aspect_indices = tokenizer.text_to_sequence(aspect)\n",
        "#             left_context_len = np.sum(text_left_indices != 0)\n",
        "#             aspect_len = np.sum(aspect_indices != 0)\n",
        "#             aspect_in_text = torch.tensor([left_context_len.item(), (left_context_len + aspect_len - 1).item()])\n",
        "#             polarity = int(polarity) + 1\n",
        "\n",
        "#             text_bert_indices = tokenizer.text_to_sequence('[CLS] ' + text_left + \" \" + aspect + \" \" + text_right + ' [SEP] ' + aspect + \" [SEP]\")\n",
        "#             bert_segments_ids = np.asarray([0] * (np.sum(text_raw_indices != 0) + 2) + [1] * (aspect_len + 1))\n",
        "#             bert_segments_ids = pad_and_truncate(bert_segments_ids, tokenizer.max_seq_len)\n",
        "\n",
        "#             text_raw_bert_indices = tokenizer.text_to_sequence(\"[CLS] \" + text_left + \" \" + aspect + \" \" + text_right + \" [SEP]\")\n",
        "#             aspect_bert_indices = tokenizer.text_to_sequence(\"[CLS] \" + aspect + \" [SEP]\")\n",
        "\n",
        "#             data = {\n",
        "#                 'text_bert_indices': text_bert_indices,\n",
        "#                 'bert_segments_ids': bert_segments_ids,\n",
        "#                 'text_raw_bert_indices': text_raw_bert_indices,\n",
        "#                 'aspect_bert_indices': aspect_bert_indices,\n",
        "#                 'text_raw_indices': text_raw_indices,\n",
        "#                 'text_raw_without_aspect_indices': text_raw_without_aspect_indices,\n",
        "#                 'text_left_indices': text_left_indices,\n",
        "#                 'text_left_with_aspect_indices': text_left_with_aspect_indices,\n",
        "#                 'text_right_indices': text_right_indices,\n",
        "#                 'text_right_with_aspect_indices': text_right_with_aspect_indices,\n",
        "#                 'aspect_indices': aspect_indices,\n",
        "#                 'aspect_in_text': aspect_in_text,\n",
        "#                 'polarity': polarity,\n",
        "#             }\n",
        "\n",
        "#             all_data.append(data)\n",
        "#         self.data = all_data\n",
        "\n",
        "#     def __getitem__(self, index):\n",
        "#         return self.data[index]\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cG8YjUsTm2Eg",
        "outputId": "7a14b5d3-e7ef-4f6d-b4b7-33137bb28eb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/transformers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGkNpSjDrXjh",
        "outputId": "145c5b4e-b3a6-429a-91fc-731069ca3dee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile /content/transformers/examples/cadec/utils_ichi.py\n",
        "import logging\n",
        "import os\n",
        "import torch\n",
        "import random\n",
        "import re\n",
        "import csv\n",
        "import pickle\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
        "from sklearn.metrics import matthews_corrcoef, f1_score\n",
        "from transformers.file_utils import is_tf_available \n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "if is_tf_available():\n",
        "    import tensorflow as tf\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def simple_accuracy(preds, labels):\n",
        "    return (preds == labels).mean()\n",
        "\n",
        "def acc_and_f1(preds, labels):\n",
        "    acc_0 = simple_accuracy(preds[:,0], labels[:,0])\n",
        "    f1_0 = f1_score(y_true=labels[:,0], y_pred=preds[:,0], average='macro')\n",
        "    acc_1 = simple_accuracy(preds[:,1], labels[:,1])\n",
        "    f1_1 = f1_score(y_true=labels[:,1], y_pred=preds[:,1], average='macro')\n",
        "    acc_2 = simple_accuracy(preds[:,2], labels[:,2])\n",
        "    f1_2 = f1_score(y_true=labels[:,2], y_pred=preds[:,2], average='macro')\n",
        "    acc_3 = simple_accuracy(preds[:,3], labels[:,3])\n",
        "    f1_3 = f1_score(y_true=labels[:,3], y_pred=preds[:,3], average='macro')\n",
        "    acc_4 = simple_accuracy(preds[:,4], labels[:,4])\n",
        "    f1_4 = f1_score(y_true=labels[:,4], y_pred=preds[:,4], average='macro')\n",
        "    return {\n",
        "        \"acc_0\": acc_0,\n",
        "        \"f1_0\": f1_0,\n",
        "        \"acc_1\": acc_1,\n",
        "        \"f1_1\": f1_1,\n",
        "        \"acc_2\": acc_2,\n",
        "        \"f1_2\": f1_2,\n",
        "        \"acc_3\": acc_3,\n",
        "        \"f1_3\": f1_3,\n",
        "        \"acc_4\": acc_4,\n",
        "        \"f1_4\": f1_4,\n",
        "    }\n",
        "\n",
        "def compute_metrics(preds, labels):\n",
        "    # return {\"acc\": simple_accuracy(preds, labels)}\n",
        "    return acc_and_f1(preds, labels)\n",
        "\n",
        "\n",
        "def pad_and_truncate(sequence, maxlen, dtype='int64', padding='pre', truncating='post', value=0):\n",
        "    x = (np.ones(maxlen) * value).astype(dtype)\n",
        "    if truncating == 'pre':\n",
        "        trunc = sequence[-maxlen:]\n",
        "    else:\n",
        "        trunc = sequence[:maxlen]\n",
        "    trunc = np.asarray(trunc, dtype=dtype)\n",
        "    if padding == 'post':\n",
        "        x[:len(trunc)] = trunc\n",
        "    else:\n",
        "        x[-len(trunc):] = trunc\n",
        "    return x\n",
        "\n",
        "class Tokenizer(object):\n",
        "    def __init__(self, max_seq_len, max_num_words=None,lower=True):\n",
        "        self.lower = lower\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.word2idx = {}\n",
        "        self.word_freq = {}\n",
        "        self.idx2word = {}\n",
        "        self.idx2word[0] = '<PAD>'\n",
        "        self.word2idx['<PAD>'] = 0\n",
        "        self.word_freq['<PAD>'] = 100000\n",
        "        self.idx = 1\n",
        "        self.max_num_words = max_num_words\n",
        "\n",
        "    def fit_on_text(self, text):\n",
        "        if self.lower:\n",
        "            text = text.lower()\n",
        "        words = text.split()\n",
        "        for word in words:\n",
        "            if word not in self.word2idx:\n",
        "                self.word2idx[word] = self.idx\n",
        "                self.word_freq[word] = 1\n",
        "                self.idx2word[self.idx] = word\n",
        "                self.idx += 1\n",
        "            else:\n",
        "                self.word_freq[word] = self.word_freq[word] + 1\n",
        "    \n",
        "    def update_tokenizer(self):\n",
        "        if self.max_num_words == None:\n",
        "            return\n",
        "        elif self.max_num_words >= self.idx:\n",
        "            return \n",
        "        else:\n",
        "            del self.word_freq['<PAD>']\n",
        "            self.word_freq = {k: v for k, v in sorted(self.word_freq.items(), key=lambda item: item[1], reverse=True)}\n",
        "            self.word2idx = {}\n",
        "            self.idx2word = {}\n",
        "            self.idx2word[0] = '<PAD>'\n",
        "            self.word2idx['<PAD>'] = 0\n",
        "            self.idx = 1\n",
        "            for i, key in enumerate(self.word_freq):\n",
        "                if i >= self.max_num_words:\n",
        "                    break\n",
        "                else:\n",
        "                    self.word2idx[key] = i+1\n",
        "                    self.idx2word[i+1] = key\n",
        "                    self.idx += 1\n",
        "            self.word_freq['<PAD>'] = 100000\n",
        "\n",
        "\n",
        "\n",
        "    def fit_on_examples(self, examples):\n",
        "        is_tf_dataset = False\n",
        "        if is_tf_available() and isinstance(examples, tf.data.Dataset):\n",
        "            is_tf_dataset = True\n",
        "        processor = CADECProcessor()\n",
        "        for example in examples:\n",
        "            if is_tf_dataset:\n",
        "                example = processor.get_example_from_tensor_dict(example)\n",
        "                example = processor.tfds_map(example)\n",
        "            self.fit_on_text(example.clean_text)\n",
        "\n",
        "    def text_to_sequence(self, text, reverse=False, padding='pre', truncating='post'):\n",
        "        if self.lower:\n",
        "            text = text.lower()\n",
        "        words = text.split()\n",
        "        unknownidx = len(self.word2idx)+1\n",
        "        if (self.max_num_words == None) or (self.max_num_words >= self.idx):\n",
        "            sequence = [self.word2idx[w] if w in self.word2idx else unknownidx for w in words]\n",
        "        else:\n",
        "            sequence = []\n",
        "            for w in words:\n",
        "                if w in self.word2idx:\n",
        "                    if self.word2idx[w] > self.max_num_words:\n",
        "                        sequence.append(unknownidx)\n",
        "                    else:\n",
        "                        sequence.append(self.word2idx[w])\n",
        "                else:\n",
        "                    sequence.append(unknownidx)\n",
        "        if len(sequence) == 0:\n",
        "            sequence = [0]\n",
        "        if reverse:\n",
        "            sequence = sequence[::-1]\n",
        "        return pad_and_truncate(sequence, self.max_seq_len, padding=padding, truncating=truncating)\n",
        "\n",
        "def _load_word_vec_glove(path, word2idx=None):\n",
        "    fin = open(path, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
        "    word_vec = {}\n",
        "    for line in fin:\n",
        "        tokens = line.rstrip().split()\n",
        "        if word2idx is None or tokens[0] in word2idx.keys():\n",
        "            try:\n",
        "                word_vec[tokens[0]] = np.asarray(tokens[1:], dtype='float32')\n",
        "            except:\n",
        "                pass\n",
        "    return word_vec\n",
        "    \n",
        "def build_embedding_matrix_glove(word2idx, embed_dim, dat_fname, fname):\n",
        "    if os.path.exists(dat_fname):\n",
        "        print('loading embedding_matrix(glove): ', dat_fname)\n",
        "        embedding_matrix = pickle.load(open(dat_fname, 'rb'))\n",
        "    else:\n",
        "        print('loading word vectors...(glove)')\n",
        "        embedding_matrix = np.zeros((len(word2idx) + 2, embed_dim))  # idx 0 and len(word2idx)+1 are all-zeros\n",
        "        fname = fname + '/glove.twitter.27B/glove.twitter.27B.' + str(embed_dim) + 'd.txt' \\\n",
        "            if embed_dim != 300 else fname + '/glove.840B.300d.txt'\n",
        "        word_vec = _load_word_vec_glove(fname, word2idx=word2idx)\n",
        "        print('building embedding_matrix:', dat_fname)\n",
        "        for word, i in word2idx.items():\n",
        "            vec = word_vec.get(word)\n",
        "            if vec is not None:\n",
        "                # words not found in embedding index will be all-zeros.\n",
        "                embedding_matrix[i] = vec\n",
        "        pickle.dump(embedding_matrix, open(dat_fname, 'wb'))\n",
        "    return embedding_matrix\n",
        "\n",
        "def build_embedding_matrix_BioASQ(word2idx, embed_dim, dat_fname, fname):\n",
        "    if os.path.exists(dat_fname):\n",
        "        print('loading embedding_matrix(BioASQ):', dat_fname)\n",
        "        embedding_matrix = pickle.load(open(dat_fname, 'rb'))\n",
        "    else:\n",
        "        print('loading word vectors...(BioASQ)')\n",
        "        embedding_matrix = np.zeros((len(word2idx) + 2, embed_dim))  # idx 0 and len(word2idx)+1 are all-zeros\n",
        "        f = open(fname + \"/word2vecTools/types.txt\",\"r\")\n",
        "        i = 0\n",
        "        names = []\n",
        "        for line in f:\n",
        "            names.append(line.split('\\n')[0])\n",
        "            i = i + 1\n",
        "        vectors = np.loadtxt(fname + \"/word2vecTools/vectors.txt\")\n",
        "        word_vec = {}\n",
        "        for (index, name) in enumerate(names):\n",
        "            word_vec[name] = index\n",
        "\n",
        "        print('building embedding_matrix:', dat_fname)\n",
        "        for word, i in word2idx.items():\n",
        "            if word in word_vec.keys():\n",
        "                vec = vectors[word_vec[word]]\n",
        "            else:\n",
        "                vec = None\n",
        "            if vec is not None:\n",
        "                # words not found in embedding index will be all-zeros.\n",
        "                embedding_matrix[i] = vec\n",
        "        pickle.dump(embedding_matrix, open(dat_fname, 'wb'))\n",
        "    return embedding_matrix\n",
        "\n",
        "def convert_examples_to_features(\n",
        "    examples,\n",
        "    tokenizer,\n",
        "    tokenizer_cleantext,\n",
        "    max_length=512,\n",
        "    task=None,\n",
        "    label_list=None,\n",
        "    output_mode=None,\n",
        "    pad_on_left=False,\n",
        "    pad_token=0,\n",
        "    pad_token_segment_id=0,\n",
        "    mask_padding_with_zero=True,\n",
        "):\n",
        "    \"\"\"\n",
        "    Loads a data file into a list of ``CADEC_InputFeatures``\n",
        "\n",
        "    Args:\n",
        "        examples: List of ``CADEC_InputExamples`` or ``tf.data.Dataset`` containing the examples.\n",
        "        tokenizer: Instance of a tokenizer that will tokenize the examples\n",
        "        tokenizer_cleantext: Instance of a tokenizer that will tokenize the examples clean text(used for our medical module) \n",
        "        max_length: Maximum example length\n",
        "        task: GLUE task\n",
        "        label_list: List of labels. Can be obtained from the processor using the ``processor.get_labels()`` method\n",
        "        output_mode: String indicating the output mode. Either ``regression`` or ``classification``\n",
        "        pad_on_left: If set to ``True``, the examples will be padded on the left rather than on the right (default)\n",
        "        pad_token: Padding token\n",
        "        pad_token_segment_id: The segment ID for the padding token (It is usually 0, but can vary such as for XLNet where it is 4)\n",
        "        mask_padding_with_zero: If set to ``True``, the attention mask will be filled by ``1`` for actual values\n",
        "            and by ``0`` for padded values. If set to ``False``, inverts it (``1`` for padded values, ``0`` for\n",
        "            actual values)\n",
        "\n",
        "    Returns:\n",
        "        If the ``examples`` input is a ``tf.data.Dataset``, will return a ``tf.data.Dataset``\n",
        "        containing the task-specific features. If the input is a list of ``CADEC_InputExamples``, will return\n",
        "        a list of task-specific ``CADEC_InputFeatures`` which can be fed to the model.\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    is_tf_dataset = False\n",
        "    if is_tf_available() and isinstance(examples, tf.data.Dataset):\n",
        "        is_tf_dataset = True\n",
        "\n",
        "    \"\"\"      Initialisation of Data Processor    \"\"\"\n",
        "    if task is not None:\n",
        "        processor = CADECProcessor()  \n",
        "        if label_list is None:\n",
        "            label_list = processor.get_labels()\n",
        "            logger.info(\"Using label list %s for task %s\" % (label_list, task))\n",
        "        if output_mode is None:\n",
        "            output_mode = \"classification\"\n",
        "            logger.info(\"Using output mode %s for task %s\" % (output_mode, task))\n",
        "\n",
        "    label_map = {label: i for i, label in enumerate(label_list)}\n",
        "\n",
        "    \"\"\"      Processing the examples    \"\"\"\n",
        "    features = []\n",
        "    for (ex_index, example) in enumerate(examples):\n",
        "        if ex_index % 10000 == 0:\n",
        "            logger.info(\"Writing example %d/%d\" % (ex_index, len(examples)))\n",
        "        if is_tf_dataset:\n",
        "            example = processor.get_example_from_tensor_dict(example)\n",
        "            example = processor.tfds_map(example)\n",
        "        \"\"\"      Assuming that each aspect consist of max 4 tokens hence making sure that aspects total tokens coming from aspects are not greater than max sequence length    \"\"\"\n",
        "        aspect_present = bool(example.aspects is not None)\n",
        "        if aspect_present:\n",
        "            if 4 * len(example.aspects) > max_length:\n",
        "                example.aspects = random.sample(example.aspects, k = int(max_length/4))\n",
        "                \n",
        "        \"\"\"      Using tokenizer.encode_plus_lcf the global features are encoded as [<special token> + text_tokens + <special token> + heading_tokens + <special token> + aspect1_token + <special token> + .... + aspectn_token + <special token>] \"\"\"\n",
        "        inputs_global = tokenizer.encode_plus_lcf(example.text, example.heading, example.aspects, add_special_tokens=True, max_length=max_length,) ######## edittttttt\n",
        "        \"\"\"      Using tokenizer.encode_plus_lcf the local features(medical module) are encoded as [<special token> + text_tokens + <special token>] \"\"\"\n",
        "        inputs_local = tokenizer.encode_plus_lcf(example.text, add_special_tokens=True, max_length=max_length,)\n",
        "        \"\"\" Generating tokens for the clean_text i.e. tokenising text based on glove or BioASQ \"\"\"\n",
        "        text_clean_indices = tokenizer_cleantext.text_to_sequence(example.clean_text)\n",
        "        \n",
        "        \"\"\"      Using tokenizer.encode_plus_lcf the aspect_indices are encoded as [<special token> + aspect1_token + <special token> + .... + aspectn_token + <special token>] \"\"\"\n",
        "        if len(example.aspects) > 2:\n",
        "            aspect_indices = tokenizer.encode_plus_lcf(example.aspects[0], example.aspects[1], example.aspects[2:], add_special_tokens=False, max_length=max_length,)\n",
        "        elif len(example.aspects) == 2:\n",
        "            aspect_indices = tokenizer.encode_plus_lcf(example.aspects[0], example.aspects[1], add_special_tokens=False, max_length=max_length,)\n",
        "        else:\n",
        "            aspect_indices = tokenizer.encode_plus_lcf(example.aspects[0], add_special_tokens=False, max_length=max_length,)\n",
        "        \n",
        "        input_global_ids, token_global_type_ids = inputs_global[\"input_ids\"], inputs_global[\"token_type_ids\"]\n",
        "        input_local_ids, token_local_type_ids = inputs_local[\"input_ids\"], inputs_local[\"token_type_ids\"]\n",
        "        \n",
        "        aspect_indices = aspect_indices[\"input_ids\"]\n",
        "        padding_length_aspect = max_length - len(aspect_indices)\n",
        "        aspect_indices = aspect_indices + ([pad_token] * padding_length_aspect)\n",
        "\n",
        "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "        # tokens are attended to.\n",
        "        attention_mask_global = [1 if mask_padding_with_zero else 0] * len(input_global_ids)\n",
        "        attention_mask_local = [1 if mask_padding_with_zero else 0] * len(input_local_ids)\n",
        "\n",
        "        # Zero-pad up to the sequence length.\n",
        "        padding_length_global = max_length - len(input_global_ids)\n",
        "        padding_length_local = max_length - len(input_local_ids)\n",
        "        if pad_on_left:\n",
        "            input_global_ids = ([pad_token] * padding_length_global) + input_global_ids\n",
        "            attention_mask_global = ([0 if mask_padding_with_zero else 1] * padding_length_global) + attention_mask_global\n",
        "            token_global_type_ids = ([pad_token_segment_id] * padding_length_global) + token_global_type_ids\n",
        "            input_local_ids = ([pad_token] * padding_length_local) + input_local_ids\n",
        "            attention_mask_local = ([0 if mask_padding_with_zero else 1] * padding_length_local) + attention_mask_local\n",
        "            token_local_type_ids = ([pad_token_segment_id] * padding_length_local) + token_local_type_ids\n",
        "        else:\n",
        "            input_global_ids = input_global_ids + ([pad_token] * padding_length_global)\n",
        "            attention_mask_global = attention_mask_global + ([0 if mask_padding_with_zero else 1] * padding_length_global)\n",
        "            token_global_type_ids = token_global_type_ids + ([pad_token_segment_id] * padding_length_global)\n",
        "            input_local_ids = input_local_ids + ([pad_token] * padding_length_local)\n",
        "            attention_mask_local = attention_mask_local + ([0 if mask_padding_with_zero else 1] * padding_length_local)\n",
        "            token_local_type_ids = token_local_type_ids + ([pad_token_segment_id] * padding_length_local)\n",
        "\n",
        "        assert len(input_global_ids) == max_length, \"Error with input length {} vs {}\".format(len(input_global_ids), max_length)\n",
        "        assert len(input_local_ids) == max_length, \"Error with input length {} vs {}\".format(len(input_local_ids), max_length)\n",
        "        assert len(attention_mask_global) == max_length, \"Error with input length {} vs {}\".format(\n",
        "            len(attention_mask_global), max_length\n",
        "        )\n",
        "        assert len(attention_mask_local) == max_length, \"Error with input length {} vs {}\".format(\n",
        "            len(attention_mask_local), max_length\n",
        "        )\n",
        "        assert len(token_global_type_ids) == max_length, \"Error with input length {} vs {}\".format(\n",
        "            len(token_global_type_ids), max_length\n",
        "        )\n",
        "        assert len(token_local_type_ids) == max_length, \"Error with input length {} vs {}\".format(\n",
        "            len(token_local_type_ids), max_length\n",
        "        )\n",
        "\n",
        "        if output_mode == \"classification\":\n",
        "            label = example.label\n",
        "        elif output_mode == \"regression\":\n",
        "            label = float(example.label)\n",
        "        else:\n",
        "            raise KeyError(output_mode)\n",
        "\n",
        "        if ex_index < 5:\n",
        "            logger.info(\"*** Example ***\")\n",
        "            logger.info(\"guid: %s\" % (example.guid))\n",
        "            logger.info(\"input_global_ids: %s\" % \" \".join([str(x) for x in input_global_ids]))\n",
        "            logger.info(\"attention_mask_global: %s\" % \" \".join([str(x) for x in attention_mask_global]))\n",
        "            logger.info(\"token_global_type_ids: %s\" % \" \".join([str(x) for x in token_global_type_ids]))\n",
        "            logger.info(\"input_local_ids: %s\" % \" \".join([str(x) for x in input_local_ids]))\n",
        "            logger.info(\"attention_mask_local: %s\" % \" \".join([str(x) for x in attention_mask_local]))\n",
        "            logger.info(\"token_local_type_ids: %s\" % \" \".join([str(x) for x in token_local_type_ids]))\n",
        "            logger.info(\"text_clean_indices: %s\" % \" \".join([str(x) for x in text_clean_indices]))\n",
        "            logger.info(\"aspect_indices: %s\" % \" \".join([str(x) for x in aspect_indices]))\n",
        "            logger.info(\"label: %s (id = %s)\" % (str(example.label), str(label)))\n",
        "\n",
        "        features.append(\n",
        "            CADEC_InputFeatures(\n",
        "                input_global_ids=input_global_ids, input_local_ids=input_local_ids, attention_mask_global=attention_mask_global, attention_mask_local=attention_mask_local,\n",
        "                token_global_type_ids=token_global_type_ids, token_local_type_ids=token_local_type_ids, text_clean_indices=text_clean_indices, aspect_indices=aspect_indices, label=label\n",
        "            )\n",
        "        )\n",
        "\n",
        "    if is_tf_available() and is_tf_dataset:\n",
        "\n",
        "        def gen():\n",
        "            for ex in features:\n",
        "                yield (\n",
        "                    {\n",
        "                        \"input_global_ids\": ex.input_global_ids,\n",
        "                        \"input_local_ids\": ex.input_local_ids,\n",
        "                        \"attention_mask_global\": ex.attention_mask_global,\n",
        "                        \"attention_mask_local\": ex.attention_mask_local,\n",
        "                        \"token_global_type_ids\": ex.token_global_type_ids,\n",
        "                        \"token_local_type_ids\": ex.token_local_type_ids,\n",
        "                        \"text_clean_indices\": ex.text_clean_indices,\n",
        "                        \"aspect_indices\": ex.aspect_indices,\n",
        "                    },\n",
        "                    ex.label,\n",
        "                )\n",
        "\n",
        "        return tf.data.Dataset.from_generator(\n",
        "            gen,\n",
        "            ({\"input_global_ids\": tf.int32, \"input_local_ids\": tf.int32, \"attention_mask_global\": tf.int32, \"attention_mask_local\": tf.int32, \"token_global_type_ids\": tf.int32, \"token_local_type_ids\": tf.int32, \"text_clean_indices\": tf.int32, \"aspect_indices\": tf.int32}, tf.int64),\n",
        "            (\n",
        "                {\n",
        "                    \"input_global_ids\": tf.TensorShape([None]),\n",
        "                    \"input_local_ids\": tf.TensorShape([None]),\n",
        "                    \"attention_mask_global\": tf.TensorShape([None]),\n",
        "                    \"attention_mask_local\": tf.TensorShape([None]),\n",
        "                    \"token_global_type_ids\": tf.TensorShape([None]),\n",
        "                    \"token_local_type_ids\": tf.TensorShape([None]),\n",
        "                    \"text_clean_indices\": tf.TensorShape([None]),\n",
        "                    \"aspect_indices\": tf.TensorShape([None]),\n",
        "                },\n",
        "                tf.TensorShape([]),\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    return features\n",
        "\n",
        "class CADECProcessor(object):\n",
        "    \"\"\"Processor for the CADEC data set (GLUE version).\"\"\"\n",
        "\n",
        "    def get_example_from_tensor_dict(self, tensor_dict):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return CADEC_InputExample(tensor_dict['idx'].numpy(),\n",
        "                            tensor_dict['heading'].numpy().decode('utf-8'),\n",
        "                            tensor_dict['text'].numpy().decode('utf-8'),\n",
        "                            tensor_dict['clean_text'].numpy().decode('utf-8'),\n",
        "                            tensor_dict['aspects'].numpy().decode('utf-8'),\n",
        "                            tensor_dict['label'].numpy())\n",
        "\n",
        "    def get_train_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return self._create_examples(\n",
        "            self._read_tsv(os.path.join(data_dir, \"cadec_train.tsv\")), \"train\")\n",
        "\n",
        "    def get_dev_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return self._create_examples(\n",
        "            self._read_tsv(os.path.join(data_dir, \"cadec_dev.tsv\")),\n",
        "            \"dev\")\n",
        "\n",
        "    def get_labels(self):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return [\"Uncertainity of Post_Diagnosis\", \"Results and Side-Effects Observed\", \"Medical Assistance\", \"Diet and Maintenance\", \"Information Source\"]\n",
        "\n",
        "    def _create_examples(self, lines, set_type):\n",
        "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "        examples = []\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        for (i, line) in enumerate(lines):\n",
        "            if i == 0:\n",
        "                continue\n",
        "            guid = \"%s-%s\" % (set_type, i)\n",
        "            label = [int(line[3]), int(line[4]), int(line[5]), int(line[6]), int(line[7])]\n",
        "            heading = None\n",
        "            text = line[2]\n",
        "            try:\n",
        "                aspects = [' ' + x for x in line[8].split('|')]\n",
        "                temp_clean_text = \" \".join(line[8].split('|'))\n",
        "            except:\n",
        "                aspects = None\n",
        "                temp_clean_text = \"\"\n",
        "                print(\"Sed\")\n",
        "            \n",
        "            # print(aspects)\n",
        "            \"\"\"    IMPORTANT    IMPORTANT    IMPORTANT    IMPORTANT    IMPORTANT   \"\"\"\n",
        "            clean_text = self.clean_str( text, lemmatizer) ######## for using glove embeddings over sentence\n",
        "            # clean_text = self.clean_str( temp_clean_text, lemmatizer) ##### for using BioASQ embeddings in aspects\n",
        "            examples.append(\n",
        "                CADEC_InputExample(guid=guid, heading=heading, text=text, clean_text=clean_text, aspects=aspects, label=label))\n",
        "        return examples\n",
        "\n",
        "    def tfds_map(self, example):\n",
        "        \"\"\"Some tensorflow_datasets datasets are not formatted the same way the GLUE datasets are.\n",
        "        This method converts examples to the correct format.\"\"\"\n",
        "        # if len(self.get_labels()) > 1:\n",
        "        #     example.label = self.get_labels()[int(example.label)]\n",
        "        return example\n",
        "\n",
        "    def clean_str(self, string1, lemmatizer):\n",
        "        \"\"\"\n",
        "        Tokenization/string cleaning for dataset\n",
        "        Every dataset is lower cased except\n",
        "        \"\"\"\n",
        "        str_stop = \"\"\n",
        "        string1 = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '',string1)\n",
        "        string1 = re.sub(r\"\\\\\", \" \", string1)    \n",
        "        string1 = re.sub(r\"\\'\", \" \", string1)    \n",
        "        string1 = re.sub(r\"\\\"\", \" \", string1)   \n",
        "        string1 = re.sub(r'(\\W)\\1+', r'\\1', string1)\n",
        "        word_list=string1.split(\" \")\n",
        "        filtered_words = [word for word in word_list if word not in stopwords.words('english')]\n",
        "        for kj in filtered_words:\n",
        "            new=lemmatizer.lemmatize(str(kj)) \n",
        "            str_stop=str_stop +\" \"+new\n",
        "            str_stop.encode('utf-8')\n",
        "        return str_stop.strip().lower()\n",
        "\n",
        "    @classmethod\n",
        "    def _read_tsv(cls, input_file, quotechar=None):\n",
        "        \"\"\"Reads a tab separated value file.\"\"\"\n",
        "        with open(input_file, \"r\", encoding=\"utf-8-sig\") as f:\n",
        "            return list(csv.reader(f, delimiter=\"\\t\", quotechar=quotechar))\n",
        "\n",
        "class CADEC_InputExample(object):\n",
        "    \"\"\"\n",
        "    A single training/test example for simple sequence classification.\n",
        "\n",
        "    Args:\n",
        "        guid: Unique id for the example.\n",
        "        heading: string. The untokenized heading of the sequence\n",
        "        text: string. The untokenized text part of the sequence.\n",
        "        clean_text: string. The untokenized text part of the sequence used for medical module(as tokenization will be different for glove and BioASQ than BERT).\n",
        "        aspects: list of string. The untokenized aspects for the sequence as a list of strings\n",
        "        Only must be specified for sequence pair tasks.\n",
        "        label: (Optional) list. The label of the example. This should be\n",
        "        specified for train and dev examples, but not for test examples.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, guid, heading, text, clean_text, aspects, label=None):\n",
        "        self.guid = guid\n",
        "        self.heading = heading\n",
        "        self.text = text\n",
        "        self.clean_text = clean_text\n",
        "        self.aspects = aspects\n",
        "        self.label = label\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.to_json_string())\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
        "        output = copy.deepcopy(self.__dict__)\n",
        "        return output\n",
        "\n",
        "    def to_json_string(self):\n",
        "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
        "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n",
        "\n",
        "\n",
        "class CADEC_InputFeatures(object):\n",
        "    \"\"\"\n",
        "    A single set of features of data.\n",
        "\n",
        "    Args:\n",
        "        input_ids: Indices of input sequence tokens in the vocabulary.\n",
        "        attention_mask: Mask to avoid performing attention on padding token indices.\n",
        "            Mask values selected in ``[0, 1]``:\n",
        "            Usually  ``1`` for tokens that are NOT MASKED, ``0`` for MASKED (padded) tokens.\n",
        "        token_type_ids: Segment token indices to indicate first and second portions of the inputs.\n",
        "        label: Label corresponding to the input\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_global_ids, input_local_ids, attention_mask_global=None, attention_mask_local=None, token_global_type_ids=None, token_local_type_ids=None, text_clean_indices=None, aspect_indices=None, label=None):\n",
        "        self.input_global_ids = input_global_ids\n",
        "        self.attention_mask_global = attention_mask_global\n",
        "        self.token_global_type_ids = token_global_type_ids\n",
        "        self.input_local_ids = input_local_ids\n",
        "        self.attention_mask_local = attention_mask_local\n",
        "        self.token_local_type_ids = token_local_type_ids\n",
        "        self.text_clean_indices = text_clean_indices\n",
        "        self.aspect_indices = aspect_indices\n",
        "        self.label = label\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.to_json_string())\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
        "        output = copy.deepcopy(self.__dict__)\n",
        "        return output\n",
        "\n",
        "    def to_json_string(self):\n",
        "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
        "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n",
        "\n",
        "\n",
        "def load_and_cache_examples(args, task, tokenizer, tokenizer_cleantext, evaluate=False):\n",
        "    if args.local_rank not in [-1, 0] and not evaluate:\n",
        "        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
        "\n",
        "    processor = CADECProcessor()  \n",
        "    output_mode = \"classification\"\n",
        "    # Load data features from cache or dataset file\n",
        "    cached_features_file = os.path.join(\n",
        "        args.data_dir,\n",
        "        \"cached_{}_{}_{}_{}\".format(\n",
        "            \"dev\" if evaluate else \"train\",\n",
        "            list(filter(None, args.model_name_or_path.split(\"/\"))).pop(),\n",
        "            str(args.max_seq_length),\n",
        "            str(task),\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    \"\"\" Load saved tokenizer for the clean_text\"\"\"\n",
        "    cached_tokenizer_cleantext_file = os.path.join(\n",
        "        args.data_dir,\n",
        "        \"cachedtokenizer_{}_{}_{}\".format(\n",
        "            list(filter(None, args.model_name_or_path.split(\"/\"))).pop(),\n",
        "            str(args.embedding_type),\n",
        "            str(task),\n",
        "        ),\n",
        "    )\n",
        "    \"\"\" if overwrite cache is disabled and saved feature and tokenizer file exists then load from the saved file else process them \"\"\"\n",
        "    if os.path.exists(cached_features_file) and os.path.exists(cached_tokenizer_cleantext_file) and not args.overwrite_cache:\n",
        "        logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
        "        features = torch.load(cached_features_file)\n",
        "        print('loading tokenizer from cache:', cached_tokenizer_cleantext_file)\n",
        "        tokenizer_cleantext = pickle.load(open(cached_tokenizer_cleantext_file, 'rb'))\n",
        "    else:\n",
        "        logger.info(\"Creating features from dataset file at %s\", args.data_dir)\n",
        "        label_list = processor.get_labels()\n",
        "        if task in [\"mnli\", \"mnli-mm\"] and args.model_type in [\"roberta\", \"xlmroberta\"]:\n",
        "            # HACK(label indices are swapped in RoBERTa pretrained model)\n",
        "            label_list[1], label_list[2] = label_list[2], label_list[1]\n",
        "        examples = (\n",
        "            processor.get_dev_examples(args.data_dir) if evaluate else processor.get_train_examples(args.data_dir)\n",
        "        )\n",
        "        if (not evaluate):\n",
        "            tokenizer_cleantext.fit_on_examples(examples)\n",
        "            tokenizer_cleantext.update_tokenizer()\n",
        "        \"\"\" features are created using this function which is defined above\"\"\"\n",
        "        features = convert_examples_to_features(\n",
        "            examples,\n",
        "            tokenizer,\n",
        "            tokenizer_cleantext,\n",
        "            label_list=label_list,\n",
        "            max_length=args.max_seq_length,\n",
        "            output_mode=output_mode,\n",
        "            pad_on_left=bool(args.model_type in [\"xlnet\"]),  # pad on the left for xlnet\n",
        "            pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n",
        "            pad_token_segment_id=4 if args.model_type in [\"xlnet\"] else 0,\n",
        "        )\n",
        "        if args.local_rank in [-1, 0]:\n",
        "            logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
        "            torch.save(features, cached_features_file)\n",
        "            pickle.dump(tokenizer_cleantext, open(cached_tokenizer_cleantext_file, 'wb'))\n",
        "\n",
        "    if args.local_rank == 0 and not evaluate:\n",
        "        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
        "\n",
        "    # Convert to Tensors and build dataset\n",
        "    all_input_global_ids = torch.tensor([f.input_global_ids for f in features], dtype=torch.long)\n",
        "    all_input_local_ids = torch.tensor([f.input_local_ids for f in features], dtype=torch.long)\n",
        "    all_attention_mask_global = torch.tensor([f.attention_mask_global for f in features], dtype=torch.long)\n",
        "    all_attention_mask_local = torch.tensor([f.attention_mask_local for f in features], dtype=torch.long)\n",
        "    all_token_global_type_ids = torch.tensor([f.token_global_type_ids for f in features], dtype=torch.long)\n",
        "    all_token_local_type_ids = torch.tensor([f.token_local_type_ids for f in features], dtype=torch.long)\n",
        "    all_text_clean_indices = torch.tensor([f.text_clean_indices for f in features], dtype=torch.long)\n",
        "    all_aspect_indices = torch.tensor([f.aspect_indices for f in features], dtype=torch.long)\n",
        "    all_labels = torch.tensor([f.label for f in features], dtype=torch.float32)\n",
        "    \n",
        "    dataset = TensorDataset(all_input_global_ids, all_input_local_ids, all_attention_mask_global, all_attention_mask_local, all_token_global_type_ids, all_token_local_type_ids, all_text_clean_indices, all_aspect_indices, all_labels)\n",
        "    \n",
        "    return dataset, tokenizer_cleantext"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing /content/transformers/examples/cadec/utils_ichi.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyy-AEpoS8Lx",
        "outputId": "a2346304-3147-4832-e9f6-1ab5c861cad6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile /content/transformers/examples/cadec/lcf_ichi.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import copy\n",
        "import numpy as np\n",
        "from transformers.modeling_bert import BertPooler, BertSelfAttention, BertPreTrainedModel, BertModel\n",
        "from torch.nn import CrossEntropyLoss, MSELoss, BCEWithLogitsLoss\n",
        "\n",
        "def compute_average_with_padding(tensor, padding):\n",
        "    \"\"\"\n",
        "    :param tensor: dimension batch_size, seq_length, hidden_size\n",
        "    :param padding: dimension batch_size, seq_length\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    batch_size, seq_length, emb_size = tensor.shape\n",
        "    entry_sizes = torch.sum(padding, axis=1)\n",
        "    return torch.sum(tensor, axis=1) / entry_sizes\n",
        "\n",
        "\n",
        "class BertMaxPooler(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, hidden_states, mask):\n",
        "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
        "        # to the first token.\n",
        "        first_token_tensor = compute_average_with_padding(hidden_states, mask)\n",
        "        pooled_output = self.dense(first_token_tensor)\n",
        "        pooled_output = self.activation(pooled_output)\n",
        "        return pooled_output\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, config, args):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.args = args\n",
        "        self.SA = BertSelfAttention(config)\n",
        "        self.tanh = torch.nn.Tanh()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        zero_tensor = torch.tensor(np.zeros((inputs.size(0), 1, 1, self.args.max_seq_length),\n",
        "                                            dtype=np.float32), dtype=torch.float32).to(self.args.device)\n",
        "        SA_out = self.SA(inputs, zero_tensor)\n",
        "        return self.tanh(SA_out[0])\n",
        "\n",
        "\n",
        "class lcf_BERT(BertPreTrainedModel):\n",
        "    def __init__(self, config, args):\n",
        "        super(lcf_BERT, self).__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "        self.args = args\n",
        "        self.config =config\n",
        "        self.bert = BertModel(config)\n",
        "        # self.bert_global_focus = self.bert\n",
        "        # self.bert_local_focus = copy.deepcopy(self.bert_global_focus) if args.use_single_bert else self.bert_global_focus\n",
        "        # self.embedder = nn.Embedding.from_pretrained(torch.from_numpy(args.word2vec).float(), freeze=True, padding_idx=0)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        # self.lstm = nn.LSTM(args.emb_size, 100, 1, batch_first=True, bidirectional=True)\n",
        "        # Co = 100\n",
        "        # self.conv13 = nn.Conv2d(1, Co, (3, 2*args.emb_size))\n",
        "        # self.conv14 = nn.Conv2d(1, Co, (4, 2*args.emb_size))\n",
        "        # self.conv15 = nn.Conv2d(1, Co, (5, 2*args.emb_size))\n",
        "        # self.dropout_1 = nn.Dropout(0.4)\n",
        "        # self.fc1 = nn.Linear(3*Co, config.num_labels)\n",
        "        # self.fc = nn.Linear(200, config.num_labels)\n",
        "        self.bert_SA = SelfAttention(config, args) ### change\n",
        "        self.linear_double_cdm_or_cdw = nn.Linear(config.hidden_size * 2,config.hidden_size)\n",
        "        self.linear_triple_lcf_global = nn.Linear(config.hidden_size * 3, config.hidden_size)\n",
        "        self.bert_pooler_org = BertPooler(config)\n",
        "        self.bert_pooler = BertMaxPooler(config) ### change\n",
        "        self.dense = nn.Linear(2*config.hidden_size, config.num_labels)\n",
        "        self.init_weights()\n",
        "\n",
        "    def conv_and_pool(self, x, conv):\n",
        "        x = F.gelu(conv(x)).squeeze(3) # (n, Co, W)\n",
        "        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
        "        return x\n",
        "\n",
        "    def feature_dynamic_mask(self, text_local_indices, aspect_indices):\n",
        "        texts = text_local_indices\n",
        "        # mask_len = self.args.SRD\n",
        "        masked_text_raw_indices = torch.zeros((text_local_indices.size(0), self.args.max_seq_length, self.config.hidden_size),\n",
        "                                          dtype=torch.float)\n",
        "        \n",
        "        masked_text_raw_indices[:, 0, :] = torch.ones((text_local_indices.size(0), self.config.hidden_size), dtype=torch.float) \n",
        "        zero_tensor = torch.tensor(0).to(self.args.device)\n",
        "        for i in range(aspect_indices.shape[0]):\n",
        "            for j in range(aspect_indices[i].shape[0]):\n",
        "                if aspect_indices[i][j] == zero_tensor:\n",
        "                    break\n",
        "                else:\n",
        "                    indices = (text_local_indices[i] == aspect_indices[i][j]).nonzero()\n",
        "                    for k in indices:\n",
        "                        masked_text_raw_indices[i][k] = torch.ones(self.config.hidden_size, dtype=torch.float)\n",
        "        return masked_text_raw_indices.to(self.args.device)\n",
        "\n",
        "    # # create the weights tensor for local context features\n",
        "    # def feature_dynamic_weighted(self, text_local_indices, aspect_indices):\n",
        "    #     texts = text_local_indices\n",
        "    #     asps = aspect_indices\n",
        "    #     # mask_len = self.args.SRD\n",
        "    #     masked_text_raw_indices = torch.zeros((text_local_indices.size(0), self.args.max_seq_length, self.config.hidden_size),\n",
        "    #                                       dtype=torch.float)\n",
        "    #     for text_i, asp_i in zip(range(len(texts)), range(len(asps))):\n",
        "    #         asp_len = np.count_nonzero(asps[asp_i]) - 2\n",
        "    #         try:\n",
        "    #             asp_begin = np.argwhere(texts[text_i] == asps[asp_i][1])[0][0]\n",
        "    #             asp_avg_index = (asp_begin * 2 + asp_len) / 2\n",
        "    #         except:\n",
        "    #             continue\n",
        "    #         distances = np.zeros(np.count_nonzero(texts[text_i]), dtype=np.float32)\n",
        "    #         for i in range(1, np.count_nonzero(texts[text_i])-1):\n",
        "    #             if abs(i - asp_avg_index) + asp_len / 2 > self.args.SRD:\n",
        "    #                 distances[i] = 1 - (abs(i - asp_avg_index)+asp_len/2\n",
        "    #                                     - self.args.SRD)/np.count_nonzero(texts[text_i])\n",
        "    #             else:\n",
        "    #                 distances[i] = 1\n",
        "    #         for i in range(len(distances)):\n",
        "    #             masked_text_raw_indices[text_i][i] = masked_text_raw_indices[text_i][i] * distances[i]\n",
        "    #     # masked_text_raw_indices = torch.from_numpy(masked_text_raw_indices)\n",
        "    #     return masked_text_raw_indices.to(self.args.device)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_global_ids=None,\n",
        "        attention_mask_global=None,\n",
        "        token_global_type_ids=None,\n",
        "        input_local_ids=None,\n",
        "        attention_mask_local=None,\n",
        "        token_local_type_ids=None,\n",
        "        text_clean_indices=None,\n",
        "        aspect_indices=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "    ):\n",
        "\n",
        "        # word_vectors = self.embedder(text_clean_indices)\n",
        "        # self.lstm.flatten_parameters()\n",
        "        # out, _ = self.lstm(word_vectors)\n",
        "        # # out = out.unsqueeze(1)   #### for using CNN\n",
        "        # # print(out.size())\n",
        "        # # print(out)\n",
        "        # logits = self.fc(out[:, -1, :])\n",
        "        # # print(logits.size())\n",
        "        # # print(logits)\n",
        "        # # raise ValueError\n",
        "        # # logits = self.fc(torch.div(word_vectors.sum(axis=1), word_vectors.shape[1]))\n",
        "\n",
        "        global_outputs, _ = self.bert(\n",
        "            input_global_ids,\n",
        "            attention_mask=attention_mask_global,\n",
        "            token_type_ids=token_global_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "        )\n",
        "\n",
        "        local_outputs, _ = self.bert(\n",
        "            input_local_ids,\n",
        "            attention_mask=attention_mask_local,\n",
        "            token_type_ids=token_local_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "        )\n",
        "\n",
        "        # out_cat = torch.cat((local_outputs, global_outputs), dim=-1)\n",
        "        # out_cat = self.linear_double_cdm_or_cdw(out_cat)\n",
        "        \n",
        "        if self.args.local_context_focus == 'cdm':\n",
        "            masked_local_text_vec = self.feature_dynamic_mask(input_local_ids, aspect_indices)\n",
        "            local_outputs = torch.mul(local_outputs, masked_local_text_vec)\n",
        "            # out_cat = torch.cat((local_outputs, global_outputs), dim=-1)\n",
        "            # out_cat = self.linear_double_cdm_or_cdw(out_cat)\n",
        "\n",
        "        elif self.args.local_context_focus == 'cdw':\n",
        "            weighted_text_local_features = self.feature_dynamic_weighted(input_local_ids, aspect_indices)\n",
        "            local_outputs = torch.mul(local_outputs, weighted_text_local_features)\n",
        "            out_cat = torch.cat((local_outputs, global_outputs), dim=-1)\n",
        "            out_cat = self.linear_double_cdm_or_cdw(out_cat)\n",
        "\n",
        "        elif self.args.local_context_focus == 'lcf_fusion':\n",
        "            masked_local_text_vec = self.feature_dynamic_mask(text_local_indices, aspect_indices)\n",
        "            masked_local_out = torch.mul(local_outputs, masked_local_text_vec)\n",
        "            weighted_text_local_features = self.feature_dynamic_weighted(text_local_indices, aspect_indices)\n",
        "            weighted_local_out = torch.mul(local_outputs, weighted_text_local_features)\n",
        "            out_cat = torch.cat((masked_local_out, global_outputs, weighted_local_out), dim=-1)\n",
        "            out_cat = self.linear_triple_lcf_global(out_cat)\n",
        "\n",
        "        # self_attention_out = self.bert_SA(local_outputs)\n",
        "        self_attention_out = self.dropout(local_outputs)\n",
        "        local_pooled_out = self.bert_pooler(self_attention_out, masked_local_text_vec)\n",
        "        self_attention_out = self.dropout(global_outputs)\n",
        "        global_pooled_out = self.bert_pooler_org(self_attention_out)\n",
        "        pooled_out = torch.cat((local_pooled_out, global_pooled_out), dim=-1)\n",
        "        logits = self.dense(pooled_out)\n",
        "        outputs = (logits,)\n",
        "\n",
        "        if labels is not None:\n",
        "            if self.num_labels == 1:\n",
        "                #  We are doing regression\n",
        "                loss_fct = MSELoss()\n",
        "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
        "            else:\n",
        "                loss_fct = BCEWithLogitsLoss()\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1, self.num_labels))\n",
        "            outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing /content/transformers/examples/cadec/lcf_ichi.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWhlcZgHS8a9",
        "outputId": "bff77777-6c69-478a-fd1e-61411708bab4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile /content/transformers/examples/cadec/run_ichi.py\n",
        "import argparse\n",
        "import glob\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "from transformers import (\n",
        "    WEIGHTS_NAME,\n",
        "    AdamW,\n",
        "    AlbertConfig,\n",
        "    AlbertForSequenceClassification,\n",
        "    AlbertTokenizer,\n",
        "    BertConfig,\n",
        "    BertForSequenceClassification,\n",
        "    BertTokenizer,\n",
        "    DistilBertConfig,\n",
        "    DistilBertForSequenceClassification,\n",
        "    DistilBertTokenizer,\n",
        "    RobertaConfig,\n",
        "    RobertaForSequenceClassification,\n",
        "    RobertaTokenizer,\n",
        "    XLMConfig,\n",
        "    XLMForSequenceClassification,\n",
        "    XLMRobertaConfig,\n",
        "    XLMRobertaForSequenceClassification,\n",
        "    XLMRobertaTokenizer,\n",
        "    XLMTokenizer,\n",
        "    XLNetConfig,\n",
        "    XLNetForSequenceClassification,\n",
        "    XLNetTokenizer,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "from lcf_ichi import lcf_BERT#, lcf_XLNET, lcf_XLM, lcf_Roberta, lcf_DistilBert, lcf_Albert, lcf_XLMRoberta\n",
        "from utils_ichi import convert_examples_to_features, CADECProcessor, compute_metrics, load_and_cache_examples, Tokenizer, pad_and_truncate, build_embedding_matrix_glove, build_embedding_matrix_BioASQ\n",
        "\n",
        "try:\n",
        "    from torch.utils.tensorboard import SummaryWriter\n",
        "except ImportError:\n",
        "    from tensorboardX import SummaryWriter\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "ALL_MODELS = sum(\n",
        "    (\n",
        "        tuple(conf.pretrained_config_archive_map.keys())\n",
        "        for conf in (\n",
        "            BertConfig,\n",
        "            XLNetConfig,\n",
        "            XLMConfig,\n",
        "            RobertaConfig,\n",
        "            DistilBertConfig,\n",
        "            AlbertConfig,\n",
        "            XLMRobertaConfig,\n",
        "        )\n",
        "    ),\n",
        "    (),\n",
        ")\n",
        "\n",
        "MODEL_CLASSES = {\n",
        "    \"bert\": (BertConfig, BertForSequenceClassification, BertTokenizer, lcf_BERT),\n",
        "}\n",
        "#     \"xlnet\": (XLNetConfig, XLNetForSequenceClassification, XLNetTokenizer, lcf_XLNET),\n",
        "#     \"xlm\": (XLMConfig, XLMForSequenceClassification, XLMTokenizer, lcf_XLM),\n",
        "#     \"roberta\": (RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer, lcf_Roberta),\n",
        "#     \"distilbert\": (DistilBertConfig, DistilBertForSequenceClassification, DistilBertTokenizer, lcf_DistilBert),\n",
        "#     \"albert\": (AlbertConfig, AlbertForSequenceClassification, AlbertTokenizer, lcf_Albert),\n",
        "#     \"xlmroberta\": (XLMRobertaConfig, XLMRobertaForSequenceClassification, XLMRobertaTokenizer, lcf_XLMRoberta),\n",
        "# }\n",
        "\n",
        "\n",
        "def set_seed(args):\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    if args.n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "\n",
        "def train(args, train_dataset, model, tokenizer, tokenizer_cleantext):\n",
        "    \"\"\" Train the model \"\"\"\n",
        "    if args.local_rank in [-1, 0]:\n",
        "        tb_writer = SummaryWriter()\n",
        "\n",
        "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
        "    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
        "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n",
        "\n",
        "    if args.max_steps > 0:\n",
        "        t_total = args.max_steps\n",
        "        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
        "    else:\n",
        "        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
        "\n",
        "    # Prepare optimizer and schedule (linear warmup and decay)\n",
        "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\n",
        "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "            \"weight_decay\": args.weight_decay,\n",
        "        },\n",
        "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
        "    ]\n",
        "\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
        "    )\n",
        "\n",
        "    # Check if saved optimizer or scheduler states exist\n",
        "    if os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\")) and os.path.isfile(\n",
        "        os.path.join(args.model_name_or_path, \"scheduler.pt\")\n",
        "    ):\n",
        "        # Load in optimizer and scheduler states\n",
        "        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n",
        "        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n",
        "\n",
        "    if args.fp16:\n",
        "        try:\n",
        "            from apex import amp\n",
        "        except ImportError:\n",
        "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
        "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
        "\n",
        "    # multi-gpu training (should be after apex fp16 initialization)\n",
        "    if args.n_gpu > 1:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # Distributed training (should be after apex fp16 initialization)\n",
        "    if args.local_rank != -1:\n",
        "        model = torch.nn.parallel.DistributedDataParallel(\n",
        "            model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True,\n",
        "        )\n",
        "\n",
        "    # Train!\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
        "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
        "    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
        "    logger.info(\n",
        "        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
        "        args.train_batch_size\n",
        "        * args.gradient_accumulation_steps\n",
        "        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n",
        "    )\n",
        "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
        "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
        "\n",
        "    global_step = 0\n",
        "    epochs_trained = 0\n",
        "    steps_trained_in_current_epoch = 0\n",
        "    # Check if continuing training from a checkpoint\n",
        "    if os.path.exists(args.model_name_or_path):\n",
        "        # set global_step to gobal_step of last saved checkpoint from model path\n",
        "        global_step = int(args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0])\n",
        "        epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n",
        "        steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n",
        "\n",
        "        logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n",
        "        logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n",
        "        logger.info(\"  Continuing training from global step %d\", global_step)\n",
        "        logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n",
        "\n",
        "    tr_loss, logging_loss = 0.0, 0.0\n",
        "    model.zero_grad()\n",
        "    train_iterator = trange(\n",
        "        epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0],\n",
        "    )\n",
        "    set_seed(args)  # Added here for reproductibility\n",
        "    for _ in train_iterator:\n",
        "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n",
        "        for step, batch in enumerate(epoch_iterator):\n",
        "\n",
        "            # Skip past any already trained steps if resuming training\n",
        "            if steps_trained_in_current_epoch > 0:\n",
        "                steps_trained_in_current_epoch -= 1\n",
        "                continue\n",
        "\n",
        "            model.train()\n",
        "            batch = tuple(t.to(args.device) for t in batch)\n",
        "            inputs = {\"input_global_ids\": batch[0], \"attention_mask_global\": batch[2], \"input_local_ids\": batch[1], \"attention_mask_local\": batch[3], \"text_clean_indices\": batch[6], \"aspect_indices\": batch[7], \"labels\": batch[8]}\n",
        "            if args.model_type != \"distilbert\":\n",
        "                inputs[\"token_global_type_ids\"] = (\n",
        "                    batch[4] if args.model_type in [\"bert\", \"xlnet\", \"albert\"] else None\n",
        "                )\n",
        "                inputs[\"token_local_type_ids\"] = (\n",
        "                    batch[5] if args.model_type in [\"bert\", \"xlnet\", \"albert\"] else None\n",
        "                )\n",
        "            # inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[3]}\n",
        "            # if args.model_type != \"distilbert\":\n",
        "            #     inputs[\"token_type_ids\"] = (\n",
        "            #         batch[2] if args.model_type in [\"bert\", \"xlnet\", \"albert\"] else None\n",
        "            #     )  # XLM, DistilBERT, RoBERTa, and XLM-RoBERTa don't use segment_ids\n",
        "            outputs = model(**inputs)\n",
        "            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n",
        "\n",
        "            if args.n_gpu > 1:\n",
        "                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
        "            if args.gradient_accumulation_steps > 1:\n",
        "                loss = loss / args.gradient_accumulation_steps\n",
        "\n",
        "            if args.fp16:\n",
        "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                    scaled_loss.backward()\n",
        "            else:\n",
        "                loss.backward()\n",
        "\n",
        "            tr_loss += loss.item()\n",
        "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
        "                if args.fp16:\n",
        "                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
        "                else:\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
        "\n",
        "                optimizer.step()\n",
        "                scheduler.step()  # Update learning rate schedule\n",
        "                model.zero_grad()\n",
        "                global_step += 1\n",
        "\n",
        "                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
        "                    logs = {}\n",
        "                    if (\n",
        "                        args.local_rank == -1 and args.evaluate_during_training\n",
        "                    ):  # Only evaluate when single GPU otherwise metrics may not average well\n",
        "                        results = evaluate(args, model, tokenizer, tokenizer_cleantext)\n",
        "                        for key, value in results.items():\n",
        "                            eval_key = \"eval_{}\".format(key)\n",
        "                            logs[eval_key] = value\n",
        "\n",
        "                    loss_scalar = (tr_loss - logging_loss) / args.logging_steps\n",
        "                    learning_rate_scalar = scheduler.get_lr()[0]\n",
        "                    logs[\"learning_rate\"] = learning_rate_scalar\n",
        "                    logs[\"loss\"] = loss_scalar\n",
        "                    logging_loss = tr_loss\n",
        "\n",
        "                    for key, value in logs.items():\n",
        "                        tb_writer.add_scalar(key, value, global_step)\n",
        "                    print(json.dumps({**logs, **{\"step\": global_step}}))\n",
        "\n",
        "                # if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n",
        "                #     # Save model checkpoint\n",
        "                #     output_dir = os.path.join(args.output_dir, \"checkpoint-{}\".format(global_step))\n",
        "                #     if not os.path.exists(output_dir):\n",
        "                #         os.makedirs(output_dir)\n",
        "                #     # model_to_save = (\n",
        "                #     #     model.module if hasattr(model, \"module\") else model\n",
        "                #     # )  # Take care of distributed/parallel training\n",
        "                #     # model_to_save.save_pretrained(output_dir)\n",
        "                #     torch.save(model.state_dict(), args.output_dir)\n",
        "                #     tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "                #     torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n",
        "                #     logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
        "\n",
        "                #     torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
        "                #     torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
        "                #     logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n",
        "\n",
        "            if args.max_steps > 0 and global_step > args.max_steps:\n",
        "                epoch_iterator.close()\n",
        "                break\n",
        "        if args.max_steps > 0 and global_step > args.max_steps:\n",
        "            train_iterator.close()\n",
        "            break\n",
        "\n",
        "    if args.local_rank in [-1, 0]:\n",
        "        tb_writer.close()\n",
        "\n",
        "    return global_step, tr_loss / global_step\n",
        "\n",
        "\n",
        "def evaluate(args, model, tokenizer, tokenizer_cleantext, prefix=\"\"):\n",
        "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
        "    eval_task_names = (\"ichi\",)\n",
        "    eval_outputs_dirs = (args.output_dir,)\n",
        "\n",
        "    results = {}\n",
        "    for eval_task, eval_output_dir in zip(eval_task_names, eval_outputs_dirs):\n",
        "        eval_dataset, tokenizer_cleantext = load_and_cache_examples(args, eval_task, tokenizer, tokenizer_cleantext, evaluate=True)\n",
        "\n",
        "        if not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0]:\n",
        "            os.makedirs(eval_output_dir)\n",
        "\n",
        "        args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
        "        # Note that DistributedSampler samples randomly\n",
        "        eval_sampler = SequentialSampler(eval_dataset)\n",
        "        eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
        "\n",
        "        # multi-gpu eval\n",
        "        if args.n_gpu > 1:\n",
        "            model = torch.nn.DataParallel(model)\n",
        "\n",
        "        # Eval!\n",
        "        logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
        "        logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
        "        logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
        "        eval_loss = 0.0\n",
        "        nb_eval_steps = 0\n",
        "        preds = None\n",
        "        preds_original = None\n",
        "        out_label_ids = None\n",
        "        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "            model.eval()\n",
        "            batch = tuple(t.to(args.device) for t in batch)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                inputs = {\"input_global_ids\": batch[0], \"attention_mask_global\": batch[2], \"input_local_ids\": batch[1], \"attention_mask_local\": batch[3], \"text_clean_indices\": batch[6], \"aspect_indices\": batch[7], \"labels\": batch[8]}\n",
        "                if args.model_type != \"distilbert\":\n",
        "                    inputs[\"token_global_type_ids\"] = (\n",
        "                        batch[4] if args.model_type in [\"bert\", \"xlnet\", \"albert\"] else None\n",
        "                    )\n",
        "                    inputs[\"token_local_type_ids\"] = (\n",
        "                        batch[5] if args.model_type in [\"bert\", \"xlnet\", \"albert\"] else None\n",
        "                    )  # XLM, DistilBERT, RoBERTa, and XLM-RoBERTa don't use segment_ids\n",
        "                outputs = model(**inputs)\n",
        "                tmp_eval_loss, logits = outputs[:2]\n",
        "\n",
        "                eval_loss += tmp_eval_loss.mean().item()\n",
        "            nb_eval_steps += 1\n",
        "            if preds is None:\n",
        "                preds = logits.detach().cpu().numpy()\n",
        "                preds_original = logits.detach().cpu().numpy()\n",
        "                out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n",
        "            else:\n",
        "                preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
        "                preds_original = np.append(preds_original, logits.detach().cpu().numpy(), axis=0)\n",
        "                out_label_ids = np.append(out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0)\n",
        "\n",
        "        eval_loss = eval_loss / nb_eval_steps\n",
        "        preds = (preds > 0)\n",
        "        result = compute_metrics(preds, out_label_ids) ######################update kar !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "        results.update(result)\n",
        "\n",
        "        output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n",
        "        with open(output_eval_file, \"w\") as writer:\n",
        "            logger.info(\"***** Eval results {} *****\".format(prefix))\n",
        "            for key in sorted(result.keys()):\n",
        "                logger.info(\"  %s = %s\", key, str(result[key]))\n",
        "                writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
        "            # for a in preds:\n",
        "            #     writer.write(str(a)+'\\n')\n",
        "            writer.write('[')\n",
        "            for a in preds_original:\n",
        "                writer.write('[')\n",
        "                for c in range(len(a)):\n",
        "                    b = a[c]\n",
        "                    if c!=6:\n",
        "                        writer.write(str(b)+',')\n",
        "                    else:\n",
        "                        writer.write(str(b))\n",
        "                writer.write(']')\n",
        "                writer.write('\\n')\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # Required parameters\n",
        "    parser.add_argument(\n",
        "        \"--data_dir\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        required=True,\n",
        "        help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--model_type\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        required=True,\n",
        "        help=\"Model type selected in the list: \" + \", \".join(MODEL_CLASSES.keys()),\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--model_name_or_path\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        required=True,\n",
        "        help=\"Path to pre-trained model or shortcut name selected in the list: \" + \", \".join(ALL_MODELS),\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--output_dir\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        required=True,\n",
        "        help=\"The output directory where the model predictions and checkpoints will be written.\",\n",
        "    )\n",
        "\n",
        "    # Other parameters\n",
        "    parser.add_argument(\n",
        "        \"--config_name\", default=\"\", type=str, help=\"Pretrained config name or path if not the same as model_name\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--tokenizer_name\",\n",
        "        default=\"\",\n",
        "        type=str,\n",
        "        help=\"Pretrained tokenizer name or path if not the same as model_name\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--cache_dir\",\n",
        "        default=\"\",\n",
        "        type=str,\n",
        "        help=\"Where do you want to store the pre-trained models downloaded from s3\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--max_seq_length\",\n",
        "        default=128,\n",
        "        type=int,\n",
        "        help=\"The maximum total input sequence length after tokenization. Sequences longer \"\n",
        "        \"than this will be truncated, sequences shorter will be padded.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--embedding_dim_word2vec\",\n",
        "        default=300,\n",
        "        type=int,\n",
        "        help=\"embedding dimension for the word vectors in the medical module\",\n",
        "    )\n",
        "    parser.add_argument(\"--do_train\", action=\"store_true\", help=\"Whether to run training.\")\n",
        "    parser.add_argument(\"--do_eval\", action=\"store_true\", help=\"Whether to run eval on the dev set.\")\n",
        "    parser.add_argument(\n",
        "        \"--evaluate_during_training\", action=\"store_true\", help=\"Rul evaluation during training at each logging step.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--do_lower_case\", action=\"store_true\", help=\"Set this flag if you are using an uncased model.\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--per_gpu_train_batch_size\", default=8, type=int, help=\"Batch size per GPU/CPU for training.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--per_gpu_eval_batch_size\", default=8, type=int, help=\"Batch size per GPU/CPU for evaluation.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--gradient_accumulation_steps\",\n",
        "        type=int,\n",
        "        default=1,\n",
        "        help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n",
        "    )\n",
        "    parser.add_argument(\"--learning_rate\", default=5e-5, type=float, help=\"The initial learning rate for Adam.\")\n",
        "    parser.add_argument(\"--weight_decay\", default=0.0, type=float, help=\"Weight decay if we apply some.\")\n",
        "    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n",
        "    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n",
        "    parser.add_argument(\n",
        "        \"--num_train_epochs\", default=3.0, type=float, help=\"Total number of training epochs to perform.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--max_steps\",\n",
        "        default=-1,\n",
        "        type=int,\n",
        "        help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\",\n",
        "    )\n",
        "    parser.add_argument(\"--warmup_steps\", default=0, type=int, help=\"Linear warmup over warmup_steps.\")\n",
        "\n",
        "    parser.add_argument(\"--logging_steps\", type=int, default=50, help=\"Log every X updates steps.\")\n",
        "    parser.add_argument(\"--save_steps\", type=int, default=10000, help=\"Save checkpoint every X updates steps.\")\n",
        "    parser.add_argument(\n",
        "        \"--eval_all_checkpoints\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number\",\n",
        "    )\n",
        "    parser.add_argument(\"--no_cuda\", action=\"store_true\", help=\"Avoid using CUDA when available\")\n",
        "    parser.add_argument(\n",
        "        \"--overwrite_output_dir\", action=\"store_true\", help=\"Overwrite the content of the output directory\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--overwrite_cache\", action=\"store_true\", help=\"Overwrite the cached training and evaluation sets\",\n",
        "    )\n",
        "    parser.add_argument(\"--seed\", type=int, default=42, help=\"random seed for initialization\")\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--fp16\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--fp16_opt_level\",\n",
        "        type=str,\n",
        "        default=\"O1\",\n",
        "        help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
        "        \"See details at https://nvidia.github.io/apex/amp.html\",\n",
        "    )\n",
        "    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"For distributed training: local_rank\")\n",
        "    parser.add_argument(\"--server_ip\", type=str, default=\"\", help=\"For distant debugging.\")\n",
        "    parser.add_argument(\"--server_port\", type=str, default=\"\", help=\"For distant debugging.\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if (\n",
        "        os.path.exists(args.output_dir)\n",
        "        and os.listdir(args.output_dir)\n",
        "        and args.do_train\n",
        "        and not args.overwrite_output_dir\n",
        "    ):\n",
        "        raise ValueError(\n",
        "            \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n",
        "                args.output_dir\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # Setup distant debugging if needed\n",
        "    if args.server_ip and args.server_port:\n",
        "        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n",
        "        import ptvsd\n",
        "\n",
        "        print(\"Waiting for debugger attach\")\n",
        "        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n",
        "        ptvsd.wait_for_attach()\n",
        "\n",
        "    # Setup CUDA, GPU & distributed training\n",
        "    if args.local_rank == -1 or args.no_cuda:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
        "        args.n_gpu = torch.cuda.device_count()\n",
        "    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
        "        torch.cuda.set_device(args.local_rank)\n",
        "        device = torch.device(\"cuda\", args.local_rank)\n",
        "        torch.distributed.init_process_group(backend=\"nccl\")\n",
        "        args.n_gpu = 1\n",
        "    args.device = device\n",
        "\n",
        "    # Setup logging\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n",
        "    )\n",
        "    logger.warning(\n",
        "        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
        "        args.local_rank,\n",
        "        device,\n",
        "        args.n_gpu,\n",
        "        bool(args.local_rank != -1),\n",
        "        args.fp16,\n",
        "    )\n",
        "\n",
        "    # Set seed\n",
        "    set_seed(args)\n",
        "\n",
        "    # Prepare GLUE task\n",
        "    args.task_name = 'cadec'\n",
        "    processor = CADECProcessor()\n",
        "    args.output_mode = \"classification\"\n",
        "    label_list = processor.get_labels()\n",
        "    num_labels = len(label_list)\n",
        "\n",
        "    # Load pretrained model and tokenizer\n",
        "    if args.local_rank not in [-1, 0]:\n",
        "        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n",
        "\n",
        "    args.model_type = args.model_type.lower()\n",
        "    config_class, model_class, tokenizer_class, lcf_model = MODEL_CLASSES[args.model_type]\n",
        "    config = config_class.from_pretrained(\n",
        "        args.config_name if args.config_name else args.model_name_or_path,\n",
        "        num_labels=num_labels,\n",
        "        finetuning_task=args.task_name,\n",
        "        cache_dir=args.cache_dir if args.cache_dir else None,\n",
        "    )\n",
        "    tokenizer = tokenizer_class.from_pretrained(\n",
        "        args.tokenizer_name if args.tokenizer_name else args.model_name_or_path,\n",
        "        do_lower_case=args.do_lower_case,\n",
        "        cache_dir=args.cache_dir if args.cache_dir else None,\n",
        "    )\n",
        "    tokenizer_cleantext = Tokenizer(max_seq_len = 1600, max_num_words=20000,lower=True)\n",
        "    # model = model_class.from_pretrained(\n",
        "    #     args.model_name_or_path,\n",
        "    #     from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
        "    #     config=config,\n",
        "    #     cache_dir=args.cache_dir if args.cache_dir else None,\n",
        "    #     force_download=True,\n",
        "    # )\n",
        "\n",
        "    args.use_single_bert = False\n",
        "    args.local_context_focus = 'cdm'\n",
        "    args.embedding_type = 'glove'\n",
        "\n",
        "    cached_embeddingmatrix_file = os.path.join(\n",
        "        args.data_dir,\n",
        "        \"cachedword2vec_{}_{}\".format(\n",
        "            str(args.task_name),\n",
        "            str(\"glove\"),\n",
        "        ),\n",
        "    )\n",
        "    cached_embeddingmatrix_path = \".\"\n",
        "    \n",
        "    train_dataset, tokenizer_cleantext = load_and_cache_examples(args, args.task_name, tokenizer, tokenizer_cleantext, evaluate=False)\n",
        "    # embedding_matrix = build_embedding_matrix_glove(tokenizer_cleantext.word2idx, args.embedding_dim_word2vec, cached_embeddingmatrix_file, cached_embeddingmatrix_path)\n",
        "    # print(embedding_matrix.shape)\n",
        "    # print(tokenizer_cleantext.word2idx)\n",
        "    \n",
        "    # args.word2vec = embedding_matrix\n",
        "    # args.emb_size = embedding_matrix.shape[1]\n",
        "    model = lcf_model.from_pretrained(\n",
        "        args.model_name_or_path,\n",
        "        from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
        "        config=config,\n",
        "        args=args,\n",
        "        cache_dir=args.cache_dir if args.cache_dir else None,\n",
        "    )\n",
        "    # model = lcf_Roberta.from_pretrained(\n",
        "    #     \"/home/bt1/17CS10037/new_transformers/transformers/roberta-large-pytorch_model.bin\",\n",
        "    #     from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
        "    #     config=config,\n",
        "    #     args=args,\n",
        "    #     cache_dir=args.cache_dir if args.cache_dir else None,\n",
        "    #     force_download=True,\n",
        "    # )\n",
        "\n",
        "    if args.local_rank == 0:\n",
        "        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n",
        "\n",
        "    model.to(args.device)\n",
        "\n",
        "    logger.info(\"Training/evaluation parameters %s\", args)\n",
        "    \n",
        "    # Training\n",
        "    if args.do_train:\n",
        "        # train_dataset, tokenizer_cleantext = load_and_cache_examples(args, args.task_name, tokenizer, tokenizer_cleantext, evaluate=False)\n",
        "        # matrix = build_embedding_matrix_glove(tokenizer_cleantext.word2idx, args.embedding_dim_word2vec, cached_glove_embeddingmatrix_file, cached_glove_embeddingmatrix_path)\n",
        "        global_step, tr_loss = train(args, train_dataset, model, tokenizer, tokenizer_cleantext)\n",
        "        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
        "\n",
        "    result = evaluate(args, model, tokenizer, tokenizer_cleantext)\n",
        "    # Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "    if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n",
        "        # Create output directory if needed\n",
        "        if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:\n",
        "            os.makedirs(args.output_dir)\n",
        "\n",
        "        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n",
        "        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "        # They can then be reloaded using `from_pretrained()`\n",
        "        # model_to_save = (\n",
        "        #     model.module if hasattr(model, \"module\") else model\n",
        "        # )  # Take care of distributed/parallel training\n",
        "        # model_to_save.save_pretrained(args.output_dir)\n",
        "        model_name = \"{}.pt\".format(args.model_type)\n",
        "        torch.save(model.state_dict(), os.path.join(args.output_dir,model_name))\n",
        "        tokenizer.save_pretrained(args.output_dir)\n",
        "\n",
        "        # Good practice: save your training arguments together with the trained model\n",
        "        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n",
        "\n",
        "        # Load a trained model and vocabulary that you have fine-tuned\n",
        "        # model = model_class.from_pretrained(args.output_dir)\n",
        "        model = lcf_model(config, args)\n",
        "        model.load_state_dict(torch.load(os.path.join(args.output_dir,model_name)))\n",
        "        tokenizer = tokenizer_class.from_pretrained(args.output_dir)\n",
        "        model.to(args.device)\n",
        "\n",
        "    # Evaluation\n",
        "    results = {}\n",
        "    if args.do_eval and args.local_rank in [-1, 0]:\n",
        "        tokenizer = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n",
        "        cached_tokenizer_cleantext_file = os.path.join(\n",
        "            args.data_dir,\n",
        "            \"cachedtokenizer_{}_{}_{}\".format(\n",
        "                list(filter(None, args.model_name_or_path.split(\"/\"))).pop(),\n",
        "                str(args.embedding_type),\n",
        "                str(args.task_name),\n",
        "            ),\n",
        "        )\n",
        "        if os.path.exists(cached_tokenizer_cleantext_file):\n",
        "            print('loading tokenizer from cache:', cached_tokenizer_cleantext_file)\n",
        "            tokenizer_cleantext = pickle.load(open(cached_tokenizer_cleantext_file, 'rb'))\n",
        "        checkpoints = [args.output_dir]\n",
        "        if args.eval_all_checkpoints:\n",
        "            checkpoints = list(\n",
        "                os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True))\n",
        "            )\n",
        "            logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n",
        "        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
        "        for checkpoint in checkpoints:\n",
        "            global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n",
        "            prefix = checkpoint.split(\"/\")[-1] if checkpoint.find(\"checkpoint\") != -1 else \"\"\n",
        "\n",
        "            # model = model_class.from_pretrained(checkpoint)\n",
        "            # model = lcf_model.from_pretrained(\n",
        "            #     args.model_name_or_path,\n",
        "            #     from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
        "            #     config=config,\n",
        "            #     args=args,\n",
        "            #     cache_dir=args.cache_dir if args.cache_dir else None,\n",
        "            # )\n",
        "\n",
        "            model = lcf_model(config, args)\n",
        "            model_name = \"{}.pt\".format(args.model_type)\n",
        "            model.load_state_dict(torch.load(os.path.join(args.output_dir,model_name)))\n",
        "            model.to(args.device)\n",
        "            result = evaluate(args, model, tokenizer, tokenizer_cleantext, prefix=prefix)\n",
        "            result = dict((k + \"_{}\".format(global_step), v) for k, v in result.items())\n",
        "            results.update(result)\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing /content/transformers/examples/cadec/run_ichi.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmpFdc6sbALY",
        "outputId": "bb05ccc8-582b-4669-e48b-9e1b5ee02d20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python ./examples/cadec/run_ichi.py --model_type bert --model_name_or_path bert-base-uncased --do_lower_case --do_train --do_eval --data_dir ./data/ichi --max_seq_length 256 --per_gpu_eval_batch_size=16 --per_gpu_train_batch_size=16 --learning_rate 5e-5 --num_train_epochs 10 --output_dir ./tmp/ichi_bert_base_new --overwrite_output_dir"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-10-17 17:07:07.969742: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "10/17/2020 17:07:09 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "10/17/2020 17:07:10 - INFO - filelock -   Lock 140269242721112 acquired on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock\n",
            "10/17/2020 17:07:10 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpf3im5us4\n",
            "Downloading: 100% 433/433 [00:00<00:00, 343kB/s]\n",
            "10/17/2020 17:07:10 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json in cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "10/17/2020 17:07:10 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "10/17/2020 17:07:10 - INFO - filelock -   Lock 140269242721112 released on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock\n",
            "10/17/2020 17:07:10 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "10/17/2020 17:07:10 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"do_sample\": false,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"finetuning_task\": \"cadec\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 5,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "10/17/2020 17:07:10 - INFO - filelock -   Lock 140269242352304 acquired on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
            "10/17/2020 17:07:10 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpiookka6w\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 908kB/s]\n",
            "10/17/2020 17:07:11 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt in cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "10/17/2020 17:07:11 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "10/17/2020 17:07:11 - INFO - filelock -   Lock 140269242352304 released on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
            "10/17/2020 17:07:11 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "10/17/2020 17:07:11 - INFO - utils_ichi -   Creating features from dataset file at ./data/ichi\n",
            "10/17/2020 17:07:20 - INFO - utils_ichi -   Writing example 0/942\n",
            "10/17/2020 17:07:20 - INFO - utils_ichi -   *** Example ***\n",
            "10/17/2020 17:07:20 - INFO - utils_ichi -   guid: train-1\n",
            "10/17/2020 17:07:20 - INFO - utils_ichi -   input_global_ids: 101 6034 5458 2791 1998 4257 9307 5897 1012 2025 2469 3251 2009 2003 1996 5423 15660 2021 1045 2572 2085 5458 2000 1996 2391 1997 15575 1012 2003 2009 2147 1010 2003 2009 2033 2893 3080 2030 2003 2009 1996 5423 15660 1029 1029 1012 102 4257 9307 5897 102 15575 102 5458 102 2025 2469 102 6034 102 5458 2791 102 3080 102 5423 15660 102 5423 15660 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:07:20 - INFO - utils_ichi -   attention_mask_global: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:07:20 - INFO - utils_ichi -   token_global_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:07:20 - INFO - utils_ichi -   input_local_ids: 101 6034 5458 2791 1998 4257 9307 5897 1012 2025 2469 3251 2009 2003 1996 5423 15660 2021 1045 2572 2085 5458 2000 1996 2391 1997 15575 1012 2003 2009 2147 1010 2003 2009 2033 2893 3080 2030 2003 2009 1996 5423 15660 1029 1029 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:07:20 - INFO - utils_ichi -   attention_mask_local: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:07:20 - INFO - utils_ichi -   token_local_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:07:20 - INFO - utils_ichi -   text_clean_indices: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16\n",
            "10/17/2020 17:07:20 - INFO - utils_ichi -   aspect_indices: 4257 9307 5897 15575 5458 2025 2469 6034 5458 2791 3080 5423 15660 5423 15660 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:07:20 - INFO - utils_ichi -   label: [0, 1, 0, 0, 0] (id = [0, 1, 0, 0, 0])\n",
            "10/17/2020 17:07:20 - INFO - utils_ichi -   *** Example ***\n",
            "10/17/2020 17:07:20 - INFO - utils_ichi -   guid: train-2\n",
            "10/17/2020 17:07:20 - INFO - utils_ichi -   input_global_ids: 101 1015 1013 1021 1013 5709 1011 2506 1012 2025 2035 1997 2009 6866 2077 1012 2182 2003 1996 13633 1012 1045 2196 3561 2009 1010 2004 2026 16160 2409 2033 2009 2001 2242 2062 1012 1045 2572 3227 1037 2092 12042 2828 1997 2711 1012 2026 6245 2106 2272 2006 2738 3402 1010 2055 1996 2168 2051 2004 2043 1045 2318 1996 5423 15660 1012 2026 6040 2000 2017 2052 2022 2000 4952 2000 2115 2303 1010 2009 2089 2022 2667 2000 2425 2017 2242 1012 3046 8738 1998 4654 17119 18380 2000 2896 16480 4244 27833 1006 2023 2003 1052 8873 6290 1005 1055 12832 2036 1010 2156 2037 7427 19274 1007 2021 1012 3374 1052 8873 6290 1012 2023 2003 2028 24665 11431 2100 3345 2017 1005 2128 2025 2893 3071 2006 1012 102 7427 19274 102 6245 102 5423 15660 102 16480 4244 27833 102 5423 15660 102 16480 4244 27833 102 6040 102 4952 102 3345 102 24665 11431 2100 102 13633 102 2156 102 12042 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:07:20 - INFO - utils_ichi -   attention_mask_global: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:07:20 - INFO - utils_ichi -   token_global_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:07:20 - INFO - utils_ichi -   input_local_ids: 101 1015 1013 1021 1013 5709 1011 2506 1012 2025 2035 1997 2009 6866 2077 1012 2182 2003 1996 13633 1012 1045 2196 3561 2009 1010 2004 2026 16160 2409 2033 2009 2001 2242 2062 1012 1045 2572 3227 1037 2092 12042 2828 1997 2711 1012 2026 6245 2106 2272 2006 2738 3402 1010 2055 1996 2168 2051 2004 2043 1045 2318 1996 5423 15660 1012 2026 6040 2000 2017 2052 2022 2000 4952 2000 2115 2303 1010 2009 2089 2022 2667 2000 2425 2017 2242 1012 3046 8738 1998 4654 17119 18380 2000 2896 16480 4244 27833 1006 2023 2003 1052 8873 6290 1005 1055 12832 2036 1010 2156 2037 7427 19274 1007 2021 1012 3374 1052 8873 6290 1012 2023 2003 2028 24665 11431 2100 3345 2017 1005 2128 2025 2893 3071 2006 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:07:20 - INFO - utils_ichi -   attention_mask_local: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:07:20 - INFO - utils_ichi -   token_local_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:07:20 - INFO - utils_ichi -   text_clean_indices: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 17 18 19 20 21 8 22 23 24 25 26 27 28 8 29 30 31 32 33 34 35 36 37 38 39 8 40 41 34 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 14 68 69\n",
            "10/17/2020 17:07:20 - INFO - utils_ichi -   aspect_indices: 7427 19274 6245 5423 15660 16480 4244 27833 5423 15660 16480 4244 27833 6040 4952 3345 24665 11431 2100 13633 2156 12042 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:07:20 - INFO - utils_ichi -   label: [0, 1, 0, 1, 0] (id = [0, 1, 0, 1, 0])\n",
            "10/17/2020 17:07:20 - INFO - utils_ichi -   *** Example ***\n",
            "10/17/2020 17:07:20 - INFO - utils_ichi -   guid: train-3\n",
            "10/17/2020 17:07:20 - INFO - utils_ichi -   input_global_ids: 101 2061 6517 2000 2156 2061 2116 2007 3471 2066 3067 999 20519 4788 5582 3471 2007 4101 3255 1010 6740 3255 1010 6245 1998 5458 2791 1012 3460 2038 2743 2035 11901 1997 5852 1998 1051 10841 6392 2078 1005 1056 2424 2505 3308 1010 3071 2001 7302 2000 2396 26378 29472 1012 1045 3030 2635 5423 15660 1998 2116 1997 1996 3471 2253 2185 2044 1016 2420 1012 1045 1005 1049 2025 2183 2067 2000 2009 1012 102 6740 3255 102 4101 3255 102 6245 102 5423 15660 102 5423 15660 102 11901 102 2066 102 6517 102 5852 102 5458 2791 102 2067 102 2156 102 3471 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:07:20 - INFO - utils_ichi -   attention_mask_global: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:07:20 - INFO - utils_ichi -   token_global_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:07:20 - INFO - utils_ichi -   input_local_ids: 101 2061 6517 2000 2156 2061 2116 2007 3471 2066 3067 999 20519 4788 5582 3471 2007 4101 3255 1010 6740 3255 1010 6245 1998 5458 2791 1012 3460 2038 2743 2035 11901 1997 5852 1998 1051 10841 6392 2078 1005 1056 2424 2505 3308 1010 3071 2001 7302 2000 2396 26378 29472 1012 1045 3030 2635 5423 15660 1998 2116 1997 1996 3471 2253 2185 2044 1016 2420 1012 1045 1005 1049 2025 2183 2067 2000 2009 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:07:20 - INFO - utils_ichi -   attention_mask_local: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:07:20 - INFO - utils_ichi -   token_local_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:07:20 - INFO - utils_ichi -   text_clean_indices: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 70 71 59 72 73 74 75 76 77 73 78 79 80 79 35 81 82 83 84 85 86 87 88 89 68 90 91 8 92 93 7 72 73 94 95 96 97 8 98 99 100\n",
            "10/17/2020 17:07:20 - INFO - utils_ichi -   aspect_indices: 6740 3255 4101 3255 6245 5423 15660 5423 15660 11901 2066 6517 5852 5458 2791 2067 2156 3471 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:07:20 - INFO - utils_ichi -   label: [1, 1, 1, 0, 0] (id = [1, 1, 1, 0, 0])\n",
            "10/17/2020 17:07:20 - INFO - utils_ichi -   *** Example ***\n",
            "10/17/2020 17:07:20 - INFO - utils_ichi -   guid: train-4\n",
            "10/17/2020 17:07:20 - INFO - utils_ichi -   input_global_ids: 101 2306 1015 3204 2051 2764 5729 6245 1010 13720 2000 4139 2870 2013 2793 1010 14978 2015 10126 1010 1998 2371 2066 1045 2001 2746 2091 2007 19857 1012 1045 2097 2196 2202 28093 7076 2153 1012 2026 3460 14964 5423 15660 2004 1037 1012 102 5729 6245 102 14978 2015 102 5423 15660 102 28093 7076 102 5423 15660 102 28093 7076 102 3204 102 14964 102 4139 102 2066 102 19857 102 2764 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:07:20 - INFO - utils_ichi -   attention_mask_global: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:07:20 - INFO - utils_ichi -   token_global_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:07:20 - INFO - utils_ichi -   input_local_ids: 101 2306 1015 3204 2051 2764 5729 6245 1010 13720 2000 4139 2870 2013 2793 1010 14978 2015 10126 1010 1998 2371 2066 1045 2001 2746 2091 2007 19857 1012 1045 2097 2196 2202 28093 7076 2153 1012 2026 3460 14964 5423 15660 2004 1037 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:07:20 - INFO - utils_ichi -   attention_mask_local: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:07:20 - INFO - utils_ichi -   token_local_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:07:20 - INFO - utils_ichi -   text_clean_indices: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 101 102 103 39 104 105 106 107 108 109 110 111 112 74 8 113 114 8 22 115 116 117 34 82 118 7 119\n",
            "10/17/2020 17:07:20 - INFO - utils_ichi -   aspect_indices: 5729 6245 14978 2015 5423 15660 28093 7076 5423 15660 28093 7076 3204 14964 4139 2066 19857 2764 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:07:20 - INFO - utils_ichi -   label: [0, 1, 0, 0, 0] (id = [0, 1, 0, 0, 0])\n",
            "10/17/2020 17:07:20 - INFO - utils_ichi -   *** Example ***\n",
            "10/17/2020 17:07:20 - INFO - utils_ichi -   guid: train-5\n",
            "10/17/2020 17:07:20 - INFO - utils_ichi -   input_global_ids: 101 1045 2031 2042 2006 5423 15660 2005 2184 2086 2005 2540 6032 2322 24798 1012 1045 17311 2904 2000 2184 24798 1998 2794 13012 27108 13741 11460 2138 1997 2152 25510 2140 1012 1045 2079 3325 4190 11251 1998 13675 16613 2075 1012 13675 16613 2075 7653 2007 18044 1012 102 4190 11251 102 2904 102 3325 102 18044 102 13675 16613 2075 102 5423 15660 102 13012 27108 102 5423 15660 102 13012 27108 102 2540 102 2184 2086 102 2794 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:07:20 - INFO - utils_ichi -   attention_mask_global: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:07:20 - INFO - utils_ichi -   token_global_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:07:20 - INFO - utils_ichi -   input_local_ids: 101 1045 2031 2042 2006 5423 15660 2005 2184 2086 2005 2540 6032 2322 24798 1012 1045 17311 2904 2000 2184 24798 1998 2794 13012 27108 13741 11460 2138 1997 2152 25510 2140 1012 1045 2079 3325 4190 11251 1998 13675 16613 2075 1012 13675 16613 2075 7653 2007 18044 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:07:20 - INFO - utils_ichi -   attention_mask_local: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:07:20 - INFO - utils_ichi -   token_local_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:07:20 - INFO - utils_ichi -   text_clean_indices: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 8 7 120 121 122 123 124 125 126 127 128 129 130 131 132 133 8 134 135 136 137 138 139 140\n",
            "10/17/2020 17:07:20 - INFO - utils_ichi -   aspect_indices: 4190 11251 2904 3325 18044 13675 16613 2075 5423 15660 13012 27108 5423 15660 13012 27108 2540 2184 2086 2794 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:07:20 - INFO - utils_ichi -   label: [0, 1, 0, 0, 0] (id = [0, 1, 0, 0, 0])\n",
            "10/17/2020 17:07:25 - INFO - utils_ichi -   Saving features into cached file ./data/ichi/cached_train_bert-base-uncased_256_cadec\n",
            "10/17/2020 17:07:26 - INFO - filelock -   Lock 140268755954712 acquired on /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock\n",
            "10/17/2020 17:07:26 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpsmjvn47b\n",
            "Downloading: 100% 440M/440M [00:13<00:00, 33.4MB/s]\n",
            "10/17/2020 17:07:40 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin in cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "10/17/2020 17:07:40 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "10/17/2020 17:07:40 - INFO - filelock -   Lock 140268755954712 released on /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock\n",
            "10/17/2020 17:07:40 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "10/17/2020 17:07:44 - INFO - transformers.modeling_utils -   Weights of lcf_BERT not initialized from pretrained model: ['bert_SA.SA.query.weight', 'bert_SA.SA.query.bias', 'bert_SA.SA.key.weight', 'bert_SA.SA.key.bias', 'bert_SA.SA.value.weight', 'bert_SA.SA.value.bias', 'linear_double_cdm_or_cdw.weight', 'linear_double_cdm_or_cdw.bias', 'linear_triple_lcf_global.weight', 'linear_triple_lcf_global.bias', 'bert_pooler_org.dense.weight', 'bert_pooler_org.dense.bias', 'bert_pooler.dense.weight', 'bert_pooler.dense.bias', 'dense.weight', 'dense.bias']\n",
            "10/17/2020 17:07:44 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in lcf_BERT: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "10/17/2020 17:07:57 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='./data/ichi', device=device(type='cuda'), do_eval=True, do_lower_case=True, do_train=True, embedding_dim_word2vec=300, embedding_type='glove', eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, local_context_focus='cdm', local_rank=-1, logging_steps=50, max_grad_norm=1.0, max_seq_length=256, max_steps=-1, model_name_or_path='bert-base-uncased', model_type='bert', n_gpu=1, no_cuda=False, num_train_epochs=10.0, output_dir='./tmp/ichi_bert_base_new', output_mode='classification', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=16, per_gpu_train_batch_size=16, save_steps=10000, seed=42, server_ip='', server_port='', task_name='cadec', tokenizer_name='', use_single_bert=False, warmup_steps=0, weight_decay=0.0)\n",
            "10/17/2020 17:07:57 - INFO - __main__ -   ***** Running training *****\n",
            "10/17/2020 17:07:57 - INFO - __main__ -     Num examples = 942\n",
            "10/17/2020 17:07:57 - INFO - __main__ -     Num Epochs = 10\n",
            "10/17/2020 17:07:57 - INFO - __main__ -     Instantaneous batch size per GPU = 16\n",
            "10/17/2020 17:07:57 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "10/17/2020 17:07:57 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "10/17/2020 17:07:57 - INFO - __main__ -     Total optimization steps = 590\n",
            "Epoch:   0% 0/10 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/59 [00:00<?, ?it/s]\u001b[A/content/transformers/examples/cadec/lcf_ichi.py:92: UserWarning: This overload of nonzero is deprecated:\n",
            "\tnonzero()\n",
            "Consider using one of the following signatures instead:\n",
            "\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
            "  indices = (text_local_indices[i] == aspect_indices[i][j]).nonzero()\n",
            "\n",
            "Iteration:   2% 1/59 [00:01<01:47,  1.86s/it]\u001b[A\n",
            "Iteration:   3% 2/59 [00:03<01:38,  1.73s/it]\u001b[A\n",
            "Iteration:   5% 3/59 [00:04<01:32,  1.65s/it]\u001b[A\n",
            "Iteration:   7% 4/59 [00:06<01:29,  1.62s/it]\u001b[A\n",
            "Iteration:   8% 5/59 [00:07<01:24,  1.56s/it]\u001b[A\n",
            "Iteration:  10% 6/59 [00:09<01:21,  1.54s/it]\u001b[A\n",
            "Iteration:  12% 7/59 [00:10<01:18,  1.52s/it]\u001b[A\n",
            "Iteration:  14% 8/59 [00:12<01:15,  1.49s/it]\u001b[A\n",
            "Iteration:  15% 9/59 [00:13<01:13,  1.47s/it]\u001b[A\n",
            "Iteration:  17% 10/59 [00:15<01:12,  1.48s/it]\u001b[A\n",
            "Iteration:  19% 11/59 [00:16<01:11,  1.49s/it]\u001b[A\n",
            "Iteration:  20% 12/59 [00:17<01:09,  1.47s/it]\u001b[A\n",
            "Iteration:  22% 13/59 [00:19<01:07,  1.46s/it]\u001b[A\n",
            "Iteration:  24% 14/59 [00:20<01:06,  1.47s/it]\u001b[A\n",
            "Iteration:  25% 15/59 [00:22<01:04,  1.47s/it]\u001b[A\n",
            "Iteration:  27% 16/59 [00:23<01:03,  1.48s/it]\u001b[A\n",
            "Iteration:  29% 17/59 [00:25<01:01,  1.47s/it]\u001b[A\n",
            "Iteration:  31% 18/59 [00:26<01:00,  1.47s/it]\u001b[A\n",
            "Iteration:  32% 19/59 [00:28<00:58,  1.47s/it]\u001b[A\n",
            "Iteration:  34% 20/59 [00:29<00:57,  1.47s/it]\u001b[A\n",
            "Iteration:  36% 21/59 [00:31<00:56,  1.48s/it]\u001b[A\n",
            "Iteration:  37% 22/59 [00:32<00:55,  1.50s/it]\u001b[A\n",
            "Iteration:  39% 23/59 [00:34<00:53,  1.48s/it]\u001b[A\n",
            "Iteration:  41% 24/59 [00:35<00:52,  1.49s/it]\u001b[A\n",
            "Iteration:  42% 25/59 [00:37<00:50,  1.49s/it]\u001b[A\n",
            "Iteration:  44% 26/59 [00:38<00:49,  1.50s/it]\u001b[A\n",
            "Iteration:  46% 27/59 [00:40<00:47,  1.49s/it]\u001b[A\n",
            "Iteration:  47% 28/59 [00:41<00:46,  1.50s/it]\u001b[A\n",
            "Iteration:  49% 29/59 [00:43<00:44,  1.49s/it]\u001b[A\n",
            "Iteration:  51% 30/59 [00:44<00:43,  1.49s/it]\u001b[A\n",
            "Iteration:  53% 31/59 [00:46<00:41,  1.48s/it]\u001b[A\n",
            "Iteration:  54% 32/59 [00:47<00:40,  1.49s/it]\u001b[A\n",
            "Iteration:  56% 33/59 [00:49<00:39,  1.51s/it]\u001b[A\n",
            "Iteration:  58% 34/59 [00:50<00:38,  1.52s/it]\u001b[A\n",
            "Iteration:  59% 35/59 [00:52<00:36,  1.50s/it]\u001b[A\n",
            "Iteration:  61% 36/59 [00:53<00:34,  1.52s/it]\u001b[A\n",
            "Iteration:  63% 37/59 [00:55<00:33,  1.52s/it]\u001b[A\n",
            "Iteration:  64% 38/59 [00:56<00:31,  1.52s/it]\u001b[A\n",
            "Iteration:  66% 39/59 [00:58<00:30,  1.52s/it]\u001b[A\n",
            "Iteration:  68% 40/59 [00:59<00:28,  1.52s/it]\u001b[A\n",
            "Iteration:  69% 41/59 [01:01<00:27,  1.53s/it]\u001b[A\n",
            "Iteration:  71% 42/59 [01:02<00:25,  1.52s/it]\u001b[A\n",
            "Iteration:  73% 43/59 [01:04<00:24,  1.52s/it]\u001b[A\n",
            "Iteration:  75% 44/59 [01:05<00:22,  1.52s/it]\u001b[A\n",
            "Iteration:  76% 45/59 [01:07<00:21,  1.52s/it]\u001b[A\n",
            "Iteration:  78% 46/59 [01:09<00:19,  1.53s/it]\u001b[A\n",
            "Iteration:  80% 47/59 [01:10<00:18,  1.53s/it]\u001b[A\n",
            "Iteration:  81% 48/59 [01:12<00:16,  1.52s/it]\u001b[A\n",
            "Iteration:  83% 49/59 [01:13<00:15,  1.52s/it]\u001b[A/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "{\"learning_rate\": 4.5762711864406784e-05, \"loss\": 0.33273931354284286, \"step\": 50}\n",
            "\n",
            "Iteration:  85% 50/59 [01:15<00:13,  1.51s/it]\u001b[A\n",
            "Iteration:  86% 51/59 [01:16<00:12,  1.53s/it]\u001b[A\n",
            "Iteration:  88% 52/59 [01:18<00:10,  1.53s/it]\u001b[A\n",
            "Iteration:  90% 53/59 [01:19<00:09,  1.52s/it]\u001b[A\n",
            "Iteration:  92% 54/59 [01:21<00:07,  1.54s/it]\u001b[A\n",
            "Iteration:  93% 55/59 [01:22<00:06,  1.53s/it]\u001b[A\n",
            "Iteration:  95% 56/59 [01:24<00:04,  1.54s/it]\u001b[A\n",
            "Iteration:  97% 57/59 [01:25<00:03,  1.55s/it]\u001b[A\n",
            "Iteration:  98% 58/59 [01:27<00:01,  1.55s/it]\u001b[A\n",
            "Iteration: 100% 59/59 [01:28<00:00,  1.51s/it]\n",
            "Epoch:  10% 1/10 [01:28<13:19, 88.82s/it]\n",
            "Iteration:   0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   2% 1/59 [00:01<01:30,  1.57s/it]\u001b[A\n",
            "Iteration:   3% 2/59 [00:03<01:29,  1.57s/it]\u001b[A\n",
            "Iteration:   5% 3/59 [00:04<01:27,  1.57s/it]\u001b[A\n",
            "Iteration:   7% 4/59 [00:06<01:27,  1.59s/it]\u001b[A\n",
            "Iteration:   8% 5/59 [00:07<01:25,  1.59s/it]\u001b[A\n",
            "Iteration:  10% 6/59 [00:09<01:23,  1.58s/it]\u001b[A\n",
            "Iteration:  12% 7/59 [00:11<01:22,  1.58s/it]\u001b[A\n",
            "Iteration:  14% 8/59 [00:12<01:20,  1.58s/it]\u001b[A\n",
            "Iteration:  15% 9/59 [00:14<01:19,  1.59s/it]\u001b[A\n",
            "Iteration:  17% 10/59 [00:15<01:17,  1.58s/it]\u001b[A\n",
            "Iteration:  19% 11/59 [00:17<01:15,  1.57s/it]\u001b[A\n",
            "Iteration:  20% 12/59 [00:18<01:14,  1.59s/it]\u001b[A\n",
            "Iteration:  22% 13/59 [00:20<01:13,  1.59s/it]\u001b[A\n",
            "Iteration:  24% 14/59 [00:22<01:12,  1.60s/it]\u001b[A\n",
            "Iteration:  25% 15/59 [00:23<01:10,  1.61s/it]\u001b[A\n",
            "Iteration:  27% 16/59 [00:25<01:08,  1.60s/it]\u001b[A\n",
            "Iteration:  29% 17/59 [00:27<01:07,  1.60s/it]\u001b[A\n",
            "Iteration:  31% 18/59 [00:28<01:05,  1.61s/it]\u001b[A\n",
            "Iteration:  32% 19/59 [00:30<01:04,  1.62s/it]\u001b[A\n",
            "Iteration:  34% 20/59 [00:31<01:03,  1.63s/it]\u001b[A\n",
            "Iteration:  36% 21/59 [00:33<01:02,  1.65s/it]\u001b[A\n",
            "Iteration:  37% 22/59 [00:35<01:01,  1.65s/it]\u001b[A\n",
            "Iteration:  39% 23/59 [00:36<00:59,  1.66s/it]\u001b[A\n",
            "Iteration:  41% 24/59 [00:38<00:58,  1.67s/it]\u001b[A\n",
            "Iteration:  42% 25/59 [00:40<00:56,  1.67s/it]\u001b[A\n",
            "Iteration:  44% 26/59 [00:41<00:54,  1.65s/it]\u001b[A\n",
            "Iteration:  46% 27/59 [00:43<00:52,  1.65s/it]\u001b[A\n",
            "Iteration:  47% 28/59 [00:45<00:51,  1.66s/it]\u001b[A\n",
            "Iteration:  49% 29/59 [00:46<00:49,  1.65s/it]\u001b[A\n",
            "Iteration:  51% 30/59 [00:48<00:47,  1.64s/it]\u001b[A\n",
            "Iteration:  53% 31/59 [00:50<00:46,  1.65s/it]\u001b[A\n",
            "Iteration:  54% 32/59 [00:51<00:44,  1.65s/it]\u001b[A\n",
            "Iteration:  56% 33/59 [00:53<00:43,  1.65s/it]\u001b[A\n",
            "Iteration:  58% 34/59 [00:55<00:41,  1.65s/it]\u001b[A\n",
            "Iteration:  59% 35/59 [00:56<00:39,  1.66s/it]\u001b[A\n",
            "Iteration:  61% 36/59 [00:58<00:38,  1.66s/it]\u001b[A\n",
            "Iteration:  63% 37/59 [01:00<00:37,  1.69s/it]\u001b[A\n",
            "Iteration:  64% 38/59 [01:01<00:35,  1.69s/it]\u001b[A\n",
            "Iteration:  66% 39/59 [01:03<00:33,  1.68s/it]\u001b[A\n",
            "Iteration:  68% 40/59 [01:05<00:32,  1.69s/it]\u001b[A{\"learning_rate\": 4.152542372881356e-05, \"loss\": 0.28500696033239364, \"step\": 100}\n",
            "\n",
            "Iteration:  69% 41/59 [01:06<00:30,  1.68s/it]\u001b[A\n",
            "Iteration:  71% 42/59 [01:08<00:29,  1.71s/it]\u001b[A\n",
            "Iteration:  73% 43/59 [01:10<00:27,  1.70s/it]\u001b[A\n",
            "Iteration:  75% 44/59 [01:12<00:25,  1.69s/it]\u001b[A\n",
            "Iteration:  76% 45/59 [01:13<00:23,  1.70s/it]\u001b[A\n",
            "Iteration:  78% 46/59 [01:15<00:21,  1.69s/it]\u001b[A\n",
            "Iteration:  80% 47/59 [01:17<00:20,  1.68s/it]\u001b[A\n",
            "Iteration:  81% 48/59 [01:18<00:18,  1.68s/it]\u001b[A\n",
            "Iteration:  83% 49/59 [01:20<00:16,  1.67s/it]\u001b[A\n",
            "Iteration:  85% 50/59 [01:22<00:14,  1.66s/it]\u001b[A\n",
            "Iteration:  86% 51/59 [01:23<00:13,  1.65s/it]\u001b[A\n",
            "Iteration:  88% 52/59 [01:25<00:11,  1.64s/it]\u001b[A\n",
            "Iteration:  90% 53/59 [01:26<00:09,  1.64s/it]\u001b[A\n",
            "Iteration:  92% 54/59 [01:28<00:08,  1.64s/it]\u001b[A\n",
            "Iteration:  93% 55/59 [01:30<00:06,  1.64s/it]\u001b[A\n",
            "Iteration:  95% 56/59 [01:31<00:04,  1.66s/it]\u001b[A\n",
            "Iteration:  97% 57/59 [01:33<00:03,  1.66s/it]\u001b[A\n",
            "Iteration:  98% 58/59 [01:35<00:01,  1.65s/it]\u001b[A\n",
            "Iteration: 100% 59/59 [01:36<00:00,  1.64s/it]\n",
            "Epoch:  20% 2/10 [03:05<12:09, 91.19s/it]\n",
            "Iteration:   0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   2% 1/59 [00:01<01:37,  1.67s/it]\u001b[A\n",
            "Iteration:   3% 2/59 [00:03<01:34,  1.66s/it]\u001b[A\n",
            "Iteration:   5% 3/59 [00:04<01:33,  1.67s/it]\u001b[A\n",
            "Iteration:   7% 4/59 [00:06<01:32,  1.67s/it]\u001b[A\n",
            "Iteration:   8% 5/59 [00:08<01:29,  1.65s/it]\u001b[A\n",
            "Iteration:  10% 6/59 [00:09<01:27,  1.66s/it]\u001b[A\n",
            "Iteration:  12% 7/59 [00:11<01:25,  1.64s/it]\u001b[A\n",
            "Iteration:  14% 8/59 [00:13<01:23,  1.63s/it]\u001b[A\n",
            "Iteration:  15% 9/59 [00:14<01:22,  1.64s/it]\u001b[A\n",
            "Iteration:  17% 10/59 [00:16<01:20,  1.65s/it]\u001b[A\n",
            "Iteration:  19% 11/59 [00:18<01:18,  1.64s/it]\u001b[A\n",
            "Iteration:  20% 12/59 [00:19<01:16,  1.63s/it]\u001b[A\n",
            "Iteration:  22% 13/59 [00:21<01:15,  1.64s/it]\u001b[A\n",
            "Iteration:  24% 14/59 [00:23<01:14,  1.65s/it]\u001b[A\n",
            "Iteration:  25% 15/59 [00:24<01:12,  1.64s/it]\u001b[A\n",
            "Iteration:  27% 16/59 [00:26<01:10,  1.63s/it]\u001b[A\n",
            "Iteration:  29% 17/59 [00:27<01:08,  1.64s/it]\u001b[A\n",
            "Iteration:  31% 18/59 [00:29<01:07,  1.64s/it]\u001b[A\n",
            "Iteration:  32% 19/59 [00:31<01:06,  1.65s/it]\u001b[A\n",
            "Iteration:  34% 20/59 [00:32<01:04,  1.64s/it]\u001b[A\n",
            "Iteration:  36% 21/59 [00:34<01:02,  1.65s/it]\u001b[A\n",
            "Iteration:  37% 22/59 [00:36<01:01,  1.66s/it]\u001b[A\n",
            "Iteration:  39% 23/59 [00:37<00:59,  1.65s/it]\u001b[A\n",
            "Iteration:  41% 24/59 [00:39<00:57,  1.64s/it]\u001b[A\n",
            "Iteration:  42% 25/59 [00:41<00:56,  1.67s/it]\u001b[A\n",
            "Iteration:  44% 26/59 [00:42<00:55,  1.67s/it]\u001b[A\n",
            "Iteration:  46% 27/59 [00:44<00:53,  1.67s/it]\u001b[A\n",
            "Iteration:  47% 28/59 [00:46<00:51,  1.66s/it]\u001b[A\n",
            "Iteration:  49% 29/59 [00:47<00:50,  1.67s/it]\u001b[A\n",
            "Iteration:  51% 30/59 [00:49<00:49,  1.69s/it]\u001b[A\n",
            "Iteration:  53% 31/59 [00:51<00:47,  1.68s/it]\u001b[A{\"learning_rate\": 3.728813559322034e-05, \"loss\": 0.2221662674844265, \"step\": 150}\n",
            "\n",
            "Iteration:  54% 32/59 [00:53<00:45,  1.68s/it]\u001b[A\n",
            "Iteration:  56% 33/59 [00:54<00:43,  1.68s/it]\u001b[A\n",
            "Iteration:  58% 34/59 [00:56<00:41,  1.67s/it]\u001b[A\n",
            "Iteration:  59% 35/59 [00:58<00:40,  1.69s/it]\u001b[A\n",
            "Iteration:  61% 36/59 [00:59<00:38,  1.68s/it]\u001b[A\n",
            "Iteration:  63% 37/59 [01:01<00:36,  1.67s/it]\u001b[A\n",
            "Iteration:  64% 38/59 [01:03<00:35,  1.67s/it]\u001b[A\n",
            "Iteration:  66% 39/59 [01:04<00:33,  1.67s/it]\u001b[A\n",
            "Iteration:  68% 40/59 [01:06<00:31,  1.66s/it]\u001b[A\n",
            "Iteration:  69% 41/59 [01:07<00:29,  1.65s/it]\u001b[A\n",
            "Iteration:  71% 42/59 [01:09<00:28,  1.67s/it]\u001b[A\n",
            "Iteration:  73% 43/59 [01:11<00:26,  1.66s/it]\u001b[A\n",
            "Iteration:  75% 44/59 [01:12<00:24,  1.66s/it]\u001b[A\n",
            "Iteration:  76% 45/59 [01:14<00:22,  1.64s/it]\u001b[A\n",
            "Iteration:  78% 46/59 [01:16<00:21,  1.65s/it]\u001b[A\n",
            "Iteration:  80% 47/59 [01:17<00:19,  1.65s/it]\u001b[A\n",
            "Iteration:  81% 48/59 [01:19<00:18,  1.65s/it]\u001b[A\n",
            "Iteration:  83% 49/59 [01:21<00:16,  1.64s/it]\u001b[A\n",
            "Iteration:  85% 50/59 [01:22<00:14,  1.66s/it]\u001b[A\n",
            "Iteration:  86% 51/59 [01:24<00:13,  1.65s/it]\u001b[A\n",
            "Iteration:  88% 52/59 [01:26<00:11,  1.66s/it]\u001b[A\n",
            "Iteration:  90% 53/59 [01:27<00:09,  1.66s/it]\u001b[A\n",
            "Iteration:  92% 54/59 [01:29<00:08,  1.65s/it]\u001b[A\n",
            "Iteration:  93% 55/59 [01:31<00:06,  1.64s/it]\u001b[A\n",
            "Iteration:  95% 56/59 [01:32<00:04,  1.65s/it]\u001b[A\n",
            "Iteration:  97% 57/59 [01:34<00:03,  1.64s/it]\u001b[A\n",
            "Iteration:  98% 58/59 [01:36<00:01,  1.66s/it]\u001b[A\n",
            "Iteration: 100% 59/59 [01:37<00:00,  1.65s/it]\n",
            "Epoch:  30% 3/10 [04:43<10:51, 93.09s/it]\n",
            "Iteration:   0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   2% 1/59 [00:01<01:38,  1.70s/it]\u001b[A\n",
            "Iteration:   3% 2/59 [00:03<01:35,  1.67s/it]\u001b[A\n",
            "Iteration:   5% 3/59 [00:04<01:32,  1.66s/it]\u001b[A\n",
            "Iteration:   7% 4/59 [00:06<01:31,  1.66s/it]\u001b[A\n",
            "Iteration:   8% 5/59 [00:08<01:30,  1.67s/it]\u001b[A\n",
            "Iteration:  10% 6/59 [00:09<01:28,  1.66s/it]\u001b[A\n",
            "Iteration:  12% 7/59 [00:11<01:26,  1.67s/it]\u001b[A\n",
            "Iteration:  14% 8/59 [00:13<01:24,  1.67s/it]\u001b[A\n",
            "Iteration:  15% 9/59 [00:14<01:23,  1.68s/it]\u001b[A\n",
            "Iteration:  17% 10/59 [00:16<01:22,  1.68s/it]\u001b[A\n",
            "Iteration:  19% 11/59 [00:18<01:20,  1.67s/it]\u001b[A\n",
            "Iteration:  20% 12/59 [00:19<01:18,  1.67s/it]\u001b[A\n",
            "Iteration:  22% 13/59 [00:21<01:15,  1.65s/it]\u001b[A\n",
            "Iteration:  24% 14/59 [00:23<01:14,  1.66s/it]\u001b[A\n",
            "Iteration:  25% 15/59 [00:24<01:12,  1.65s/it]\u001b[A\n",
            "Iteration:  27% 16/59 [00:26<01:11,  1.65s/it]\u001b[A\n",
            "Iteration:  29% 17/59 [00:28<01:09,  1.66s/it]\u001b[A\n",
            "Iteration:  31% 18/59 [00:29<01:07,  1.65s/it]\u001b[A\n",
            "Iteration:  32% 19/59 [00:31<01:06,  1.66s/it]\u001b[A\n",
            "Iteration:  34% 20/59 [00:33<01:04,  1.65s/it]\u001b[A\n",
            "Iteration:  36% 21/59 [00:34<01:03,  1.66s/it]\u001b[A\n",
            "Iteration:  37% 22/59 [00:36<01:01,  1.67s/it]\u001b[A{\"learning_rate\": 3.305084745762712e-05, \"loss\": 0.16302396632730962, \"step\": 200}\n",
            "\n",
            "Iteration:  39% 23/59 [00:38<00:59,  1.67s/it]\u001b[A\n",
            "Iteration:  41% 24/59 [00:39<00:57,  1.66s/it]\u001b[A\n",
            "Iteration:  42% 25/59 [00:41<00:56,  1.66s/it]\u001b[A\n",
            "Iteration:  44% 26/59 [00:43<00:54,  1.66s/it]\u001b[A\n",
            "Iteration:  46% 27/59 [00:44<00:53,  1.66s/it]\u001b[A\n",
            "Iteration:  47% 28/59 [00:46<00:50,  1.64s/it]\u001b[A\n",
            "Iteration:  49% 29/59 [00:48<00:50,  1.67s/it]\u001b[A\n",
            "Iteration:  51% 30/59 [00:49<00:48,  1.67s/it]\u001b[A\n",
            "Iteration:  53% 31/59 [00:51<00:46,  1.66s/it]\u001b[A\n",
            "Iteration:  54% 32/59 [00:53<00:44,  1.67s/it]\u001b[A\n",
            "Iteration:  56% 33/59 [00:54<00:43,  1.67s/it]\u001b[A\n",
            "Iteration:  58% 34/59 [00:56<00:41,  1.66s/it]\u001b[A\n",
            "Iteration:  59% 35/59 [00:58<00:40,  1.67s/it]\u001b[A\n",
            "Iteration:  61% 36/59 [00:59<00:38,  1.68s/it]\u001b[A\n",
            "Iteration:  63% 37/59 [01:01<00:36,  1.67s/it]\u001b[A\n",
            "Iteration:  64% 38/59 [01:03<00:34,  1.66s/it]\u001b[A\n",
            "Iteration:  66% 39/59 [01:04<00:33,  1.66s/it]\u001b[A\n",
            "Iteration:  68% 40/59 [01:06<00:31,  1.67s/it]\u001b[A\n",
            "Iteration:  69% 41/59 [01:08<00:29,  1.67s/it]\u001b[A\n",
            "Iteration:  71% 42/59 [01:09<00:28,  1.67s/it]\u001b[A\n",
            "Iteration:  73% 43/59 [01:11<00:26,  1.68s/it]\u001b[A\n",
            "Iteration:  75% 44/59 [01:13<00:25,  1.67s/it]\u001b[A\n",
            "Iteration:  76% 45/59 [01:14<00:23,  1.67s/it]\u001b[A\n",
            "Iteration:  78% 46/59 [01:16<00:21,  1.67s/it]\u001b[A\n",
            "Iteration:  80% 47/59 [01:18<00:20,  1.67s/it]\u001b[A\n",
            "Iteration:  81% 48/59 [01:19<00:18,  1.67s/it]\u001b[A\n",
            "Iteration:  83% 49/59 [01:21<00:16,  1.67s/it]\u001b[A\n",
            "Iteration:  85% 50/59 [01:23<00:14,  1.67s/it]\u001b[A\n",
            "Iteration:  86% 51/59 [01:24<00:13,  1.65s/it]\u001b[A\n",
            "Iteration:  88% 52/59 [01:26<00:11,  1.67s/it]\u001b[A\n",
            "Iteration:  90% 53/59 [01:28<00:10,  1.67s/it]\u001b[A\n",
            "Iteration:  92% 54/59 [01:29<00:08,  1.67s/it]\u001b[A\n",
            "Iteration:  93% 55/59 [01:31<00:06,  1.68s/it]\u001b[A\n",
            "Iteration:  95% 56/59 [01:33<00:05,  1.67s/it]\u001b[A\n",
            "Iteration:  97% 57/59 [01:34<00:03,  1.67s/it]\u001b[A\n",
            "Iteration:  98% 58/59 [01:36<00:01,  1.65s/it]\u001b[A\n",
            "Iteration: 100% 59/59 [01:37<00:00,  1.66s/it]\n",
            "Epoch:  40% 4/10 [06:20<09:27, 94.55s/it]\n",
            "Iteration:   0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   2% 1/59 [00:01<01:36,  1.66s/it]\u001b[A\n",
            "Iteration:   3% 2/59 [00:03<01:34,  1.65s/it]\u001b[A\n",
            "Iteration:   5% 3/59 [00:04<01:32,  1.65s/it]\u001b[A\n",
            "Iteration:   7% 4/59 [00:06<01:30,  1.64s/it]\u001b[A\n",
            "Iteration:   8% 5/59 [00:08<01:29,  1.66s/it]\u001b[A\n",
            "Iteration:  10% 6/59 [00:09<01:27,  1.64s/it]\u001b[A\n",
            "Iteration:  12% 7/59 [00:11<01:26,  1.67s/it]\u001b[A\n",
            "Iteration:  14% 8/59 [00:13<01:24,  1.65s/it]\u001b[A\n",
            "Iteration:  15% 9/59 [00:14<01:22,  1.66s/it]\u001b[A\n",
            "Iteration:  17% 10/59 [00:16<01:21,  1.66s/it]\u001b[A\n",
            "Iteration:  19% 11/59 [00:18<01:19,  1.66s/it]\u001b[A\n",
            "Iteration:  20% 12/59 [00:19<01:18,  1.66s/it]\u001b[A\n",
            "Iteration:  22% 13/59 [00:21<01:16,  1.67s/it]\u001b[A{\"learning_rate\": 2.88135593220339e-05, \"loss\": 0.13960952997207643, \"step\": 250}\n",
            "\n",
            "Iteration:  24% 14/59 [00:23<01:15,  1.67s/it]\u001b[A\n",
            "Iteration:  25% 15/59 [00:24<01:13,  1.66s/it]\u001b[A\n",
            "Iteration:  27% 16/59 [00:26<01:11,  1.66s/it]\u001b[A\n",
            "Iteration:  29% 17/59 [00:28<01:09,  1.65s/it]\u001b[A\n",
            "Iteration:  31% 18/59 [00:29<01:07,  1.65s/it]\u001b[A\n",
            "Iteration:  32% 19/59 [00:31<01:06,  1.66s/it]\u001b[A\n",
            "Iteration:  34% 20/59 [00:33<01:04,  1.65s/it]\u001b[A\n",
            "Iteration:  36% 21/59 [00:34<01:03,  1.66s/it]\u001b[A\n",
            "Iteration:  37% 22/59 [00:36<01:01,  1.65s/it]\u001b[A\n",
            "Iteration:  39% 23/59 [00:38<00:59,  1.66s/it]\u001b[A\n",
            "Iteration:  41% 24/59 [00:39<00:58,  1.67s/it]\u001b[A\n",
            "Iteration:  42% 25/59 [00:41<00:57,  1.68s/it]\u001b[A\n",
            "Iteration:  44% 26/59 [00:43<00:55,  1.68s/it]\u001b[A\n",
            "Iteration:  46% 27/59 [00:44<00:53,  1.68s/it]\u001b[A\n",
            "Iteration:  47% 28/59 [00:46<00:51,  1.68s/it]\u001b[A\n",
            "Iteration:  49% 29/59 [00:48<00:50,  1.68s/it]\u001b[A\n",
            "Iteration:  51% 30/59 [00:49<00:48,  1.66s/it]\u001b[A\n",
            "Iteration:  53% 31/59 [00:51<00:46,  1.66s/it]\u001b[A\n",
            "Iteration:  54% 32/59 [00:53<00:44,  1.66s/it]\u001b[A\n",
            "Iteration:  56% 33/59 [00:54<00:43,  1.66s/it]\u001b[A\n",
            "Iteration:  58% 34/59 [00:56<00:41,  1.66s/it]\u001b[A\n",
            "Iteration:  59% 35/59 [00:58<00:39,  1.66s/it]\u001b[A\n",
            "Iteration:  61% 36/59 [00:59<00:38,  1.67s/it]\u001b[A\n",
            "Iteration:  63% 37/59 [01:01<00:36,  1.68s/it]\u001b[A\n",
            "Iteration:  64% 38/59 [01:03<00:35,  1.67s/it]\u001b[A\n",
            "Iteration:  66% 39/59 [01:04<00:33,  1.67s/it]\u001b[A\n",
            "Iteration:  68% 40/59 [01:06<00:31,  1.67s/it]\u001b[A\n",
            "Iteration:  69% 41/59 [01:08<00:30,  1.70s/it]\u001b[A\n",
            "Iteration:  71% 42/59 [01:09<00:28,  1.68s/it]\u001b[A\n",
            "Iteration:  73% 43/59 [01:11<00:26,  1.68s/it]\u001b[A\n",
            "Iteration:  75% 44/59 [01:13<00:25,  1.68s/it]\u001b[A\n",
            "Iteration:  76% 45/59 [01:14<00:23,  1.68s/it]\u001b[A\n",
            "Iteration:  78% 46/59 [01:16<00:21,  1.67s/it]\u001b[A\n",
            "Iteration:  80% 47/59 [01:18<00:19,  1.67s/it]\u001b[A\n",
            "Iteration:  81% 48/59 [01:19<00:18,  1.67s/it]\u001b[A\n",
            "Iteration:  83% 49/59 [01:21<00:16,  1.66s/it]\u001b[A\n",
            "Iteration:  85% 50/59 [01:23<00:14,  1.67s/it]\u001b[A\n",
            "Iteration:  86% 51/59 [01:24<00:13,  1.67s/it]\u001b[A\n",
            "Iteration:  88% 52/59 [01:26<00:11,  1.68s/it]\u001b[A\n",
            "Iteration:  90% 53/59 [01:28<00:09,  1.66s/it]\u001b[A\n",
            "Iteration:  92% 54/59 [01:29<00:08,  1.66s/it]\u001b[A\n",
            "Iteration:  93% 55/59 [01:31<00:06,  1.66s/it]\u001b[A\n",
            "Iteration:  95% 56/59 [01:33<00:04,  1.66s/it]\u001b[A\n",
            "Iteration:  97% 57/59 [01:34<00:03,  1.67s/it]\u001b[A\n",
            "Iteration:  98% 58/59 [01:36<00:01,  1.68s/it]\u001b[A\n",
            "Iteration: 100% 59/59 [01:38<00:00,  1.66s/it]\n",
            "Epoch:  50% 5/10 [07:59<07:58, 95.61s/it]\n",
            "Iteration:   0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   2% 1/59 [00:01<01:36,  1.66s/it]\u001b[A\n",
            "Iteration:   3% 2/59 [00:03<01:35,  1.67s/it]\u001b[A\n",
            "Iteration:   5% 3/59 [00:04<01:33,  1.66s/it]\u001b[A\n",
            "Iteration:   7% 4/59 [00:06<01:31,  1.66s/it]\u001b[A{\"learning_rate\": 2.457627118644068e-05, \"loss\": 0.07817897785454989, \"step\": 300}\n",
            "\n",
            "Iteration:   8% 5/59 [00:08<01:29,  1.65s/it]\u001b[A\n",
            "Iteration:  10% 6/59 [00:09<01:28,  1.66s/it]\u001b[A\n",
            "Iteration:  12% 7/59 [00:11<01:26,  1.65s/it]\u001b[A\n",
            "Iteration:  14% 8/59 [00:13<01:24,  1.67s/it]\u001b[A\n",
            "Iteration:  15% 9/59 [00:14<01:23,  1.67s/it]\u001b[A\n",
            "Iteration:  17% 10/59 [00:16<01:21,  1.67s/it]\u001b[A\n",
            "Iteration:  19% 11/59 [00:18<01:20,  1.67s/it]\u001b[A\n",
            "Iteration:  20% 12/59 [00:19<01:18,  1.67s/it]\u001b[A\n",
            "Iteration:  22% 13/59 [00:21<01:17,  1.69s/it]\u001b[A\n",
            "Iteration:  24% 14/59 [00:23<01:15,  1.67s/it]\u001b[A\n",
            "Iteration:  25% 15/59 [00:25<01:13,  1.67s/it]\u001b[A\n",
            "Iteration:  27% 16/59 [00:26<01:12,  1.68s/it]\u001b[A\n",
            "Iteration:  29% 17/59 [00:28<01:10,  1.67s/it]\u001b[A\n",
            "Iteration:  31% 18/59 [00:30<01:08,  1.67s/it]\u001b[A\n",
            "Iteration:  32% 19/59 [00:31<01:06,  1.66s/it]\u001b[A\n",
            "Iteration:  34% 20/59 [00:33<01:04,  1.66s/it]\u001b[A\n",
            "Iteration:  36% 21/59 [00:35<01:03,  1.67s/it]\u001b[A\n",
            "Iteration:  37% 22/59 [00:36<01:01,  1.67s/it]\u001b[A\n",
            "Iteration:  39% 23/59 [00:38<00:59,  1.66s/it]\u001b[A\n",
            "Iteration:  41% 24/59 [00:40<00:58,  1.67s/it]\u001b[A\n",
            "Iteration:  42% 25/59 [00:41<00:56,  1.66s/it]\u001b[A\n",
            "Iteration:  44% 26/59 [00:43<00:54,  1.67s/it]\u001b[A\n",
            "Iteration:  46% 27/59 [00:44<00:52,  1.66s/it]\u001b[A\n",
            "Iteration:  47% 28/59 [00:46<00:51,  1.66s/it]\u001b[A\n",
            "Iteration:  49% 29/59 [00:48<00:50,  1.67s/it]\u001b[A\n",
            "Iteration:  51% 30/59 [00:50<00:48,  1.68s/it]\u001b[A\n",
            "Iteration:  53% 31/59 [00:51<00:46,  1.67s/it]\u001b[A\n",
            "Iteration:  54% 32/59 [00:53<00:44,  1.65s/it]\u001b[A\n",
            "Iteration:  56% 33/59 [00:54<00:43,  1.66s/it]\u001b[A\n",
            "Iteration:  58% 34/59 [00:56<00:41,  1.65s/it]\u001b[A\n",
            "Iteration:  59% 35/59 [00:58<00:39,  1.66s/it]\u001b[A\n",
            "Iteration:  61% 36/59 [00:59<00:37,  1.64s/it]\u001b[A\n",
            "Iteration:  63% 37/59 [01:01<00:36,  1.66s/it]\u001b[A\n",
            "Iteration:  64% 38/59 [01:03<00:35,  1.67s/it]\u001b[A\n",
            "Iteration:  66% 39/59 [01:04<00:33,  1.66s/it]\u001b[A\n",
            "Iteration:  68% 40/59 [01:06<00:31,  1.68s/it]\u001b[A\n",
            "Iteration:  69% 41/59 [01:08<00:30,  1.67s/it]\u001b[A\n",
            "Iteration:  71% 42/59 [01:09<00:28,  1.66s/it]\u001b[A\n",
            "Iteration:  73% 43/59 [01:11<00:26,  1.66s/it]\u001b[A\n",
            "Iteration:  75% 44/59 [01:13<00:24,  1.66s/it]\u001b[A\n",
            "Iteration:  76% 45/59 [01:14<00:23,  1.66s/it]\u001b[A\n",
            "Iteration:  78% 46/59 [01:16<00:21,  1.66s/it]\u001b[A\n",
            "Iteration:  80% 47/59 [01:18<00:19,  1.65s/it]\u001b[A\n",
            "Iteration:  81% 48/59 [01:19<00:18,  1.66s/it]\u001b[A\n",
            "Iteration:  83% 49/59 [01:21<00:16,  1.67s/it]\u001b[A\n",
            "Iteration:  85% 50/59 [01:23<00:15,  1.68s/it]\u001b[A\n",
            "Iteration:  86% 51/59 [01:24<00:13,  1.69s/it]\u001b[A\n",
            "Iteration:  88% 52/59 [01:26<00:11,  1.69s/it]\u001b[A\n",
            "Iteration:  90% 53/59 [01:28<00:10,  1.69s/it]\u001b[A\n",
            "Iteration:  92% 54/59 [01:29<00:08,  1.66s/it]\u001b[A{\"learning_rate\": 2.033898305084746e-05, \"loss\": 0.0556631587818265, \"step\": 350}\n",
            "\n",
            "Iteration:  93% 55/59 [01:31<00:06,  1.67s/it]\u001b[A\n",
            "Iteration:  95% 56/59 [01:33<00:05,  1.68s/it]\u001b[A\n",
            "Iteration:  97% 57/59 [01:34<00:03,  1.66s/it]\u001b[A\n",
            "Iteration:  98% 58/59 [01:36<00:01,  1.66s/it]\u001b[A\n",
            "Iteration: 100% 59/59 [01:38<00:00,  1.66s/it]\n",
            "Epoch:  60% 6/10 [09:37<06:25, 96.34s/it]\n",
            "Iteration:   0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   2% 1/59 [00:01<01:37,  1.68s/it]\u001b[A\n",
            "Iteration:   3% 2/59 [00:03<01:35,  1.68s/it]\u001b[A\n",
            "Iteration:   5% 3/59 [00:05<01:34,  1.69s/it]\u001b[A\n",
            "Iteration:   7% 4/59 [00:06<01:32,  1.68s/it]\u001b[A\n",
            "Iteration:   8% 5/59 [00:08<01:30,  1.67s/it]\u001b[A\n",
            "Iteration:  10% 6/59 [00:10<01:28,  1.67s/it]\u001b[A\n",
            "Iteration:  12% 7/59 [00:11<01:26,  1.67s/it]\u001b[A\n",
            "Iteration:  14% 8/59 [00:13<01:25,  1.67s/it]\u001b[A\n",
            "Iteration:  15% 9/59 [00:15<01:23,  1.68s/it]\u001b[A\n",
            "Iteration:  17% 10/59 [00:16<01:22,  1.68s/it]\u001b[A\n",
            "Iteration:  19% 11/59 [00:18<01:20,  1.68s/it]\u001b[A\n",
            "Iteration:  20% 12/59 [00:20<01:18,  1.67s/it]\u001b[A\n",
            "Iteration:  22% 13/59 [00:21<01:16,  1.66s/it]\u001b[A\n",
            "Iteration:  24% 14/59 [00:23<01:15,  1.68s/it]\u001b[A\n",
            "Iteration:  25% 15/59 [00:25<01:13,  1.68s/it]\u001b[A\n",
            "Iteration:  27% 16/59 [00:26<01:11,  1.66s/it]\u001b[A\n",
            "Iteration:  29% 17/59 [00:28<01:10,  1.68s/it]\u001b[A\n",
            "Iteration:  31% 18/59 [00:30<01:08,  1.67s/it]\u001b[A\n",
            "Iteration:  32% 19/59 [00:31<01:06,  1.65s/it]\u001b[A\n",
            "Iteration:  34% 20/59 [00:33<01:04,  1.66s/it]\u001b[A\n",
            "Iteration:  36% 21/59 [00:35<01:03,  1.66s/it]\u001b[A\n",
            "Iteration:  37% 22/59 [00:36<01:02,  1.68s/it]\u001b[A\n",
            "Iteration:  39% 23/59 [00:38<01:00,  1.68s/it]\u001b[A\n",
            "Iteration:  41% 24/59 [00:40<00:58,  1.67s/it]\u001b[A\n",
            "Iteration:  42% 25/59 [00:41<00:56,  1.66s/it]\u001b[A\n",
            "Iteration:  44% 26/59 [00:43<00:55,  1.67s/it]\u001b[A\n",
            "Iteration:  46% 27/59 [00:45<00:53,  1.66s/it]\u001b[A\n",
            "Iteration:  47% 28/59 [00:46<00:51,  1.67s/it]\u001b[A\n",
            "Iteration:  49% 29/59 [00:48<00:50,  1.67s/it]\u001b[A\n",
            "Iteration:  51% 30/59 [00:50<00:48,  1.67s/it]\u001b[A\n",
            "Iteration:  53% 31/59 [00:51<00:46,  1.68s/it]\u001b[A\n",
            "Iteration:  54% 32/59 [00:53<00:44,  1.66s/it]\u001b[A\n",
            "Iteration:  56% 33/59 [00:55<00:43,  1.66s/it]\u001b[A\n",
            "Iteration:  58% 34/59 [00:56<00:41,  1.66s/it]\u001b[A\n",
            "Iteration:  59% 35/59 [00:58<00:39,  1.66s/it]\u001b[A\n",
            "Iteration:  61% 36/59 [01:00<00:38,  1.66s/it]\u001b[A\n",
            "Iteration:  63% 37/59 [01:01<00:36,  1.65s/it]\u001b[A\n",
            "Iteration:  64% 38/59 [01:03<00:34,  1.66s/it]\u001b[A\n",
            "Iteration:  66% 39/59 [01:05<00:33,  1.65s/it]\u001b[A\n",
            "Iteration:  68% 40/59 [01:06<00:31,  1.66s/it]\u001b[A\n",
            "Iteration:  69% 41/59 [01:08<00:29,  1.65s/it]\u001b[A\n",
            "Iteration:  71% 42/59 [01:09<00:28,  1.66s/it]\u001b[A\n",
            "Iteration:  73% 43/59 [01:11<00:26,  1.66s/it]\u001b[A\n",
            "Iteration:  75% 44/59 [01:13<00:24,  1.65s/it]\u001b[A\n",
            "Iteration:  76% 45/59 [01:14<00:23,  1.66s/it]\u001b[A{\"learning_rate\": 1.6101694915254237e-05, \"loss\": 0.02947233946993947, \"step\": 400}\n",
            "\n",
            "Iteration:  78% 46/59 [01:16<00:21,  1.65s/it]\u001b[A\n",
            "Iteration:  80% 47/59 [01:18<00:20,  1.67s/it]\u001b[A\n",
            "Iteration:  81% 48/59 [01:19<00:18,  1.66s/it]\u001b[A\n",
            "Iteration:  83% 49/59 [01:21<00:16,  1.67s/it]\u001b[A\n",
            "Iteration:  85% 50/59 [01:23<00:15,  1.67s/it]\u001b[A\n",
            "Iteration:  86% 51/59 [01:24<00:13,  1.66s/it]\u001b[A\n",
            "Iteration:  88% 52/59 [01:26<00:11,  1.67s/it]\u001b[A\n",
            "Iteration:  90% 53/59 [01:28<00:10,  1.67s/it]\u001b[A\n",
            "Iteration:  92% 54/59 [01:29<00:08,  1.66s/it]\u001b[A\n",
            "Iteration:  93% 55/59 [01:31<00:06,  1.66s/it]\u001b[A\n",
            "Iteration:  95% 56/59 [01:33<00:05,  1.68s/it]\u001b[A\n",
            "Iteration:  97% 57/59 [01:34<00:03,  1.67s/it]\u001b[A\n",
            "Iteration:  98% 58/59 [01:36<00:01,  1.66s/it]\u001b[A\n",
            "Iteration: 100% 59/59 [01:38<00:00,  1.66s/it]\n",
            "Epoch:  70% 7/10 [11:15<04:50, 96.86s/it]\n",
            "Iteration:   0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   2% 1/59 [00:01<01:37,  1.68s/it]\u001b[A\n",
            "Iteration:   3% 2/59 [00:03<01:35,  1.67s/it]\u001b[A\n",
            "Iteration:   5% 3/59 [00:04<01:33,  1.66s/it]\u001b[A\n",
            "Iteration:   7% 4/59 [00:06<01:31,  1.67s/it]\u001b[A\n",
            "Iteration:   8% 5/59 [00:08<01:30,  1.67s/it]\u001b[A\n",
            "Iteration:  10% 6/59 [00:09<01:28,  1.67s/it]\u001b[A\n",
            "Iteration:  12% 7/59 [00:11<01:27,  1.67s/it]\u001b[A\n",
            "Iteration:  14% 8/59 [00:13<01:25,  1.68s/it]\u001b[A\n",
            "Iteration:  15% 9/59 [00:15<01:24,  1.69s/it]\u001b[A\n",
            "Iteration:  17% 10/59 [00:16<01:21,  1.66s/it]\u001b[A\n",
            "Iteration:  19% 11/59 [00:18<01:19,  1.66s/it]\u001b[A\n",
            "Iteration:  20% 12/59 [00:19<01:17,  1.66s/it]\u001b[A\n",
            "Iteration:  22% 13/59 [00:21<01:16,  1.67s/it]\u001b[A\n",
            "Iteration:  24% 14/59 [00:23<01:15,  1.68s/it]\u001b[A\n",
            "Iteration:  25% 15/59 [00:25<01:13,  1.66s/it]\u001b[A\n",
            "Iteration:  27% 16/59 [00:26<01:11,  1.67s/it]\u001b[A\n",
            "Iteration:  29% 17/59 [00:28<01:09,  1.67s/it]\u001b[A\n",
            "Iteration:  31% 18/59 [00:29<01:08,  1.66s/it]\u001b[A\n",
            "Iteration:  32% 19/59 [00:31<01:06,  1.66s/it]\u001b[A\n",
            "Iteration:  34% 20/59 [00:33<01:04,  1.66s/it]\u001b[A\n",
            "Iteration:  36% 21/59 [00:34<01:02,  1.66s/it]\u001b[A\n",
            "Iteration:  37% 22/59 [00:36<01:01,  1.67s/it]\u001b[A\n",
            "Iteration:  39% 23/59 [00:38<01:00,  1.68s/it]\u001b[A\n",
            "Iteration:  41% 24/59 [00:39<00:58,  1.67s/it]\u001b[A\n",
            "Iteration:  42% 25/59 [00:41<00:57,  1.69s/it]\u001b[A\n",
            "Iteration:  44% 26/59 [00:43<00:55,  1.68s/it]\u001b[A\n",
            "Iteration:  46% 27/59 [00:45<00:53,  1.68s/it]\u001b[A\n",
            "Iteration:  47% 28/59 [00:46<00:51,  1.67s/it]\u001b[A\n",
            "Iteration:  49% 29/59 [00:48<00:50,  1.67s/it]\u001b[A\n",
            "Iteration:  51% 30/59 [00:50<00:48,  1.66s/it]\u001b[A\n",
            "Iteration:  53% 31/59 [00:51<00:46,  1.67s/it]\u001b[A\n",
            "Iteration:  54% 32/59 [00:53<00:44,  1.66s/it]\u001b[A\n",
            "Iteration:  56% 33/59 [00:55<00:43,  1.66s/it]\u001b[A\n",
            "Iteration:  58% 34/59 [00:56<00:41,  1.65s/it]\u001b[A\n",
            "Iteration:  59% 35/59 [00:58<00:39,  1.65s/it]\u001b[A\n",
            "Iteration:  61% 36/59 [01:00<00:38,  1.67s/it]\u001b[A{\"learning_rate\": 1.1864406779661018e-05, \"loss\": 0.020764634842053056, \"step\": 450}\n",
            "\n",
            "Iteration:  63% 37/59 [01:01<00:36,  1.67s/it]\u001b[A\n",
            "Iteration:  64% 38/59 [01:03<00:35,  1.67s/it]\u001b[A\n",
            "Iteration:  66% 39/59 [01:05<00:33,  1.66s/it]\u001b[A\n",
            "Iteration:  68% 40/59 [01:06<00:31,  1.66s/it]\u001b[A\n",
            "Iteration:  69% 41/59 [01:08<00:30,  1.67s/it]\u001b[A\n",
            "Iteration:  71% 42/59 [01:10<00:28,  1.66s/it]\u001b[A\n",
            "Iteration:  73% 43/59 [01:11<00:26,  1.65s/it]\u001b[A\n",
            "Iteration:  75% 44/59 [01:13<00:24,  1.65s/it]\u001b[A\n",
            "Iteration:  76% 45/59 [01:14<00:23,  1.65s/it]\u001b[A\n",
            "Iteration:  78% 46/59 [01:16<00:21,  1.66s/it]\u001b[A\n",
            "Iteration:  80% 47/59 [01:18<00:20,  1.68s/it]\u001b[A\n",
            "Iteration:  81% 48/59 [01:20<00:18,  1.68s/it]\u001b[A\n",
            "Iteration:  83% 49/59 [01:21<00:16,  1.67s/it]\u001b[A\n",
            "Iteration:  85% 50/59 [01:23<00:15,  1.67s/it]\u001b[A\n",
            "Iteration:  86% 51/59 [01:24<00:13,  1.66s/it]\u001b[A\n",
            "Iteration:  88% 52/59 [01:26<00:11,  1.66s/it]\u001b[A\n",
            "Iteration:  90% 53/59 [01:28<00:10,  1.67s/it]\u001b[A\n",
            "Iteration:  92% 54/59 [01:29<00:08,  1.67s/it]\u001b[A\n",
            "Iteration:  93% 55/59 [01:31<00:06,  1.66s/it]\u001b[A\n",
            "Iteration:  95% 56/59 [01:33<00:04,  1.66s/it]\u001b[A\n",
            "Iteration:  97% 57/59 [01:34<00:03,  1.65s/it]\u001b[A\n",
            "Iteration:  98% 58/59 [01:36<00:01,  1.65s/it]\u001b[A\n",
            "Iteration: 100% 59/59 [01:37<00:00,  1.66s/it]\n",
            "Epoch:  80% 8/10 [12:53<03:14, 97.20s/it]\n",
            "Iteration:   0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   2% 1/59 [00:01<01:39,  1.72s/it]\u001b[A\n",
            "Iteration:   3% 2/59 [00:03<01:36,  1.70s/it]\u001b[A\n",
            "Iteration:   5% 3/59 [00:05<01:34,  1.69s/it]\u001b[A\n",
            "Iteration:   7% 4/59 [00:06<01:32,  1.69s/it]\u001b[A\n",
            "Iteration:   8% 5/59 [00:08<01:31,  1.69s/it]\u001b[A\n",
            "Iteration:  10% 6/59 [00:10<01:29,  1.69s/it]\u001b[A\n",
            "Iteration:  12% 7/59 [00:11<01:27,  1.68s/it]\u001b[A\n",
            "Iteration:  14% 8/59 [00:13<01:24,  1.66s/it]\u001b[A\n",
            "Iteration:  15% 9/59 [00:15<01:22,  1.65s/it]\u001b[A\n",
            "Iteration:  17% 10/59 [00:16<01:21,  1.66s/it]\u001b[A\n",
            "Iteration:  19% 11/59 [00:18<01:19,  1.66s/it]\u001b[A\n",
            "Iteration:  20% 12/59 [00:20<01:18,  1.66s/it]\u001b[A\n",
            "Iteration:  22% 13/59 [00:21<01:15,  1.64s/it]\u001b[A\n",
            "Iteration:  24% 14/59 [00:23<01:14,  1.65s/it]\u001b[A\n",
            "Iteration:  25% 15/59 [00:24<01:12,  1.64s/it]\u001b[A\n",
            "Iteration:  27% 16/59 [00:26<01:10,  1.65s/it]\u001b[A\n",
            "Iteration:  29% 17/59 [00:28<01:09,  1.65s/it]\u001b[A\n",
            "Iteration:  31% 18/59 [00:29<01:08,  1.66s/it]\u001b[A\n",
            "Iteration:  32% 19/59 [00:31<01:06,  1.65s/it]\u001b[A\n",
            "Iteration:  34% 20/59 [00:33<01:04,  1.65s/it]\u001b[A\n",
            "Iteration:  36% 21/59 [00:34<01:03,  1.66s/it]\u001b[A\n",
            "Iteration:  37% 22/59 [00:36<01:01,  1.66s/it]\u001b[A\n",
            "Iteration:  39% 23/59 [00:38<01:00,  1.67s/it]\u001b[A\n",
            "Iteration:  41% 24/59 [00:39<00:58,  1.67s/it]\u001b[A\n",
            "Iteration:  42% 25/59 [00:41<00:56,  1.65s/it]\u001b[A\n",
            "Iteration:  44% 26/59 [00:43<00:55,  1.67s/it]\u001b[A\n",
            "Iteration:  46% 27/59 [00:44<00:53,  1.67s/it]\u001b[A{\"learning_rate\": 7.627118644067798e-06, \"loss\": 0.011524382065981626, \"step\": 500}\n",
            "\n",
            "Iteration:  47% 28/59 [00:46<00:51,  1.66s/it]\u001b[A\n",
            "Iteration:  49% 29/59 [00:48<00:49,  1.64s/it]\u001b[A\n",
            "Iteration:  51% 30/59 [00:49<00:47,  1.65s/it]\u001b[A\n",
            "Iteration:  53% 31/59 [00:51<00:46,  1.66s/it]\u001b[A\n",
            "Iteration:  54% 32/59 [00:53<00:45,  1.67s/it]\u001b[A\n",
            "Iteration:  56% 33/59 [00:54<00:43,  1.66s/it]\u001b[A\n",
            "Iteration:  58% 34/59 [00:56<00:41,  1.66s/it]\u001b[A\n",
            "Iteration:  59% 35/59 [00:58<00:40,  1.67s/it]\u001b[A\n",
            "Iteration:  61% 36/59 [00:59<00:38,  1.66s/it]\u001b[A\n",
            "Iteration:  63% 37/59 [01:01<00:36,  1.67s/it]\u001b[A\n",
            "Iteration:  64% 38/59 [01:03<00:35,  1.67s/it]\u001b[A\n",
            "Iteration:  66% 39/59 [01:04<00:33,  1.66s/it]\u001b[A\n",
            "Iteration:  68% 40/59 [01:06<00:31,  1.65s/it]\u001b[A\n",
            "Iteration:  69% 41/59 [01:08<00:29,  1.64s/it]\u001b[A\n",
            "Iteration:  71% 42/59 [01:09<00:28,  1.65s/it]\u001b[A\n",
            "Iteration:  73% 43/59 [01:11<00:26,  1.67s/it]\u001b[A\n",
            "Iteration:  75% 44/59 [01:13<00:25,  1.67s/it]\u001b[A\n",
            "Iteration:  76% 45/59 [01:14<00:23,  1.67s/it]\u001b[A\n",
            "Iteration:  78% 46/59 [01:16<00:21,  1.67s/it]\u001b[A\n",
            "Iteration:  80% 47/59 [01:18<00:20,  1.68s/it]\u001b[A\n",
            "Iteration:  81% 48/59 [01:19<00:18,  1.69s/it]\u001b[A\n",
            "Iteration:  83% 49/59 [01:21<00:16,  1.67s/it]\u001b[A\n",
            "Iteration:  85% 50/59 [01:23<00:14,  1.67s/it]\u001b[A\n",
            "Iteration:  86% 51/59 [01:24<00:13,  1.69s/it]\u001b[A\n",
            "Iteration:  88% 52/59 [01:26<00:11,  1.67s/it]\u001b[A\n",
            "Iteration:  90% 53/59 [01:28<00:10,  1.67s/it]\u001b[A\n",
            "Iteration:  92% 54/59 [01:29<00:08,  1.67s/it]\u001b[A\n",
            "Iteration:  93% 55/59 [01:31<00:06,  1.67s/it]\u001b[A\n",
            "Iteration:  95% 56/59 [01:33<00:05,  1.67s/it]\u001b[A\n",
            "Iteration:  97% 57/59 [01:34<00:03,  1.66s/it]\u001b[A\n",
            "Iteration:  98% 58/59 [01:36<00:01,  1.66s/it]\u001b[A\n",
            "Iteration: 100% 59/59 [01:37<00:00,  1.66s/it]\n",
            "Epoch:  90% 9/10 [14:31<01:37, 97.42s/it]\n",
            "Iteration:   0% 0/59 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   2% 1/59 [00:01<01:35,  1.65s/it]\u001b[A\n",
            "Iteration:   3% 2/59 [00:03<01:34,  1.66s/it]\u001b[A\n",
            "Iteration:   5% 3/59 [00:04<01:32,  1.66s/it]\u001b[A\n",
            "Iteration:   7% 4/59 [00:06<01:31,  1.66s/it]\u001b[A\n",
            "Iteration:   8% 5/59 [00:08<01:29,  1.66s/it]\u001b[A\n",
            "Iteration:  10% 6/59 [00:09<01:27,  1.66s/it]\u001b[A\n",
            "Iteration:  12% 7/59 [00:11<01:26,  1.65s/it]\u001b[A\n",
            "Iteration:  14% 8/59 [00:13<01:24,  1.65s/it]\u001b[A\n",
            "Iteration:  15% 9/59 [00:14<01:23,  1.66s/it]\u001b[A\n",
            "Iteration:  17% 10/59 [00:16<01:21,  1.65s/it]\u001b[A\n",
            "Iteration:  19% 11/59 [00:18<01:19,  1.66s/it]\u001b[A\n",
            "Iteration:  20% 12/59 [00:19<01:17,  1.65s/it]\u001b[A\n",
            "Iteration:  22% 13/59 [00:21<01:16,  1.66s/it]\u001b[A\n",
            "Iteration:  24% 14/59 [00:23<01:14,  1.66s/it]\u001b[A\n",
            "Iteration:  25% 15/59 [00:24<01:13,  1.66s/it]\u001b[A\n",
            "Iteration:  27% 16/59 [00:26<01:11,  1.66s/it]\u001b[A\n",
            "Iteration:  29% 17/59 [00:28<01:09,  1.67s/it]\u001b[A\n",
            "Iteration:  31% 18/59 [00:29<01:07,  1.66s/it]\u001b[A{\"learning_rate\": 3.3898305084745763e-06, \"loss\": 0.010970658762380481, \"step\": 550}\n",
            "\n",
            "Iteration:  32% 19/59 [00:31<01:06,  1.66s/it]\u001b[A\n",
            "Iteration:  34% 20/59 [00:33<01:04,  1.66s/it]\u001b[A\n",
            "Iteration:  36% 21/59 [00:34<01:03,  1.66s/it]\u001b[A\n",
            "Iteration:  37% 22/59 [00:36<01:01,  1.66s/it]\u001b[A\n",
            "Iteration:  39% 23/59 [00:38<00:59,  1.66s/it]\u001b[A\n",
            "Iteration:  41% 24/59 [00:39<00:57,  1.64s/it]\u001b[A\n",
            "Iteration:  42% 25/59 [00:41<00:56,  1.66s/it]\u001b[A\n",
            "Iteration:  44% 26/59 [00:43<00:54,  1.66s/it]\u001b[A\n",
            "Iteration:  46% 27/59 [00:44<00:53,  1.66s/it]\u001b[A\n",
            "Iteration:  47% 28/59 [00:46<00:51,  1.66s/it]\u001b[A\n",
            "Iteration:  49% 29/59 [00:48<00:49,  1.66s/it]\u001b[A\n",
            "Iteration:  51% 30/59 [00:49<00:48,  1.66s/it]\u001b[A\n",
            "Iteration:  53% 31/59 [00:51<00:46,  1.65s/it]\u001b[A\n",
            "Iteration:  54% 32/59 [00:53<00:45,  1.67s/it]\u001b[A\n",
            "Iteration:  56% 33/59 [00:54<00:43,  1.68s/it]\u001b[A\n",
            "Iteration:  58% 34/59 [00:56<00:41,  1.67s/it]\u001b[A\n",
            "Iteration:  59% 35/59 [00:58<00:40,  1.67s/it]\u001b[A\n",
            "Iteration:  61% 36/59 [00:59<00:38,  1.67s/it]\u001b[A\n",
            "Iteration:  63% 37/59 [01:01<00:36,  1.67s/it]\u001b[A\n",
            "Iteration:  64% 38/59 [01:03<00:34,  1.66s/it]\u001b[A\n",
            "Iteration:  66% 39/59 [01:04<00:33,  1.66s/it]\u001b[A\n",
            "Iteration:  68% 40/59 [01:06<00:31,  1.67s/it]\u001b[A\n",
            "Iteration:  69% 41/59 [01:08<00:29,  1.66s/it]\u001b[A\n",
            "Iteration:  71% 42/59 [01:09<00:28,  1.66s/it]\u001b[A\n",
            "Iteration:  73% 43/59 [01:11<00:26,  1.66s/it]\u001b[A\n",
            "Iteration:  75% 44/59 [01:13<00:24,  1.65s/it]\u001b[A\n",
            "Iteration:  76% 45/59 [01:14<00:23,  1.67s/it]\u001b[A\n",
            "Iteration:  78% 46/59 [01:16<00:21,  1.66s/it]\u001b[A\n",
            "Iteration:  80% 47/59 [01:18<00:19,  1.65s/it]\u001b[A\n",
            "Iteration:  81% 48/59 [01:19<00:18,  1.65s/it]\u001b[A\n",
            "Iteration:  83% 49/59 [01:21<00:16,  1.65s/it]\u001b[A\n",
            "Iteration:  85% 50/59 [01:22<00:14,  1.65s/it]\u001b[A\n",
            "Iteration:  86% 51/59 [01:24<00:13,  1.66s/it]\u001b[A\n",
            "Iteration:  88% 52/59 [01:26<00:11,  1.66s/it]\u001b[A\n",
            "Iteration:  90% 53/59 [01:28<00:10,  1.67s/it]\u001b[A\n",
            "Iteration:  92% 54/59 [01:29<00:08,  1.67s/it]\u001b[A\n",
            "Iteration:  93% 55/59 [01:31<00:06,  1.67s/it]\u001b[A\n",
            "Iteration:  95% 56/59 [01:33<00:05,  1.68s/it]\u001b[A\n",
            "Iteration:  97% 57/59 [01:34<00:03,  1.67s/it]\u001b[A\n",
            "Iteration:  98% 58/59 [01:36<00:01,  1.67s/it]\u001b[A\n",
            "Iteration: 100% 59/59 [01:37<00:00,  1.66s/it]\n",
            "Epoch: 100% 10/10 [16:08<00:00, 96.89s/it]\n",
            "10/17/2020 17:24:06 - INFO - __main__ -    global_step = 590, average loss = 0.11504282171490712\n",
            "10/17/2020 17:24:06 - INFO - utils_ichi -   Creating features from dataset file at ./data/ichi\n",
            "10/17/2020 17:24:09 - INFO - utils_ichi -   Writing example 0/300\n",
            "10/17/2020 17:24:09 - INFO - utils_ichi -   *** Example ***\n",
            "10/17/2020 17:24:09 - INFO - utils_ichi -   guid: dev-1\n",
            "10/17/2020 17:24:09 - INFO - utils_ichi -   input_global_ids: 101 18348 2187 2849 1010 2200 5729 2009 8450 1010 2046 3917 3085 2009 8450 1010 1998 14954 6881 5544 1012 2036 2312 2601 24851 2006 2192 1012 2323 2022 7917 1012 102 18348 102 2009 8450 102 2187 2849 102 2192 102 5544 102 24851 102 2200 5729 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:24:09 - INFO - utils_ichi -   attention_mask_global: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:24:09 - INFO - utils_ichi -   token_global_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:24:09 - INFO - utils_ichi -   input_local_ids: 101 18348 2187 2849 1010 2200 5729 2009 8450 1010 2046 3917 3085 2009 8450 1010 1998 14954 6881 5544 1012 2036 2312 2601 24851 2006 2192 1012 2323 2022 7917 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:24:09 - INFO - utils_ichi -   attention_mask_local: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:24:09 - INFO - utils_ichi -   token_local_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:24:09 - INFO - utils_ichi -   text_clean_indices: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 854 152 1314 105 5764 4478 5764 4611 5569 5596 154 6108 726 4717 4531 3467 9097\n",
            "10/17/2020 17:24:09 - INFO - utils_ichi -   aspect_indices: 18348 2009 8450 2187 2849 2192 5544 24851 2200 5729 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:24:09 - INFO - utils_ichi -   label: [0, 1, 0, 0, 0] (id = [0, 1, 0, 0, 0])\n",
            "10/17/2020 17:24:09 - INFO - utils_ichi -   *** Example ***\n",
            "10/17/2020 17:24:09 - INFO - utils_ichi -   guid: dev-2\n",
            "10/17/2020 17:24:09 - INFO - utils_ichi -   input_global_ids: 101 3083 17357 2579 2007 2833 1010 1037 2261 2847 2044 1045 5281 2460 2791 1997 3052 1010 1037 3168 1997 6245 1010 13675 16613 2075 1010 6314 4308 2063 2097 2644 2635 3202 1012 9826 2064 2025 16755 2023 4319 1010 1045 2572 2145 13417 2217 3896 2074 2044 1015 17357 1998 10047 4011 2000 2202 3807 1013 2154 4293 24798 1012 1045 2074 2064 2102 2562 2183 2031 2000 3046 2242 2842 1012 102 2217 3896 102 6245 102 13675 16613 2075 102 6314 102 5281 102 2833 102 2833 102 4319 102 2154 102 3202 102 2460 2791 1997 3052 102 2242 2842 102 4308 2063 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:24:09 - INFO - utils_ichi -   attention_mask_global: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:24:09 - INFO - utils_ichi -   token_global_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:24:09 - INFO - utils_ichi -   input_local_ids: 101 3083 17357 2579 2007 2833 1010 1037 2261 2847 2044 1045 5281 2460 2791 1997 3052 1010 1037 3168 1997 6245 1010 13675 16613 2075 1010 6314 4308 2063 2097 2644 2635 3202 1012 9826 2064 2025 16755 2023 4319 1010 1045 2572 2145 13417 2217 3896 2074 2044 1015 17357 1998 10047 4011 2000 2202 3807 1013 2154 4293 24798 1012 1045 2074 2064 2102 2562 2183 2031 2000 3046 2242 2842 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:24:09 - INFO - utils_ichi -   attention_mask_local: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:24:09 - INFO - utils_ichi -   token_local_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:24:09 - INFO - utils_ichi -   text_clean_indices: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5809 973 494 6443 871 481 1729 1730 1912 106 5214 1286 9097 166 93 1616 1398 1496 945 8 420 218 205 206 102 973 6419 2110 115 9097 9097 8 2363 1327 98 50 27 6143\n",
            "10/17/2020 17:24:09 - INFO - utils_ichi -   aspect_indices: 2217 3896 6245 13675 16613 2075 6314 5281 2833 2833 4319 2154 3202 2460 2791 1997 3052 2242 2842 4308 2063 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:24:09 - INFO - utils_ichi -   label: [0, 1, 0, 0, 0] (id = [0, 1, 0, 0, 0])\n",
            "10/17/2020 17:24:09 - INFO - utils_ichi -   *** Example ***\n",
            "10/17/2020 17:24:09 - INFO - utils_ichi -   guid: dev-3\n",
            "10/17/2020 17:24:09 - INFO - utils_ichi -   input_global_ids: 101 2792 1997 6387 4487 29212 9879 3053 2019 3178 1010 2059 2019 2792 1997 6387 10720 2015 1998 28464 9879 3053 2019 3178 1010 1998 2506 27333 18679 2005 2178 2154 1012 9526 2003 4788 2084 1996 3291 1012 102 4487 29212 102 27333 18679 102 10720 2015 102 28464 102 3291 102 4788 102 2506 102 2154 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:24:09 - INFO - utils_ichi -   attention_mask_global: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:24:09 - INFO - utils_ichi -   token_global_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:24:09 - INFO - utils_ichi -   input_local_ids: 101 2792 1997 6387 4487 29212 9879 3053 2019 3178 1010 2059 2019 2792 1997 6387 10720 2015 1998 28464 9879 3053 2019 3178 1010 1998 2506 27333 18679 2005 2178 2154 1012 9526 2003 4788 2084 1996 3291 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:24:09 - INFO - utils_ichi -   attention_mask_local: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:24:09 - INFO - utils_ichi -   token_local_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:24:09 - INFO - utils_ichi -   text_clean_indices: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3365 1869 314 3405 875 9097 3365 1869 3942 9097 3405 875 9097 645 9097 600 277 2073 619 2670\n",
            "10/17/2020 17:24:09 - INFO - utils_ichi -   aspect_indices: 4487 29212 27333 18679 10720 2015 28464 3291 4788 2506 2154 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:24:09 - INFO - utils_ichi -   label: [0, 1, 0, 0, 0] (id = [0, 1, 0, 0, 0])\n",
            "10/17/2020 17:24:09 - INFO - utils_ichi -   *** Example ***\n",
            "10/17/2020 17:24:09 - INFO - utils_ichi -   guid: dev-4\n",
            "10/17/2020 17:24:09 - INFO - utils_ichi -   input_global_ids: 101 2044 2635 11146 2953 1998 2383 6740 3255 1998 3768 1997 2491 2043 3061 1999 1996 16956 1010 2026 2852 16250 5423 15660 1012 1045 2031 2018 4852 2512 2644 3255 1999 3300 1998 2067 1998 2085 5729 23690 1998 5255 1999 2398 1998 2519 2007 2009 11714 13529 2100 3110 2035 2058 1012 2026 2919 3798 2031 10548 2021 2085 1045 2572 4452 1045 2089 2031 9113 4053 2030 11290 3471 1998 2044 3752 2023 4573 7928 2097 2644 2635 5423 15660 1012 102 11290 3471 102 10548 102 2009 11714 102 2491 102 13529 2100 102 3255 1999 3300 102 6740 3255 102 5255 102 9113 4053 102 16250 102 7928 102 11146 2953 102 5423 15660 102 11146 2953 102 5423 15660 102 3110 102 4452 102 2398 102 2519 102 2067 102 5729 102 3061 102 4852 102 23690 102 2919 3798 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:24:09 - INFO - utils_ichi -   attention_mask_global: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:24:09 - INFO - utils_ichi -   token_global_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:24:09 - INFO - utils_ichi -   input_local_ids: 101 2044 2635 11146 2953 1998 2383 6740 3255 1998 3768 1997 2491 2043 3061 1999 1996 16956 1010 2026 2852 16250 5423 15660 1012 1045 2031 2018 4852 2512 2644 3255 1999 3300 1998 2067 1998 2085 5729 23690 1998 5255 1999 2398 1998 2519 2007 2009 11714 13529 2100 3110 2035 2058 1012 2026 2919 3798 2031 10548 2021 2085 1045 2572 4452 1045 2089 2031 9113 4053 2030 11290 3471 1998 2044 3752 2023 4573 7928 2097 2644 2635 5423 15660 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:24:09 - INFO - utils_ichi -   attention_mask_local: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:24:09 - INFO - utils_ichi -   token_local_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:24:09 - INFO - utils_ichi -   text_clean_indices: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 189 93 1392 80 155 2394 2576 1024 9097 278 256 41 8 841 2145 166 155 2049 99 105 1274 2010 1021 294 7181 9097 151 826 34 597 192 230 8 1229 8 46 1774 321 886 73 713 714 1634 166 93 41\n",
            "10/17/2020 17:24:09 - INFO - utils_ichi -   aspect_indices: 11290 3471 10548 2009 11714 2491 13529 2100 3255 1999 3300 6740 3255 5255 9113 4053 16250 7928 11146 2953 5423 15660 11146 2953 5423 15660 3110 4452 2398 2519 2067 5729 3061 4852 23690 2919 3798 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:24:09 - INFO - utils_ichi -   label: [0, 1, 0, 0, 1] (id = [0, 1, 0, 0, 1])\n",
            "10/17/2020 17:24:09 - INFO - utils_ichi -   *** Example ***\n",
            "10/17/2020 17:24:09 - INFO - utils_ichi -   guid: dev-5\n",
            "10/17/2020 17:24:09 - INFO - utils_ichi -   input_global_ids: 101 4293 10930 2388 1011 1999 1011 2375 2038 3638 3279 1010 2606 3279 1010 6740 12403 19230 1999 2187 2849 1998 3456 1010 5976 2668 5699 5418 1010 16596 12070 3255 2029 2003 2025 16596 12070 2050 1010 15861 2067 1998 5099 3255 1010 16612 3279 1997 3110 1999 10393 1010 4487 29212 1010 4432 3471 1010 6034 5458 2791 1010 3768 1997 18923 1010 1998 2053 4372 19877 15593 2213 2005 2166 1012 2016 2038 2018 2195 27339 2015 1998 2040 4282 2054 2842 2016 2038 5281 1998 2025 2988 2000 2149 1012 2852 2359 2000 3313 13004 2000 2871 11460 1012 2057 2165 2014 6135 2125 1996 4319 1998 2016 2003 2927 2000 2031 2709 1997 3110 1999 2014 10393 1010 2053 2062 6740 13675 25167 2012 2305 2030 4487 29212 1012 2635 5423 19419 5648 1010 6819 2102 1039 1004 13164 13004 102 3768 1997 18923 102 2606 3279 102 6909 102 16596 12070 2050 102 3279 1997 3110 102 6740 12403 19230 102 6740 13675 25167 102 4487 29212 102 5099 3255 102 3255 102 16596 12070 102 3638 3279 102 5423 19419 5648 102 5699 102 2522 4160 10790 102 4319 102 28093 2378 102 16480 4244 27833 102 5423 19419 5648 102 5699 102 2522 4160 10790 102 28093 2378 102 16480 4244 27833 102 5423 19419 5648 102 2187 2849 102 3456 102 10393 102 2988 102 3110 102 5281 102 4432 102 2668 102 2522 4160 10790 102 3471 102 5418 102 5458 2791 102 15861 102\n",
            "10/17/2020 17:24:09 - INFO - utils_ichi -   attention_mask_global: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "10/17/2020 17:24:09 - INFO - utils_ichi -   token_global_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "10/17/2020 17:24:09 - INFO - utils_ichi -   input_local_ids: 101 4293 10930 2388 1011 1999 1011 2375 2038 3638 3279 1010 2606 3279 1010 6740 12403 19230 1999 2187 2849 1998 3456 1010 5976 2668 5699 5418 1010 16596 12070 3255 2029 2003 2025 16596 12070 2050 1010 15861 2067 1998 5099 3255 1010 16612 3279 1997 3110 1999 10393 1010 4487 29212 1010 4432 3471 1010 6034 5458 2791 1010 3768 1997 18923 1010 1998 2053 4372 19877 15593 2213 2005 2166 1012 2016 2038 2018 2195 27339 2015 1998 2040 4282 2054 2842 2016 2038 5281 1998 2025 2988 2000 2149 1012 2852 2359 2000 3313 13004 2000 2871 11460 1012 2057 2165 2014 6135 2125 1996 4319 1998 2016 2003 2927 2000 2031 2709 1997 3110 1999 2014 10393 1010 2053 2062 6740 13675 25167 2012 2305 2030 4487 29212 1012 2635 5423 19419 5648 1010 6819 2102 1039 1004 13164 13004 1997 2522 4160 10790 1998 2097 2196 2202 2178 28093 2378 1012 3020 16480 4244 27833 1999 2216 2058 3438 2003 2085 2179 2000 2022 9474 3391 2114 6909 999 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:24:09 - INFO - utils_ichi -   attention_mask_local: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:24:09 - INFO - utils_ichi -   token_local_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:24:09 - INFO - utils_ichi -   text_clean_indices: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1260 9097 9097 1736 1311 1310 1311 80 2286 152 153 1013 2617 268 4505 9097 4159 155 9097 5408 99 479 79 5040 1966 151 5881 2963 720 721 1 982 2394 7598 9097 547 2352 681 9097 583 797 481 2232 7264 278 2830 1373 226 1288 1814 2176 175 2575 234 520 1449 151 5881 80 1358 669 728 93 8496 5461 2462 2081 473 3767 226 1305 22 115 600 1047 2424 54 846 193 9097 4232 9097\n",
            "10/17/2020 17:24:09 - INFO - utils_ichi -   aspect_indices: 3768 1997 18923 2606 3279 6909 16596 12070 2050 3279 1997 3110 6740 12403 19230 6740 13675 25167 4487 29212 5099 3255 3255 16596 12070 3638 3279 5423 19419 5648 5699 2522 4160 10790 4319 28093 2378 16480 4244 27833 5423 19419 5648 5699 2522 4160 10790 28093 2378 16480 4244 27833 5423 19419 5648 2187 2849 3456 10393 2988 3110 5281 4432 2668 2522 4160 10790 3471 5418 5458 2791 15861 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/17/2020 17:24:09 - INFO - utils_ichi -   label: [0, 1, 0, 1, 0] (id = [0, 1, 0, 1, 0])\n",
            "10/17/2020 17:24:10 - INFO - utils_ichi -   Saving features into cached file ./data/ichi/cached_dev_bert-base-uncased_256_ichi\n",
            "10/17/2020 17:24:11 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "10/17/2020 17:24:11 - INFO - __main__ -     Num examples = 300\n",
            "10/17/2020 17:24:11 - INFO - __main__ -     Batch size = 16\n",
            "Evaluating: 100% 19/19 [00:11<00:00,  1.60it/s]\n",
            "10/17/2020 17:24:22 - INFO - __main__ -   ***** Eval results  *****\n",
            "10/17/2020 17:24:22 - INFO - __main__ -     acc_0 = 0.8366666666666667\n",
            "10/17/2020 17:24:22 - INFO - __main__ -     acc_1 = 0.99\n",
            "10/17/2020 17:24:22 - INFO - __main__ -     acc_2 = 0.8766666666666667\n",
            "10/17/2020 17:24:22 - INFO - __main__ -     acc_3 = 0.9233333333333333\n",
            "10/17/2020 17:24:22 - INFO - __main__ -     acc_4 = 0.93\n",
            "10/17/2020 17:24:22 - INFO - __main__ -     f1_0 = 0.5529061102831595\n",
            "10/17/2020 17:24:22 - INFO - __main__ -     f1_1 = 0.49748743718592964\n",
            "10/17/2020 17:24:22 - INFO - __main__ -     f1_2 = 0.7359594662099479\n",
            "10/17/2020 17:24:22 - INFO - __main__ -     f1_3 = 0.8015815959741193\n",
            "10/17/2020 17:24:22 - INFO - __main__ -     f1_4 = 0.6975661274062694\n",
            "10/17/2020 17:24:22 - INFO - __main__ -   Saving model checkpoint to ./tmp/ichi_bert_base_new\n",
            "10/17/2020 17:24:28 - INFO - transformers.tokenization_utils -   Model name './tmp/ichi_bert_base_new' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming './tmp/ichi_bert_base_new' is a path or url to a directory containing tokenizer files.\n",
            "10/17/2020 17:24:28 - INFO - transformers.tokenization_utils -   loading file ./tmp/ichi_bert_base_new/vocab.txt\n",
            "10/17/2020 17:24:28 - INFO - transformers.tokenization_utils -   loading file ./tmp/ichi_bert_base_new/added_tokens.json\n",
            "10/17/2020 17:24:28 - INFO - transformers.tokenization_utils -   loading file ./tmp/ichi_bert_base_new/special_tokens_map.json\n",
            "10/17/2020 17:24:28 - INFO - transformers.tokenization_utils -   loading file ./tmp/ichi_bert_base_new/tokenizer_config.json\n",
            "10/17/2020 17:24:28 - INFO - transformers.tokenization_utils -   Model name './tmp/ichi_bert_base_new' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming './tmp/ichi_bert_base_new' is a path or url to a directory containing tokenizer files.\n",
            "10/17/2020 17:24:28 - INFO - transformers.tokenization_utils -   loading file ./tmp/ichi_bert_base_new/vocab.txt\n",
            "10/17/2020 17:24:28 - INFO - transformers.tokenization_utils -   loading file ./tmp/ichi_bert_base_new/added_tokens.json\n",
            "10/17/2020 17:24:28 - INFO - transformers.tokenization_utils -   loading file ./tmp/ichi_bert_base_new/special_tokens_map.json\n",
            "10/17/2020 17:24:28 - INFO - transformers.tokenization_utils -   loading file ./tmp/ichi_bert_base_new/tokenizer_config.json\n",
            "loading tokenizer from cache: ./data/ichi/cachedtokenizer_bert-base-uncased_glove_cadec\n",
            "10/17/2020 17:24:28 - INFO - __main__ -   Evaluate the following checkpoints: ['./tmp/ichi_bert_base_new']\n",
            "10/17/2020 17:24:32 - INFO - utils_ichi -   Loading features from cached file ./data/ichi/cached_dev_bert-base-uncased_256_ichi\n",
            "loading tokenizer from cache: ./data/ichi/cachedtokenizer_bert-base-uncased_glove_ichi\n",
            "10/17/2020 17:24:32 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "10/17/2020 17:24:32 - INFO - __main__ -     Num examples = 300\n",
            "10/17/2020 17:24:32 - INFO - __main__ -     Batch size = 16\n",
            "Evaluating: 100% 19/19 [00:11<00:00,  1.60it/s]\n",
            "10/17/2020 17:24:44 - INFO - __main__ -   ***** Eval results  *****\n",
            "10/17/2020 17:24:44 - INFO - __main__ -     acc_0 = 0.8366666666666667\n",
            "10/17/2020 17:24:44 - INFO - __main__ -     acc_1 = 0.99\n",
            "10/17/2020 17:24:44 - INFO - __main__ -     acc_2 = 0.8766666666666667\n",
            "10/17/2020 17:24:44 - INFO - __main__ -     acc_3 = 0.9233333333333333\n",
            "10/17/2020 17:24:44 - INFO - __main__ -     acc_4 = 0.93\n",
            "10/17/2020 17:24:44 - INFO - __main__ -     f1_0 = 0.5529061102831595\n",
            "10/17/2020 17:24:44 - INFO - __main__ -     f1_1 = 0.49748743718592964\n",
            "10/17/2020 17:24:44 - INFO - __main__ -     f1_2 = 0.7359594662099479\n",
            "10/17/2020 17:24:44 - INFO - __main__ -     f1_3 = 0.8015815959741193\n",
            "10/17/2020 17:24:44 - INFO - __main__ -     f1_4 = 0.6975661274062694\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H01TYmyzS8n4",
        "outputId": "39fc8b93-5a5b-42f7-f401-2555ba07320a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Oct 16 12:18:46 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.23.05    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   52C    P8    10W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PlJzq0GsRjF",
        "outputId": "23ca1369-e83c-4d1f-dd8c-7bab9eb15d47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bioasq.lip6.fr\t\t      hubconf.py\tsetup.cfg\n",
            "biomedicalWordVectors.tar.gz  index.html\tsetup.py\n",
            "CONTRIBUTING.md\t\t      index.html.1\tsrc\n",
            "data\t\t\t      LICENSE\t\ttemplates\n",
            "deploy_multi_version_doc.sh   Makefile\t\ttests\n",
            "docker\t\t\t      MANIFEST.in\ttmp\n",
            "docs\t\t\t      notebooks\t\ttransformers-cli\n",
            "examples\t\t      README.md\t\tutils\n",
            "glove.840B.300d.txt\t      requirements.txt\tvalohai.yaml\n",
            "glove.840B.300d.zip\t      runs\t\tword2vecTools\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g11ukVnhuyD2",
        "outputId": "c11bd94e-de90-44e6-e8e0-a9a58bec7852",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python ./examples/ichi/run_ichi.py --model_type bert --model_name_or_path bert-base-uncased --do_lower_case --do_train --do_eval --data_dir ./data/ichi --max_seq_length 256 --per_gpu_eval_batch_size=16 --per_gpu_train_batch_size=16 --learning_rate 5e-5 --num_train_epochs 1 --output_dir ./tmp/ichi_bert_base_new --overwrite_output_dir"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-10-15 13:29:18.930506: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "10/15/2020 13:29:20 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "10/15/2020 13:29:20 - INFO - filelock -   Lock 140066953920072 acquired on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock\n",
            "10/15/2020 13:29:20 - INFO - filelock -   Lock 140066953920072 released on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock\n",
            "10/15/2020 13:29:20 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "10/15/2020 13:29:20 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"do_sample\": false,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"finetuning_task\": \"ichi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 7,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "10/15/2020 13:29:21 - INFO - filelock -   Lock 140066954079312 acquired on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
            "10/15/2020 13:29:21 - INFO - filelock -   Lock 140066954079312 released on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
            "10/15/2020 13:29:21 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "10/15/2020 13:29:21 - INFO - utils_ichi -   Loading features from cached file ./data/ichi/cached_train_bert-base-uncased_256_ichi\n",
            "loading tokenizer from cache: ./data/ichi/cachedtokenizer_bert-base-uncased_glove_ichi\n",
            "10/15/2020 13:29:25 - INFO - filelock -   Lock 140066815710152 acquired on /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock\n",
            "10/15/2020 13:29:25 - INFO - filelock -   Lock 140066815710152 released on /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock\n",
            "10/15/2020 13:29:25 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "10/15/2020 13:29:28 - INFO - transformers.modeling_utils -   Weights of lcf_BERT not initialized from pretrained model: ['bert_SA.SA.query.weight', 'bert_SA.SA.query.bias', 'bert_SA.SA.key.weight', 'bert_SA.SA.key.bias', 'bert_SA.SA.value.weight', 'bert_SA.SA.value.bias', 'linear_double_cdm_or_cdw.weight', 'linear_double_cdm_or_cdw.bias', 'linear_triple_lcf_global.weight', 'linear_triple_lcf_global.bias', 'bert_pooler.dense.weight', 'bert_pooler.dense.bias', 'dense.weight', 'dense.bias']\n",
            "10/15/2020 13:29:28 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in lcf_BERT: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "10/15/2020 13:29:33 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='./data/ichi', device=device(type='cuda'), do_eval=True, do_lower_case=True, do_train=True, embedding_dim_word2vec=300, embedding_type='glove', eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, local_context_focus='cdm', local_rank=-1, logging_steps=50, max_grad_norm=1.0, max_seq_length=256, max_steps=-1, model_name_or_path='bert-base-uncased', model_type='bert', n_gpu=1, no_cuda=False, num_train_epochs=1.0, output_dir='./tmp/ichi_bert_base_new', output_mode='classification', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=16, per_gpu_train_batch_size=16, save_steps=10000, seed=42, server_ip='', server_port='', task_name='ichi', tokenizer_name='', use_single_bert=False, warmup_steps=0, weight_decay=0.0)\n",
            "10/15/2020 13:29:33 - INFO - __main__ -   ***** Running training *****\n",
            "10/15/2020 13:29:33 - INFO - __main__ -     Num examples = 8000\n",
            "10/15/2020 13:29:33 - INFO - __main__ -     Num Epochs = 1\n",
            "10/15/2020 13:29:33 - INFO - __main__ -     Instantaneous batch size per GPU = 16\n",
            "10/15/2020 13:29:33 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "10/15/2020 13:29:33 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "10/15/2020 13:29:33 - INFO - __main__ -     Total optimization steps = 500\n",
            "Epoch:   0% 0/1 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/500 [00:00<?, ?it/s]\u001b[A/content/transformers/examples/ichi/lcf_ichi.py:65: UserWarning: This overload of nonzero is deprecated:\n",
            "\tnonzero()\n",
            "Consider using one of the following signatures instead:\n",
            "\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
            "  indices = (text_local_indices[i] == aspect_indices[i][j]).nonzero()\n",
            "\n",
            "Iteration:   0% 1/500 [00:01<12:43,  1.53s/it]\u001b[A\n",
            "Iteration:   0% 2/500 [00:02<12:28,  1.50s/it]\u001b[A\n",
            "Iteration:   1% 3/500 [00:04<12:21,  1.49s/it]\u001b[A\n",
            "Iteration:   1% 4/500 [00:05<12:10,  1.47s/it]\u001b[A\n",
            "Iteration:   1% 5/500 [00:07<12:04,  1.46s/it]\u001b[A\n",
            "Iteration:   1% 6/500 [00:08<12:01,  1.46s/it]\u001b[A\n",
            "Iteration:   1% 7/500 [00:10<12:01,  1.46s/it]\u001b[A\n",
            "Iteration:   2% 8/500 [00:11<11:59,  1.46s/it]\u001b[A\n",
            "Iteration:   2% 9/500 [00:13<11:55,  1.46s/it]\u001b[A\n",
            "Iteration:   2% 10/500 [00:14<11:53,  1.46s/it]\u001b[A\n",
            "Iteration:   2% 11/500 [00:16<11:51,  1.45s/it]\u001b[A\n",
            "Iteration:   2% 12/500 [00:17<11:55,  1.47s/it]\u001b[A\n",
            "Iteration:   3% 13/500 [00:19<11:54,  1.47s/it]\u001b[A\n",
            "Iteration:   3% 14/500 [00:20<11:55,  1.47s/it]\u001b[A\n",
            "Iteration:   3% 15/500 [00:21<11:55,  1.48s/it]\u001b[A\n",
            "Iteration:   3% 16/500 [00:23<11:57,  1.48s/it]\u001b[A\n",
            "Iteration:   3% 17/500 [00:24<11:57,  1.49s/it]\u001b[A\n",
            "Iteration:   4% 18/500 [00:26<11:55,  1.48s/it]\u001b[A\n",
            "Iteration:   4% 19/500 [00:27<11:55,  1.49s/it]\u001b[A\n",
            "Iteration:   4% 20/500 [00:29<11:56,  1.49s/it]\u001b[A\n",
            "Iteration:   4% 21/500 [00:30<11:54,  1.49s/it]\u001b[A\n",
            "Iteration:   4% 22/500 [00:32<11:52,  1.49s/it]\u001b[A\n",
            "Iteration:   5% 23/500 [00:33<11:53,  1.50s/it]\u001b[A\n",
            "Iteration:   5% 24/500 [00:35<11:51,  1.49s/it]\u001b[A\n",
            "Iteration:   5% 25/500 [00:36<11:51,  1.50s/it]\u001b[A\n",
            "Iteration:   5% 26/500 [00:38<11:51,  1.50s/it]\u001b[A\n",
            "Iteration:   5% 27/500 [00:39<11:49,  1.50s/it]\u001b[A\n",
            "Iteration:   6% 28/500 [00:41<11:50,  1.51s/it]\u001b[A\n",
            "Iteration:   6% 29/500 [00:42<11:49,  1.51s/it]\u001b[A\n",
            "Iteration:   6% 30/500 [00:44<11:53,  1.52s/it]\u001b[A\n",
            "Iteration:   6% 31/500 [00:46<11:52,  1.52s/it]\u001b[A\n",
            "Iteration:   6% 32/500 [00:47<11:53,  1.52s/it]\u001b[A\n",
            "Iteration:   7% 33/500 [00:49<11:51,  1.52s/it]\u001b[A\n",
            "Iteration:   7% 34/500 [00:50<11:53,  1.53s/it]\u001b[A\n",
            "Iteration:   7% 35/500 [00:52<11:51,  1.53s/it]\u001b[A\n",
            "Iteration:   7% 36/500 [00:53<11:48,  1.53s/it]\u001b[A\n",
            "Iteration:   7% 37/500 [00:55<11:51,  1.54s/it]\u001b[A\n",
            "Iteration:   8% 38/500 [00:56<11:52,  1.54s/it]\u001b[A\n",
            "Iteration:   8% 39/500 [00:58<11:51,  1.54s/it]\u001b[A\n",
            "Iteration:   8% 40/500 [00:59<11:53,  1.55s/it]\u001b[A\n",
            "Iteration:   8% 41/500 [01:01<11:53,  1.55s/it]\u001b[A\n",
            "Iteration:   8% 42/500 [01:03<11:51,  1.55s/it]\u001b[A\n",
            "Iteration:   9% 43/500 [01:04<11:51,  1.56s/it]\u001b[A\n",
            "Iteration:   9% 44/500 [01:06<11:52,  1.56s/it]\u001b[A\n",
            "Iteration:   9% 45/500 [01:07<11:50,  1.56s/it]\u001b[A\n",
            "Iteration:   9% 46/500 [01:09<11:55,  1.58s/it]\u001b[A\n",
            "Iteration:   9% 47/500 [01:10<11:53,  1.57s/it]\u001b[A\n",
            "Iteration:  10% 48/500 [01:12<11:59,  1.59s/it]\u001b[A\n",
            "Iteration:  10% 49/500 [01:14<11:57,  1.59s/it]\u001b[A/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "{\"learning_rate\": 4.5e-05, \"loss\": 1.619144320487976, \"step\": 50}\n",
            "\n",
            "Iteration:  10% 50/500 [01:15<12:00,  1.60s/it]\u001b[A\n",
            "Iteration:  10% 51/500 [01:17<11:58,  1.60s/it]\u001b[A\n",
            "Iteration:  10% 52/500 [01:18<11:59,  1.61s/it]\u001b[A\n",
            "Iteration:  11% 53/500 [01:20<11:56,  1.60s/it]\u001b[A\n",
            "Iteration:  11% 54/500 [01:22<11:55,  1.60s/it]\u001b[A\n",
            "Iteration:  11% 55/500 [01:23<11:55,  1.61s/it]\u001b[A\n",
            "Iteration:  11% 56/500 [01:25<11:55,  1.61s/it]\u001b[A\n",
            "Iteration:  11% 57/500 [01:27<11:54,  1.61s/it]\u001b[A\n",
            "Iteration:  12% 58/500 [01:28<11:54,  1.62s/it]\u001b[A\n",
            "Iteration:  12% 59/500 [01:30<11:53,  1.62s/it]\u001b[A\n",
            "Iteration:  12% 60/500 [01:31<11:57,  1.63s/it]\u001b[A\n",
            "Iteration:  12% 61/500 [01:33<11:57,  1.63s/it]\u001b[A\n",
            "Iteration:  12% 62/500 [01:35<12:02,  1.65s/it]\u001b[A\n",
            "Iteration:  13% 63/500 [01:36<12:01,  1.65s/it]\u001b[A\n",
            "Iteration:  13% 64/500 [01:38<12:03,  1.66s/it]\u001b[A\n",
            "Iteration:  13% 65/500 [01:40<12:04,  1.67s/it]\u001b[A\n",
            "Iteration:  13% 66/500 [01:41<12:05,  1.67s/it]\u001b[A\n",
            "Iteration:  13% 67/500 [01:43<12:10,  1.69s/it]\u001b[A\n",
            "Iteration:  14% 68/500 [01:45<12:11,  1.69s/it]\u001b[A\n",
            "Iteration:  14% 69/500 [01:47<12:12,  1.70s/it]\u001b[A\n",
            "Iteration:  14% 70/500 [01:48<12:12,  1.70s/it]\u001b[A\n",
            "Iteration:  14% 71/500 [01:50<12:11,  1.70s/it]\u001b[A\n",
            "Iteration:  14% 72/500 [01:52<12:13,  1.71s/it]\u001b[A\n",
            "Iteration:  15% 73/500 [01:53<12:15,  1.72s/it]\u001b[A\n",
            "Iteration:  15% 74/500 [01:55<12:15,  1.73s/it]\u001b[A\n",
            "Iteration:  15% 75/500 [01:57<12:16,  1.73s/it]\u001b[A\n",
            "Iteration:  15% 76/500 [01:59<12:19,  1.74s/it]\u001b[A\n",
            "Iteration:  15% 77/500 [02:01<12:19,  1.75s/it]\u001b[A\n",
            "Iteration:  16% 78/500 [02:02<12:17,  1.75s/it]\u001b[A\n",
            "Iteration:  16% 79/500 [02:04<12:17,  1.75s/it]\u001b[A\n",
            "Iteration:  16% 80/500 [02:06<12:16,  1.75s/it]\u001b[A\n",
            "Iteration:  16% 81/500 [02:08<12:17,  1.76s/it]\u001b[A\n",
            "Iteration:  16% 82/500 [02:09<12:16,  1.76s/it]\u001b[A\n",
            "Iteration:  17% 83/500 [02:11<12:11,  1.75s/it]\u001b[A\n",
            "Iteration:  17% 84/500 [02:13<12:07,  1.75s/it]\u001b[A\n",
            "Iteration:  17% 85/500 [02:15<12:04,  1.75s/it]\u001b[A\n",
            "Iteration:  17% 86/500 [02:16<11:58,  1.74s/it]\u001b[A\n",
            "Iteration:  17% 87/500 [02:18<11:55,  1.73s/it]\u001b[A\n",
            "Iteration:  18% 88/500 [02:20<11:56,  1.74s/it]\u001b[A\n",
            "Iteration:  18% 89/500 [02:21<11:54,  1.74s/it]\u001b[A\n",
            "Iteration:  18% 90/500 [02:23<11:44,  1.72s/it]\u001b[A\n",
            "Iteration:  18% 91/500 [02:25<11:41,  1.71s/it]\u001b[A\n",
            "Iteration:  18% 92/500 [02:27<11:33,  1.70s/it]\u001b[A\n",
            "Iteration:  19% 93/500 [02:28<11:31,  1.70s/it]\u001b[A\n",
            "Iteration:  19% 94/500 [02:30<11:25,  1.69s/it]\u001b[A\n",
            "Iteration:  19% 95/500 [02:32<11:25,  1.69s/it]\u001b[A\n",
            "Iteration:  19% 96/500 [02:33<11:23,  1.69s/it]\u001b[A\n",
            "Iteration:  19% 97/500 [02:35<11:17,  1.68s/it]\u001b[A\n",
            "Iteration:  20% 98/500 [02:37<11:15,  1.68s/it]\u001b[A\n",
            "Iteration:  20% 99/500 [02:38<11:16,  1.69s/it]\u001b[A{\"learning_rate\": 4e-05, \"loss\": 1.223893141746521, \"step\": 100}\n",
            "\n",
            "Iteration:  20% 100/500 [02:40<11:12,  1.68s/it]\u001b[A\n",
            "Iteration:  20% 101/500 [02:42<11:13,  1.69s/it]\u001b[A\n",
            "Iteration:  20% 102/500 [02:43<11:10,  1.69s/it]\u001b[A\n",
            "Iteration:  21% 103/500 [02:45<11:09,  1.69s/it]\u001b[A\n",
            "Iteration:  21% 104/500 [02:47<11:09,  1.69s/it]\u001b[A\n",
            "Iteration:  21% 105/500 [02:48<11:05,  1.69s/it]\u001b[A\n",
            "Iteration:  21% 106/500 [02:50<11:07,  1.69s/it]\u001b[A\n",
            "Iteration:  21% 107/500 [02:52<11:07,  1.70s/it]\u001b[A\n",
            "Iteration:  22% 108/500 [02:54<11:06,  1.70s/it]\u001b[A\n",
            "Iteration:  22% 109/500 [02:55<11:03,  1.70s/it]\u001b[A\n",
            "Iteration:  22% 110/500 [02:57<11:00,  1.69s/it]\u001b[A\n",
            "Iteration:  22% 111/500 [02:59<10:57,  1.69s/it]\u001b[A\n",
            "Iteration:  22% 112/500 [03:00<10:55,  1.69s/it]\u001b[A\n",
            "Iteration:  23% 113/500 [03:02<10:53,  1.69s/it]\u001b[A\n",
            "Iteration:  23% 114/500 [03:04<10:54,  1.70s/it]\u001b[A\n",
            "Iteration:  23% 115/500 [03:05<10:52,  1.69s/it]\u001b[A\n",
            "Iteration:  23% 116/500 [03:07<10:54,  1.71s/it]\u001b[A\n",
            "Iteration:  23% 117/500 [03:09<10:55,  1.71s/it]\u001b[A\n",
            "Iteration:  24% 118/500 [03:11<10:52,  1.71s/it]\u001b[A\n",
            "Iteration:  24% 119/500 [03:12<10:51,  1.71s/it]\u001b[A\n",
            "Iteration:  24% 120/500 [03:14<10:51,  1.72s/it]\u001b[A\n",
            "Iteration:  24% 121/500 [03:16<10:49,  1.71s/it]\u001b[A\n",
            "Iteration:  24% 122/500 [03:17<10:46,  1.71s/it]\u001b[A\n",
            "Iteration:  25% 123/500 [03:19<10:49,  1.72s/it]\u001b[A\n",
            "Iteration:  25% 124/500 [03:21<10:45,  1.72s/it]\u001b[A\n",
            "Iteration:  25% 125/500 [03:23<10:42,  1.71s/it]\u001b[A\n",
            "Iteration:  25% 126/500 [03:24<10:42,  1.72s/it]\u001b[A\n",
            "Iteration:  25% 127/500 [03:26<10:39,  1.71s/it]\u001b[A\n",
            "Iteration:  26% 128/500 [03:28<10:37,  1.71s/it]\u001b[A\n",
            "Iteration:  26% 129/500 [03:29<10:32,  1.70s/it]\u001b[A\n",
            "Iteration:  26% 130/500 [03:31<10:34,  1.72s/it]\u001b[A\n",
            "Iteration:  26% 131/500 [03:33<10:31,  1.71s/it]\u001b[A\n",
            "Iteration:  26% 132/500 [03:35<10:31,  1.72s/it]\u001b[A\n",
            "Iteration:  27% 133/500 [03:36<10:28,  1.71s/it]\u001b[A\n",
            "Iteration:  27% 134/500 [03:38<10:26,  1.71s/it]\u001b[A\n",
            "Iteration:  27% 135/500 [03:40<10:24,  1.71s/it]\u001b[A\n",
            "Iteration:  27% 136/500 [03:41<10:25,  1.72s/it]\u001b[A\n",
            "Iteration:  27% 137/500 [03:43<10:20,  1.71s/it]\u001b[A\n",
            "Iteration:  28% 138/500 [03:45<10:18,  1.71s/it]\u001b[A\n",
            "Iteration:  28% 139/500 [03:46<10:15,  1.70s/it]\u001b[A\n",
            "Iteration:  28% 140/500 [03:48<10:12,  1.70s/it]\u001b[A\n",
            "Iteration:  28% 141/500 [03:50<10:10,  1.70s/it]\u001b[A\n",
            "Iteration:  28% 142/500 [03:52<10:09,  1.70s/it]\u001b[A\n",
            "Iteration:  29% 143/500 [03:53<10:11,  1.71s/it]\u001b[A\n",
            "Iteration:  29% 144/500 [03:55<10:05,  1.70s/it]\u001b[A\n",
            "Iteration:  29% 145/500 [03:57<10:02,  1.70s/it]\u001b[A\n",
            "Iteration:  29% 146/500 [03:58<10:00,  1.70s/it]\u001b[A\n",
            "Iteration:  29% 147/500 [04:00<09:58,  1.70s/it]\u001b[A\n",
            "Iteration:  30% 148/500 [04:02<09:58,  1.70s/it]\u001b[A\n",
            "Iteration:  30% 149/500 [04:03<09:56,  1.70s/it]\u001b[A{\"learning_rate\": 3.5e-05, \"loss\": 1.14622136592865, \"step\": 150}\n",
            "\n",
            "Iteration:  30% 150/500 [04:05<09:52,  1.69s/it]\u001b[A\n",
            "Iteration:  30% 151/500 [04:07<09:49,  1.69s/it]\u001b[A\n",
            "Iteration:  30% 152/500 [04:09<09:46,  1.69s/it]\u001b[A\n",
            "Iteration:  31% 153/500 [04:10<09:44,  1.68s/it]\u001b[A\n",
            "Iteration:  31% 154/500 [04:12<09:44,  1.69s/it]\u001b[A\n",
            "Iteration:  31% 155/500 [04:14<09:44,  1.69s/it]\u001b[A\n",
            "Iteration:  31% 156/500 [04:15<09:43,  1.70s/it]\u001b[A\n",
            "Iteration:  31% 157/500 [04:17<09:42,  1.70s/it]\u001b[A\n",
            "Iteration:  32% 158/500 [04:19<09:37,  1.69s/it]\u001b[A\n",
            "Iteration:  32% 159/500 [04:20<09:36,  1.69s/it]\u001b[A\n",
            "Iteration:  32% 160/500 [04:22<09:36,  1.70s/it]\u001b[A\n",
            "Iteration:  32% 161/500 [04:24<09:32,  1.69s/it]\u001b[A\n",
            "Iteration:  32% 162/500 [04:25<09:31,  1.69s/it]\u001b[A\n",
            "Iteration:  33% 163/500 [04:27<09:32,  1.70s/it]\u001b[A\n",
            "Iteration:  33% 164/500 [04:29<09:29,  1.70s/it]\u001b[A\n",
            "Iteration:  33% 165/500 [04:31<09:28,  1.70s/it]\u001b[A\n",
            "Iteration:  33% 166/500 [04:32<09:28,  1.70s/it]\u001b[A\n",
            "Iteration:  33% 167/500 [04:34<09:24,  1.70s/it]\u001b[A\n",
            "Iteration:  34% 168/500 [04:36<09:25,  1.70s/it]\u001b[A\n",
            "Iteration:  34% 169/500 [04:37<09:25,  1.71s/it]\u001b[A\n",
            "Iteration:  34% 170/500 [04:39<09:23,  1.71s/it]\u001b[A\n",
            "Iteration:  34% 171/500 [04:41<09:24,  1.72s/it]\u001b[A\n",
            "Iteration:  34% 172/500 [04:43<09:21,  1.71s/it]\u001b[A\n",
            "Iteration:  35% 173/500 [04:44<09:19,  1.71s/it]\u001b[A\n",
            "Iteration:  35% 174/500 [04:46<09:15,  1.70s/it]\u001b[A\n",
            "Iteration:  35% 175/500 [04:48<09:15,  1.71s/it]\u001b[A\n",
            "Iteration:  35% 176/500 [04:49<09:13,  1.71s/it]\u001b[A\n",
            "Iteration:  35% 177/500 [04:51<09:12,  1.71s/it]\u001b[A\n",
            "Iteration:  36% 178/500 [04:53<09:09,  1.71s/it]\u001b[A\n",
            "Iteration:  36% 179/500 [04:54<09:07,  1.71s/it]\u001b[A\n",
            "Iteration:  36% 180/500 [04:56<09:06,  1.71s/it]\u001b[A\n",
            "Iteration:  36% 181/500 [04:58<09:05,  1.71s/it]\u001b[A\n",
            "Iteration:  36% 182/500 [05:00<09:03,  1.71s/it]\u001b[A\n",
            "Iteration:  37% 183/500 [05:01<09:01,  1.71s/it]\u001b[A\n",
            "Iteration:  37% 184/500 [05:03<09:00,  1.71s/it]\u001b[A\n",
            "Iteration:  37% 185/500 [05:05<08:56,  1.70s/it]\u001b[A\n",
            "Iteration:  37% 186/500 [05:06<08:54,  1.70s/it]\u001b[A\n",
            "Iteration:  37% 187/500 [05:08<08:50,  1.69s/it]\u001b[A\n",
            "Iteration:  38% 188/500 [05:10<08:47,  1.69s/it]\u001b[A\n",
            "Iteration:  38% 189/500 [05:11<08:47,  1.70s/it]\u001b[A\n",
            "Iteration:  38% 190/500 [05:13<08:45,  1.70s/it]\u001b[A\n",
            "Iteration:  38% 191/500 [05:15<08:46,  1.70s/it]\u001b[A\n",
            "Iteration:  38% 192/500 [05:17<08:42,  1.70s/it]\u001b[A\n",
            "Iteration:  39% 193/500 [05:18<08:41,  1.70s/it]\u001b[A\n",
            "Iteration:  39% 194/500 [05:20<08:40,  1.70s/it]\u001b[A\n",
            "Iteration:  39% 195/500 [05:22<08:38,  1.70s/it]\u001b[A\n",
            "Iteration:  39% 196/500 [05:23<08:38,  1.70s/it]\u001b[A\n",
            "Iteration:  39% 197/500 [05:25<08:37,  1.71s/it]\u001b[A\n",
            "Iteration:  40% 198/500 [05:27<08:32,  1.70s/it]\u001b[A\n",
            "Iteration:  40% 199/500 [05:28<08:30,  1.70s/it]\u001b[A{\"learning_rate\": 3e-05, \"loss\": 1.029599256515503, \"step\": 200}\n",
            "\n",
            "Iteration:  40% 200/500 [05:30<08:29,  1.70s/it]\u001b[A\n",
            "Iteration:  40% 201/500 [05:32<08:28,  1.70s/it]\u001b[A\n",
            "Iteration:  40% 202/500 [05:34<08:28,  1.71s/it]\u001b[A\n",
            "Iteration:  41% 203/500 [05:35<08:25,  1.70s/it]\u001b[A\n",
            "Iteration:  41% 204/500 [05:37<08:25,  1.71s/it]\u001b[A\n",
            "Iteration:  41% 205/500 [05:39<08:23,  1.71s/it]\u001b[A\n",
            "Iteration:  41% 206/500 [05:40<08:21,  1.71s/it]\u001b[A\n",
            "Iteration:  41% 207/500 [05:42<08:18,  1.70s/it]\u001b[A\n",
            "Iteration:  42% 208/500 [05:44<08:16,  1.70s/it]\u001b[A\n",
            "Iteration:  42% 209/500 [05:46<08:14,  1.70s/it]\u001b[A\n",
            "Iteration:  42% 210/500 [05:47<08:14,  1.71s/it]\u001b[A\n",
            "Iteration:  42% 211/500 [05:49<08:11,  1.70s/it]\u001b[A\n",
            "Iteration:  42% 212/500 [05:51<08:07,  1.69s/it]\u001b[A\n",
            "Iteration:  43% 213/500 [05:52<08:06,  1.70s/it]\u001b[A\n",
            "Iteration:  43% 214/500 [05:54<08:02,  1.69s/it]\u001b[A\n",
            "Iteration:  43% 215/500 [05:56<08:01,  1.69s/it]\u001b[A\n",
            "Iteration:  43% 216/500 [05:57<07:58,  1.69s/it]\u001b[A\n",
            "Iteration:  43% 217/500 [05:59<07:57,  1.69s/it]\u001b[A\n",
            "Iteration:  44% 218/500 [06:01<07:56,  1.69s/it]\u001b[A\n",
            "Iteration:  44% 219/500 [06:02<07:53,  1.68s/it]\u001b[A\n",
            "Iteration:  44% 220/500 [06:04<07:54,  1.69s/it]\u001b[A\n",
            "Iteration:  44% 221/500 [06:06<07:52,  1.69s/it]\u001b[A\n",
            "Iteration:  44% 222/500 [06:07<07:49,  1.69s/it]\u001b[A\n",
            "Iteration:  45% 223/500 [06:09<07:47,  1.69s/it]\u001b[A\n",
            "Iteration:  45% 224/500 [06:11<07:45,  1.69s/it]\u001b[A\n",
            "Iteration:  45% 225/500 [06:13<07:45,  1.69s/it]\u001b[A\n",
            "Iteration:  45% 226/500 [06:14<07:43,  1.69s/it]\u001b[A\n",
            "Iteration:  45% 227/500 [06:16<07:42,  1.69s/it]\u001b[A\n",
            "Iteration:  46% 228/500 [06:18<07:40,  1.69s/it]\u001b[A\n",
            "Iteration:  46% 229/500 [06:19<07:39,  1.70s/it]\u001b[A\n",
            "Iteration:  46% 230/500 [06:21<07:36,  1.69s/it]\u001b[A\n",
            "Iteration:  46% 231/500 [06:23<07:34,  1.69s/it]\u001b[A\n",
            "Iteration:  46% 232/500 [06:24<07:32,  1.69s/it]\u001b[A\n",
            "Iteration:  47% 233/500 [06:26<07:32,  1.69s/it]\u001b[A\n",
            "Iteration:  47% 234/500 [06:28<07:30,  1.69s/it]\u001b[A\n",
            "Iteration:  47% 235/500 [06:29<07:27,  1.69s/it]\u001b[A\n",
            "Iteration:  47% 236/500 [06:31<07:29,  1.70s/it]\u001b[A\n",
            "Iteration:  47% 237/500 [06:33<07:28,  1.71s/it]\u001b[A\n",
            "Iteration:  48% 238/500 [06:35<07:25,  1.70s/it]\u001b[A\n",
            "Iteration:  48% 239/500 [06:36<07:24,  1.70s/it]\u001b[A\n",
            "Iteration:  48% 240/500 [06:38<07:20,  1.69s/it]\u001b[A\n",
            "Iteration:  48% 241/500 [06:40<07:18,  1.69s/it]\u001b[A\n",
            "Iteration:  48% 242/500 [06:41<07:17,  1.69s/it]\u001b[A\n",
            "Iteration:  49% 243/500 [06:43<07:16,  1.70s/it]\u001b[A\n",
            "Iteration:  49% 244/500 [06:45<07:14,  1.70s/it]\u001b[A\n",
            "Iteration:  49% 245/500 [06:46<07:11,  1.69s/it]\u001b[A\n",
            "Iteration:  49% 246/500 [06:48<07:10,  1.70s/it]\u001b[A\n",
            "Iteration:  49% 247/500 [06:50<07:09,  1.70s/it]\u001b[A\n",
            "Iteration:  50% 248/500 [06:52<07:06,  1.69s/it]\u001b[A\n",
            "Iteration:  50% 249/500 [06:53<07:05,  1.69s/it]\u001b[A{\"learning_rate\": 2.5e-05, \"loss\": 1.038466501235962, \"step\": 250}\n",
            "\n",
            "Iteration:  50% 250/500 [06:55<07:04,  1.70s/it]\u001b[A\n",
            "Iteration:  50% 251/500 [06:57<07:02,  1.70s/it]\u001b[A\n",
            "Iteration:  50% 252/500 [06:58<07:02,  1.70s/it]\u001b[A\n",
            "Iteration:  51% 253/500 [07:00<06:59,  1.70s/it]\u001b[A\n",
            "Iteration:  51% 254/500 [07:02<06:57,  1.70s/it]\u001b[A\n",
            "Iteration:  51% 255/500 [07:03<06:54,  1.69s/it]\u001b[A\n",
            "Iteration:  51% 256/500 [07:05<06:54,  1.70s/it]\u001b[A\n",
            "Iteration:  51% 257/500 [07:07<06:51,  1.70s/it]\u001b[A\n",
            "Iteration:  52% 258/500 [07:09<06:50,  1.70s/it]\u001b[A\n",
            "Iteration:  52% 259/500 [07:10<06:51,  1.71s/it]\u001b[A\n",
            "Iteration:  52% 260/500 [07:12<06:47,  1.70s/it]\u001b[A\n",
            "Iteration:  52% 261/500 [07:14<06:46,  1.70s/it]\u001b[A\n",
            "Iteration:  52% 262/500 [07:15<06:43,  1.70s/it]\u001b[A\n",
            "Iteration:  53% 263/500 [07:17<06:42,  1.70s/it]\u001b[A\n",
            "Iteration:  53% 264/500 [07:19<06:40,  1.70s/it]\u001b[A\n",
            "Iteration:  53% 265/500 [07:20<06:41,  1.71s/it]\u001b[A\n",
            "Iteration:  53% 266/500 [07:22<06:38,  1.70s/it]\u001b[A\n",
            "Iteration:  53% 267/500 [07:24<06:39,  1.71s/it]\u001b[A\n",
            "Iteration:  54% 268/500 [07:26<06:34,  1.70s/it]\u001b[A\n",
            "Iteration:  54% 269/500 [07:27<06:31,  1.70s/it]\u001b[A\n",
            "Iteration:  54% 270/500 [07:29<06:32,  1.71s/it]\u001b[A\n",
            "Iteration:  54% 271/500 [07:31<06:30,  1.70s/it]\u001b[A\n",
            "Iteration:  54% 272/500 [07:32<06:28,  1.70s/it]\u001b[A\n",
            "Iteration:  55% 273/500 [07:34<06:27,  1.71s/it]\u001b[A\n",
            "Iteration:  55% 274/500 [07:36<06:24,  1.70s/it]\u001b[A\n",
            "Iteration:  55% 275/500 [07:37<06:23,  1.71s/it]\u001b[A\n",
            "Iteration:  55% 276/500 [07:39<06:21,  1.70s/it]\u001b[A\n",
            "Iteration:  55% 277/500 [07:41<06:19,  1.70s/it]\u001b[A\n",
            "Iteration:  56% 278/500 [07:43<06:17,  1.70s/it]\u001b[A\n",
            "Iteration:  56% 279/500 [07:44<06:15,  1.70s/it]\u001b[A\n",
            "Iteration:  56% 280/500 [07:46<06:14,  1.70s/it]\u001b[A\n",
            "Iteration:  56% 281/500 [07:48<06:12,  1.70s/it]\u001b[A\n",
            "Iteration:  56% 282/500 [07:49<06:11,  1.70s/it]\u001b[A\n",
            "Iteration:  57% 283/500 [07:51<06:11,  1.71s/it]\u001b[A\n",
            "Iteration:  57% 284/500 [07:53<06:07,  1.70s/it]\u001b[A\n",
            "Iteration:  57% 285/500 [07:54<06:04,  1.70s/it]\u001b[A\n",
            "Iteration:  57% 286/500 [07:56<06:03,  1.70s/it]\u001b[A\n",
            "Iteration:  57% 287/500 [07:58<06:02,  1.70s/it]\u001b[A\n",
            "Iteration:  58% 288/500 [08:00<06:02,  1.71s/it]\u001b[A\n",
            "Iteration:  58% 289/500 [08:01<06:00,  1.71s/it]\u001b[A\n",
            "Iteration:  58% 290/500 [08:03<05:58,  1.71s/it]\u001b[A\n",
            "Iteration:  58% 291/500 [08:05<05:55,  1.70s/it]\u001b[A\n",
            "Iteration:  58% 292/500 [08:06<05:54,  1.71s/it]\u001b[A\n",
            "Iteration:  59% 293/500 [08:08<05:52,  1.70s/it]\u001b[A\n",
            "Iteration:  59% 294/500 [08:10<05:49,  1.70s/it]\u001b[A\n",
            "Iteration:  59% 295/500 [08:12<05:49,  1.70s/it]\u001b[A\n",
            "Iteration:  59% 296/500 [08:13<05:48,  1.71s/it]\u001b[A\n",
            "Iteration:  59% 297/500 [08:15<05:46,  1.71s/it]\u001b[A\n",
            "Iteration:  60% 298/500 [08:17<05:44,  1.70s/it]\u001b[A\n",
            "Iteration:  60% 299/500 [08:18<05:43,  1.71s/it]\u001b[A{\"learning_rate\": 2e-05, \"loss\": 1.00252512216568, \"step\": 300}\n",
            "\n",
            "Iteration:  60% 300/500 [08:20<05:40,  1.70s/it]\u001b[A\n",
            "Iteration:  60% 301/500 [08:22<05:40,  1.71s/it]\u001b[A\n",
            "Iteration:  60% 302/500 [08:23<05:37,  1.70s/it]\u001b[A\n",
            "Iteration:  61% 303/500 [08:25<05:35,  1.70s/it]\u001b[A\n",
            "Iteration:  61% 304/500 [08:27<05:33,  1.70s/it]\u001b[A\n",
            "Iteration:  61% 305/500 [08:29<05:32,  1.71s/it]\u001b[A\n",
            "Iteration:  61% 306/500 [08:30<05:31,  1.71s/it]\u001b[A\n",
            "Iteration:  61% 307/500 [08:32<05:28,  1.70s/it]\u001b[A\n",
            "Iteration:  62% 308/500 [08:34<05:27,  1.70s/it]\u001b[A\n",
            "Iteration:  62% 309/500 [08:35<05:26,  1.71s/it]\u001b[A\n",
            "Iteration:  62% 310/500 [08:37<05:23,  1.70s/it]\u001b[A\n",
            "Iteration:  62% 311/500 [08:39<05:23,  1.71s/it]\u001b[A\n",
            "Iteration:  62% 312/500 [08:41<05:22,  1.71s/it]\u001b[A\n",
            "Iteration:  63% 313/500 [08:42<05:19,  1.71s/it]\u001b[A\n",
            "Iteration:  63% 314/500 [08:44<05:18,  1.71s/it]\u001b[A\n",
            "Iteration:  63% 315/500 [08:46<05:15,  1.71s/it]\u001b[A\n",
            "Iteration:  63% 316/500 [08:47<05:14,  1.71s/it]\u001b[A\n",
            "Iteration:  63% 317/500 [08:49<05:11,  1.70s/it]\u001b[A\n",
            "Iteration:  64% 318/500 [08:51<05:10,  1.70s/it]\u001b[A\n",
            "Iteration:  64% 319/500 [08:53<05:08,  1.71s/it]\u001b[A\n",
            "Iteration:  64% 320/500 [08:54<05:07,  1.71s/it]\u001b[A\n",
            "Iteration:  64% 321/500 [08:56<05:06,  1.71s/it]\u001b[A\n",
            "Iteration:  64% 322/500 [08:58<05:04,  1.71s/it]\u001b[A\n",
            "Iteration:  65% 323/500 [08:59<05:02,  1.71s/it]\u001b[A\n",
            "Iteration:  65% 324/500 [09:01<05:01,  1.72s/it]\u001b[A\n",
            "Iteration:  65% 325/500 [09:03<04:59,  1.71s/it]\u001b[A\n",
            "Iteration:  65% 326/500 [09:04<04:57,  1.71s/it]\u001b[A\n",
            "Iteration:  65% 327/500 [09:06<04:56,  1.72s/it]\u001b[A\n",
            "Iteration:  66% 328/500 [09:08<04:55,  1.72s/it]\u001b[A\n",
            "Iteration:  66% 329/500 [09:10<04:54,  1.72s/it]\u001b[A\n",
            "Iteration:  66% 330/500 [09:11<04:52,  1.72s/it]\u001b[A\n",
            "Iteration:  66% 331/500 [09:13<04:48,  1.71s/it]\u001b[A\n",
            "Iteration:  66% 332/500 [09:15<04:46,  1.71s/it]\u001b[A\n",
            "Iteration:  67% 333/500 [09:16<04:44,  1.70s/it]\u001b[A\n",
            "Iteration:  67% 334/500 [09:18<04:42,  1.70s/it]\u001b[A\n",
            "Iteration:  67% 335/500 [09:20<04:41,  1.71s/it]\u001b[A\n",
            "Iteration:  67% 336/500 [09:22<04:40,  1.71s/it]\u001b[A\n",
            "Iteration:  67% 337/500 [09:23<04:37,  1.70s/it]\u001b[A\n",
            "Iteration:  68% 338/500 [09:25<04:36,  1.71s/it]\u001b[A\n",
            "Iteration:  68% 339/500 [09:27<04:35,  1.71s/it]\u001b[A\n",
            "Iteration:  68% 340/500 [09:28<04:34,  1.71s/it]\u001b[A\n",
            "Iteration:  68% 341/500 [09:30<04:33,  1.72s/it]\u001b[A\n",
            "Iteration:  68% 342/500 [09:32<04:30,  1.71s/it]\u001b[A\n",
            "Iteration:  69% 343/500 [09:34<04:28,  1.71s/it]\u001b[A\n",
            "Iteration:  69% 344/500 [09:35<04:26,  1.71s/it]\u001b[A\n",
            "Iteration:  69% 345/500 [09:37<04:24,  1.71s/it]\u001b[A\n",
            "Iteration:  69% 346/500 [09:39<04:23,  1.71s/it]\u001b[A\n",
            "Iteration:  69% 347/500 [09:40<04:21,  1.71s/it]\u001b[A\n",
            "Iteration:  70% 348/500 [09:42<04:19,  1.71s/it]\u001b[A\n",
            "Iteration:  70% 349/500 [09:44<04:17,  1.71s/it]\u001b[A{\"learning_rate\": 1.5e-05, \"loss\": 0.8930046623945236, \"step\": 350}\n",
            "\n",
            "Iteration:  70% 350/500 [09:46<04:16,  1.71s/it]\u001b[A\n",
            "Iteration:  70% 351/500 [09:47<04:14,  1.71s/it]\u001b[A\n",
            "Iteration:  70% 352/500 [09:49<04:13,  1.71s/it]\u001b[A\n",
            "Iteration:  71% 353/500 [09:51<04:10,  1.71s/it]\u001b[A\n",
            "Iteration:  71% 354/500 [09:52<04:08,  1.70s/it]\u001b[A\n",
            "Iteration:  71% 355/500 [09:54<04:06,  1.70s/it]\u001b[A\n",
            "Iteration:  71% 356/500 [09:56<04:04,  1.70s/it]\u001b[A\n",
            "Iteration:  71% 357/500 [09:57<04:03,  1.70s/it]\u001b[A\n",
            "Iteration:  72% 358/500 [09:59<04:02,  1.71s/it]\u001b[A\n",
            "Iteration:  72% 359/500 [10:01<04:00,  1.70s/it]\u001b[A\n",
            "Iteration:  72% 360/500 [10:03<03:58,  1.70s/it]\u001b[A\n",
            "Iteration:  72% 361/500 [10:04<03:56,  1.70s/it]\u001b[A\n",
            "Iteration:  72% 362/500 [10:06<03:55,  1.71s/it]\u001b[A\n",
            "Iteration:  73% 363/500 [10:08<03:53,  1.71s/it]\u001b[A\n",
            "Iteration:  73% 364/500 [10:09<03:51,  1.70s/it]\u001b[A\n",
            "Iteration:  73% 365/500 [10:11<03:49,  1.70s/it]\u001b[A\n",
            "Iteration:  73% 366/500 [10:13<03:48,  1.71s/it]\u001b[A\n",
            "Iteration:  73% 367/500 [10:15<03:46,  1.71s/it]\u001b[A\n",
            "Iteration:  74% 368/500 [10:16<03:44,  1.70s/it]\u001b[A\n",
            "Iteration:  74% 369/500 [10:18<03:43,  1.71s/it]\u001b[A\n",
            "Iteration:  74% 370/500 [10:20<03:42,  1.71s/it]\u001b[A\n",
            "Iteration:  74% 371/500 [10:21<03:39,  1.70s/it]\u001b[A\n",
            "Iteration:  74% 372/500 [10:23<03:38,  1.71s/it]\u001b[A\n",
            "Iteration:  75% 373/500 [10:25<03:35,  1.70s/it]\u001b[A\n",
            "Iteration:  75% 374/500 [10:26<03:35,  1.71s/it]\u001b[A\n",
            "Iteration:  75% 375/500 [10:28<03:34,  1.71s/it]\u001b[A\n",
            "Iteration:  75% 376/500 [10:30<03:31,  1.70s/it]\u001b[A\n",
            "Iteration:  75% 377/500 [10:32<03:28,  1.70s/it]\u001b[A\n",
            "Iteration:  76% 378/500 [10:33<03:27,  1.70s/it]\u001b[A\n",
            "Iteration:  76% 379/500 [10:35<03:26,  1.70s/it]\u001b[A\n",
            "Iteration:  76% 380/500 [10:37<03:24,  1.71s/it]\u001b[A\n",
            "Iteration:  76% 381/500 [10:38<03:22,  1.70s/it]\u001b[A\n",
            "Iteration:  76% 382/500 [10:40<03:21,  1.71s/it]\u001b[A\n",
            "Iteration:  77% 383/500 [10:42<03:19,  1.71s/it]\u001b[A\n",
            "Iteration:  77% 384/500 [10:44<03:19,  1.72s/it]\u001b[A\n",
            "Iteration:  77% 385/500 [10:45<03:16,  1.71s/it]\u001b[A\n",
            "Iteration:  77% 386/500 [10:47<03:15,  1.71s/it]\u001b[A\n",
            "Iteration:  77% 387/500 [10:49<03:13,  1.71s/it]\u001b[A\n",
            "Iteration:  78% 388/500 [10:50<03:10,  1.70s/it]\u001b[A\n",
            "Iteration:  78% 389/500 [10:52<03:08,  1.70s/it]\u001b[A\n",
            "Iteration:  78% 390/500 [10:54<03:06,  1.70s/it]\u001b[A\n",
            "Iteration:  78% 391/500 [10:55<03:05,  1.70s/it]\u001b[A\n",
            "Iteration:  78% 392/500 [10:57<03:04,  1.71s/it]\u001b[A\n",
            "Iteration:  79% 393/500 [10:59<03:01,  1.70s/it]\u001b[A\n",
            "Iteration:  79% 394/500 [11:01<03:00,  1.71s/it]\u001b[A\n",
            "Iteration:  79% 395/500 [11:02<02:59,  1.71s/it]\u001b[A\n",
            "Iteration:  79% 396/500 [11:04<02:57,  1.71s/it]\u001b[A\n",
            "Iteration:  79% 397/500 [11:06<02:55,  1.71s/it]\u001b[A\n",
            "Iteration:  80% 398/500 [11:07<02:53,  1.70s/it]\u001b[A\n",
            "Iteration:  80% 399/500 [11:09<02:52,  1.71s/it]\u001b[A{\"learning_rate\": 1e-05, \"loss\": 0.9103680700063705, \"step\": 400}\n",
            "\n",
            "Iteration:  80% 400/500 [11:11<02:50,  1.70s/it]\u001b[A\n",
            "Iteration:  80% 401/500 [11:12<02:48,  1.70s/it]\u001b[A\n",
            "Iteration:  80% 402/500 [11:14<02:47,  1.70s/it]\u001b[A\n",
            "Iteration:  81% 403/500 [11:16<02:45,  1.71s/it]\u001b[A\n",
            "Iteration:  81% 404/500 [11:18<02:43,  1.70s/it]\u001b[A\n",
            "Iteration:  81% 405/500 [11:19<02:42,  1.71s/it]\u001b[A\n",
            "Iteration:  81% 406/500 [11:21<02:40,  1.71s/it]\u001b[A\n",
            "Iteration:  81% 407/500 [11:23<02:38,  1.70s/it]\u001b[A\n",
            "Iteration:  82% 408/500 [11:24<02:36,  1.71s/it]\u001b[A\n",
            "Iteration:  82% 409/500 [11:26<02:34,  1.70s/it]\u001b[A\n",
            "Iteration:  82% 410/500 [11:28<02:32,  1.70s/it]\u001b[A\n",
            "Iteration:  82% 411/500 [11:29<02:30,  1.70s/it]\u001b[A\n",
            "Iteration:  82% 412/500 [11:31<02:29,  1.70s/it]\u001b[A\n",
            "Iteration:  83% 413/500 [11:33<02:27,  1.70s/it]\u001b[A\n",
            "Iteration:  83% 414/500 [11:35<02:26,  1.70s/it]\u001b[A\n",
            "Iteration:  83% 415/500 [11:36<02:24,  1.70s/it]\u001b[A\n",
            "Iteration:  83% 416/500 [11:38<02:23,  1.71s/it]\u001b[A\n",
            "Iteration:  83% 417/500 [11:40<02:21,  1.71s/it]\u001b[A\n",
            "Iteration:  84% 418/500 [11:41<02:19,  1.71s/it]\u001b[A\n",
            "Iteration:  84% 419/500 [11:43<02:18,  1.71s/it]\u001b[A\n",
            "Iteration:  84% 420/500 [11:45<02:16,  1.71s/it]\u001b[A\n",
            "Iteration:  84% 421/500 [11:47<02:15,  1.71s/it]\u001b[A\n",
            "Iteration:  84% 422/500 [11:48<02:13,  1.71s/it]\u001b[A\n",
            "Iteration:  85% 423/500 [11:50<02:11,  1.70s/it]\u001b[A\n",
            "Iteration:  85% 424/500 [11:52<02:09,  1.70s/it]\u001b[A\n",
            "Iteration:  85% 425/500 [11:53<02:07,  1.70s/it]\u001b[A\n",
            "Iteration:  85% 426/500 [11:55<02:06,  1.71s/it]\u001b[A\n",
            "Iteration:  85% 427/500 [11:57<02:04,  1.70s/it]\u001b[A\n",
            "Iteration:  86% 428/500 [11:59<02:02,  1.71s/it]\u001b[A\n",
            "Iteration:  86% 429/500 [12:00<02:01,  1.70s/it]\u001b[A\n",
            "Iteration:  86% 430/500 [12:02<01:59,  1.70s/it]\u001b[A\n",
            "Iteration:  86% 431/500 [12:04<01:57,  1.70s/it]\u001b[A\n",
            "Iteration:  86% 432/500 [12:05<01:55,  1.70s/it]\u001b[A\n",
            "Iteration:  87% 433/500 [12:07<01:54,  1.71s/it]\u001b[A\n",
            "Iteration:  87% 434/500 [12:09<01:52,  1.71s/it]\u001b[A\n",
            "Iteration:  87% 435/500 [12:10<01:50,  1.70s/it]\u001b[A\n",
            "Iteration:  87% 436/500 [12:12<01:49,  1.71s/it]\u001b[A\n",
            "Iteration:  87% 437/500 [12:14<01:47,  1.71s/it]\u001b[A\n",
            "Iteration:  88% 438/500 [12:16<01:45,  1.70s/it]\u001b[A\n",
            "Iteration:  88% 439/500 [12:17<01:43,  1.70s/it]\u001b[A\n",
            "Iteration:  88% 440/500 [12:19<01:42,  1.70s/it]\u001b[A\n",
            "Iteration:  88% 441/500 [12:21<01:40,  1.70s/it]\u001b[A\n",
            "Iteration:  88% 442/500 [12:22<01:38,  1.70s/it]\u001b[A\n",
            "Iteration:  89% 443/500 [12:24<01:36,  1.69s/it]\u001b[A\n",
            "Iteration:  89% 444/500 [12:26<01:35,  1.70s/it]\u001b[A\n",
            "Iteration:  89% 445/500 [12:27<01:33,  1.70s/it]\u001b[A\n",
            "Iteration:  89% 446/500 [12:29<01:31,  1.70s/it]\u001b[A\n",
            "Iteration:  89% 447/500 [12:31<01:30,  1.71s/it]\u001b[A\n",
            "Iteration:  90% 448/500 [12:33<01:29,  1.71s/it]\u001b[A\n",
            "Iteration:  90% 449/500 [12:34<01:26,  1.70s/it]\u001b[A{\"learning_rate\": 5e-06, \"loss\": 0.9352099204063415, \"step\": 450}\n",
            "\n",
            "Iteration:  90% 450/500 [12:36<01:25,  1.70s/it]\u001b[A\n",
            "Iteration:  90% 451/500 [12:38<01:23,  1.71s/it]\u001b[A\n",
            "Iteration:  90% 452/500 [12:39<01:21,  1.71s/it]\u001b[A\n",
            "Iteration:  91% 453/500 [12:41<01:20,  1.71s/it]\u001b[A\n",
            "Iteration:  91% 454/500 [12:43<01:18,  1.71s/it]\u001b[A\n",
            "Iteration:  91% 455/500 [12:45<01:16,  1.70s/it]\u001b[A\n",
            "Iteration:  91% 456/500 [12:46<01:15,  1.71s/it]\u001b[A\n",
            "Iteration:  91% 457/500 [12:48<01:13,  1.71s/it]\u001b[A\n",
            "Iteration:  92% 458/500 [12:50<01:11,  1.70s/it]\u001b[A\n",
            "Iteration:  92% 459/500 [12:51<01:09,  1.70s/it]\u001b[A\n",
            "Iteration:  92% 460/500 [12:53<01:07,  1.70s/it]\u001b[A\n",
            "Iteration:  92% 461/500 [12:55<01:06,  1.70s/it]\u001b[A\n",
            "Iteration:  92% 462/500 [12:56<01:04,  1.70s/it]\u001b[A\n",
            "Iteration:  93% 463/500 [12:58<01:03,  1.70s/it]\u001b[A\n",
            "Iteration:  93% 464/500 [13:00<01:01,  1.71s/it]\u001b[A\n",
            "Iteration:  93% 465/500 [13:02<00:59,  1.71s/it]\u001b[A\n",
            "Iteration:  93% 466/500 [13:03<00:58,  1.71s/it]\u001b[A\n",
            "Iteration:  93% 467/500 [13:05<00:56,  1.71s/it]\u001b[A\n",
            "Iteration:  94% 468/500 [13:07<00:54,  1.71s/it]\u001b[A\n",
            "Iteration:  94% 469/500 [13:08<00:52,  1.71s/it]\u001b[A\n",
            "Iteration:  94% 470/500 [13:10<00:51,  1.70s/it]\u001b[A\n",
            "Iteration:  94% 471/500 [13:12<00:49,  1.70s/it]\u001b[A\n",
            "Iteration:  94% 472/500 [13:14<00:47,  1.71s/it]\u001b[A\n",
            "Iteration:  95% 473/500 [13:15<00:45,  1.70s/it]\u001b[A\n",
            "Iteration:  95% 474/500 [13:17<00:44,  1.70s/it]\u001b[A\n",
            "Iteration:  95% 475/500 [13:19<00:42,  1.70s/it]\u001b[A\n",
            "Iteration:  95% 476/500 [13:20<00:40,  1.70s/it]\u001b[A\n",
            "Iteration:  95% 477/500 [13:22<00:39,  1.70s/it]\u001b[A\n",
            "Iteration:  96% 478/500 [13:24<00:37,  1.71s/it]\u001b[A\n",
            "Iteration:  96% 479/500 [13:25<00:35,  1.71s/it]\u001b[A\n",
            "Iteration:  96% 480/500 [13:27<00:34,  1.70s/it]\u001b[A\n",
            "Iteration:  96% 481/500 [13:29<00:32,  1.70s/it]\u001b[A\n",
            "Iteration:  96% 482/500 [13:31<00:30,  1.71s/it]\u001b[A\n",
            "Iteration:  97% 483/500 [13:32<00:28,  1.70s/it]\u001b[A\n",
            "Iteration:  97% 484/500 [13:34<00:27,  1.70s/it]\u001b[A\n",
            "Iteration:  97% 485/500 [13:36<00:25,  1.70s/it]\u001b[A\n",
            "Iteration:  97% 486/500 [13:37<00:23,  1.70s/it]\u001b[A\n",
            "Iteration:  97% 487/500 [13:39<00:22,  1.70s/it]\u001b[A\n",
            "Iteration:  98% 488/500 [13:41<00:20,  1.71s/it]\u001b[A\n",
            "Iteration:  98% 489/500 [13:42<00:18,  1.70s/it]\u001b[A\n",
            "Iteration:  98% 490/500 [13:44<00:17,  1.70s/it]\u001b[A\n",
            "Iteration:  98% 491/500 [13:46<00:15,  1.70s/it]\u001b[A\n",
            "Iteration:  98% 492/500 [13:48<00:13,  1.71s/it]\u001b[A\n",
            "Iteration:  99% 493/500 [13:49<00:11,  1.71s/it]\u001b[A\n",
            "Iteration:  99% 494/500 [13:51<00:10,  1.71s/it]\u001b[A\n",
            "Iteration:  99% 495/500 [13:53<00:08,  1.71s/it]\u001b[A\n",
            "Iteration:  99% 496/500 [13:54<00:06,  1.71s/it]\u001b[A\n",
            "Iteration:  99% 497/500 [13:56<00:05,  1.70s/it]\u001b[A\n",
            "Iteration: 100% 498/500 [13:58<00:03,  1.70s/it]\u001b[A\n",
            "Iteration: 100% 499/500 [13:59<00:01,  1.70s/it]\u001b[A{\"learning_rate\": 0.0, \"loss\": 0.9121549987792968, \"step\": 500}\n",
            "\n",
            "Iteration: 100% 500/500 [14:01<00:00,  1.68s/it]\n",
            "Epoch: 100% 1/1 [14:01<00:00, 841.67s/it]\n",
            "10/15/2020 13:43:34 - INFO - __main__ -    global_step = 500, average loss = 1.0710587359666823\n",
            "10/15/2020 13:43:34 - INFO - utils_ichi -   Loading features from cached file ./data/ichi/cached_dev_bert-base-uncased_256_ichi\n",
            "loading tokenizer from cache: ./data/ichi/cachedtokenizer_bert-base-uncased_glove_ichi\n",
            "10/15/2020 13:43:36 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "10/15/2020 13:43:36 - INFO - __main__ -     Num examples = 3000\n",
            "10/15/2020 13:43:36 - INFO - __main__ -     Batch size = 16\n",
            "Evaluating: 100% 188/188 [01:53<00:00,  1.65it/s]\n",
            "10/15/2020 13:45:29 - INFO - __main__ -   ***** Eval results  *****\n",
            "10/15/2020 13:45:29 - INFO - __main__ -     acc = 0.6706666666666666\n",
            "10/15/2020 13:45:29 - INFO - __main__ -   Saving model checkpoint to ./tmp/ichi_bert_base_new\n",
            "10/15/2020 13:45:34 - INFO - transformers.tokenization_utils -   Model name './tmp/ichi_bert_base_new' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming './tmp/ichi_bert_base_new' is a path or url to a directory containing tokenizer files.\n",
            "10/15/2020 13:45:34 - INFO - transformers.tokenization_utils -   loading file ./tmp/ichi_bert_base_new/vocab.txt\n",
            "10/15/2020 13:45:34 - INFO - transformers.tokenization_utils -   loading file ./tmp/ichi_bert_base_new/added_tokens.json\n",
            "10/15/2020 13:45:34 - INFO - transformers.tokenization_utils -   loading file ./tmp/ichi_bert_base_new/special_tokens_map.json\n",
            "10/15/2020 13:45:34 - INFO - transformers.tokenization_utils -   loading file ./tmp/ichi_bert_base_new/tokenizer_config.json\n",
            "10/15/2020 13:45:34 - INFO - transformers.tokenization_utils -   Model name './tmp/ichi_bert_base_new' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming './tmp/ichi_bert_base_new' is a path or url to a directory containing tokenizer files.\n",
            "10/15/2020 13:45:34 - INFO - transformers.tokenization_utils -   loading file ./tmp/ichi_bert_base_new/vocab.txt\n",
            "10/15/2020 13:45:34 - INFO - transformers.tokenization_utils -   loading file ./tmp/ichi_bert_base_new/added_tokens.json\n",
            "10/15/2020 13:45:34 - INFO - transformers.tokenization_utils -   loading file ./tmp/ichi_bert_base_new/special_tokens_map.json\n",
            "10/15/2020 13:45:34 - INFO - transformers.tokenization_utils -   loading file ./tmp/ichi_bert_base_new/tokenizer_config.json\n",
            "loading tokenizer from cache: ./data/ichi/cachedtokenizer_bert-base-uncased_glove_ichi\n",
            "10/15/2020 13:45:34 - INFO - __main__ -   Evaluate the following checkpoints: ['./tmp/ichi_bert_base_new']\n",
            "10/15/2020 13:45:38 - INFO - utils_ichi -   Loading features from cached file ./data/ichi/cached_dev_bert-base-uncased_256_ichi\n",
            "loading tokenizer from cache: ./data/ichi/cachedtokenizer_bert-base-uncased_glove_ichi\n",
            "10/15/2020 13:45:39 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "10/15/2020 13:45:39 - INFO - __main__ -     Num examples = 3000\n",
            "10/15/2020 13:45:39 - INFO - __main__ -     Batch size = 16\n",
            "Evaluating: 100% 188/188 [01:53<00:00,  1.65it/s]\n",
            "10/15/2020 13:47:33 - INFO - __main__ -   ***** Eval results  *****\n",
            "10/15/2020 13:47:33 - INFO - __main__ -     acc = 0.6706666666666666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9QFTiYbc0nA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}