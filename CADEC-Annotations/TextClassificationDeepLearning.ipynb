{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jCV8CCMbRyhl"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv('train.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NhaLH_LnY14l"
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('dev.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 598
    },
    "colab_type": "code",
    "id": "S59Z239qzERZ",
    "outputId": "fce0a092-3b68-4b20-aed5-98f881a53894"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ids</th>\n",
       "      <th>body</th>\n",
       "      <th>Uncertainity of Post_Diagnosis</th>\n",
       "      <th>Results and Side-Effects Observed</th>\n",
       "      <th>Medical Assistance</th>\n",
       "      <th>Diet and Maintenance</th>\n",
       "      <th>Information Source</th>\n",
       "      <th>Concepts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>LIPITOR.449.txt</td>\n",
       "      <td>Extreme tiredness and flatulence. Not sure whe...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>flatulence|exhaustion|tired|Not sure|Extreme|t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>LIPITOR.188.txt</td>\n",
       "      <td>1/7/05-continued. not all of it posted before....</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>package insert|depression|Lipitor|cholesterol|...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>LIPITOR.541.txt</td>\n",
       "      <td>So sad to see so many with problems like mine!...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>muscle pain|joint pain|depression|Lipitor|Lipi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>LIPITOR.810.txt</td>\n",
       "      <td>Within 1 month time developed severe depressio...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>severe depression|headaches|Lipitor|statins|Li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>LIPITOR.393.txt</td>\n",
       "      <td>I have been on lipitor for 10 years for heart ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>leg weakness|changed|experience|Potassium|cram...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  ...                                           Concepts\n",
       "0           0  ...  flatulence|exhaustion|tired|Not sure|Extreme|t...\n",
       "1           1  ...  package insert|depression|Lipitor|cholesterol|...\n",
       "2           2  ...  muscle pain|joint pain|depression|Lipitor|Lipi...\n",
       "3           3  ...  severe depression|headaches|Lipitor|statins|Li...\n",
       "4           4  ...  leg weakness|changed|experience|Potassium|cram...\n",
       "\n",
       "[5 rows x 9 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 750
    },
    "colab_type": "code",
    "id": "6BU4tt9TzEi_",
    "outputId": "94040de1-2db9-4067-f64b-8cd493e621cf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ids</th>\n",
       "      <th>body</th>\n",
       "      <th>Uncertainity of Post_Diagnosis</th>\n",
       "      <th>Results and Side-Effects Observed</th>\n",
       "      <th>Medical Assistance</th>\n",
       "      <th>Diet and Maintenance</th>\n",
       "      <th>Information Source</th>\n",
       "      <th>Concepts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>LIPITOR.595.txt</td>\n",
       "      <td>Swelling left arm, very severe itching, intole...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Swelling|itching|left arm|hand|dreams|bruise|v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ARTHROTEC.101.txt</td>\n",
       "      <td>1st pill taken with food, a few hours after i ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>side effects|depression|cramping|upset|experie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>LIPITOR.367.txt</td>\n",
       "      <td>episode of intense dizziness lasting nearly an...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dizziness|lassitude|chills|shivers|problem|wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>LIPITOR.74.txt</td>\n",
       "      <td>After taking Crestor and having muscle pain an...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>liver problems|decreased|itchy|control|crawly|...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>LIPITOR.389.txt</td>\n",
       "      <td>75 yo mother-in-law has memory loss, hair loss...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>lack of appetite|hair loss|stroke|sciatica|los...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  ...                                           Concepts\n",
       "0           0  ...  Swelling|itching|left arm|hand|dreams|bruise|v...\n",
       "1           1  ...  side effects|depression|cramping|upset|experie...\n",
       "2           2  ...  dizziness|lassitude|chills|shivers|problem|wor...\n",
       "3           3  ...  liver problems|decreased|itchy|control|crawly|...\n",
       "4           4  ...  lack of appetite|hair loss|stroke|sciatica|los...\n",
       "\n",
       "[5 rows x 9 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E3HJfZaTY6RT"
   },
   "outputs": [],
   "source": [
    "train_body =df_train['body'].to_list()\n",
    "test_body =df_test['body'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A-3cQ-jzZFvU"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"can not \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
    "    text = re.sub('\\W', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = text.strip(' ')\n",
    "    return text\n",
    "\n",
    "for idx,text in enumerate(train_body):\n",
    "  train_body[idx] = clean_text(text)\n",
    "\n",
    "for idx,text in enumerate(test_body):\n",
    "  test_body[idx] = clean_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5u6waqooxvxS"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "str_total = ' '.join(train_body) + ' '.join(test_body)\n",
    "list_total = str_total.split()\n",
    "dict_count = Counter(list_total)\n",
    "\n",
    "vocab = dict()\n",
    "for key,val in dict_count.items():\n",
    "  vocab[key] = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7Ag50422yesk",
    "outputId": "1642c0dc-6487-4a5c-8841-ae222c5eb055"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6640\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vWs35Myuzp8_"
   },
   "outputs": [],
   "source": [
    "x_train = list()\n",
    "x_test = list()\n",
    "def encode_text(toks,type='train'):\n",
    "  encod_arr = [None]*len(toks)\n",
    "  try:\n",
    "    for idx,tok in enumerate(toks):\n",
    "      encod_arr[idx] = vocab[tok]\n",
    "  except: pass\n",
    "\n",
    "  if type =='train' : x_train.append(encod_arr)\n",
    "  if type =='test' : x_test.append(encod_arr)\n",
    "\n",
    "\n",
    "for item in train_body:\n",
    "  encode_text(item.split(), 'train')\n",
    "\n",
    "for item in test_body:\n",
    "  encode_text(item.split(), 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "9zVN2b2d1zMs",
    "outputId": "62c3db61-d4a7-4945-8ca1-132caa065c0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 9, 17, 18, 19, 8, 7, 20, 8, 7, 21, 22, 23, 24, 8, 7, 9, 10] extreme tiredness and flatulence not sure whether it is the lipitor but i am now tired to the point of exhaustion is it work is it me getting older or is it the lipitor\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0],train_body[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9744bUYH0hG3"
   },
   "source": [
    "# **FastText Deep Learning Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4fCve4k3h1bE"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing import sequence\n",
    "from keras import Input, Model\n",
    "from keras.layers import Embedding, GlobalAveragePooling1D, Dense\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "sm = SMOTE(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZCmSlT0Ih51o"
   },
   "outputs": [],
   "source": [
    "class FastText(object):\n",
    "    def __init__(self, maxlen, max_features, embedding_dims,\n",
    "                 class_num=2,\n",
    "                 last_activation='softmax'):\n",
    "        self.maxlen = maxlen\n",
    "        self.max_features = max_features\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.class_num = class_num\n",
    "        self.last_activation = last_activation\n",
    "\n",
    "    def get_model(self):\n",
    "        input = Input((self.maxlen,))\n",
    "\n",
    "        embedding = Embedding(self.max_features, self.embedding_dims, input_length=self.maxlen)(input)\n",
    "        x = GlobalAveragePooling1D()(embedding)\n",
    "\n",
    "        output = Dense(self.class_num, activation=self.last_activation)(x)\n",
    "        model = Model(inputs=input, outputs=output)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Sg-r2MGasoAQ",
    "outputId": "bd18cc88-9ed1-478f-baeb-b1dcb270854d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "942 train sequences\n",
      "300 test sequences\n",
      "Average train sequence length: 86\n",
      "Average test sequence length: 80\n",
      "Pad sequences (samples x time)...\n",
      "x_train shape: (942, 707)\n",
      "x_test shape: (300, 707)\n",
      "Build model...\n",
      "Train...\n",
      "Reporting Category:  Uncertainity of Post_Diagnosis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1550 samples, validate on 300 samples\n",
      "Epoch 1/10\n",
      "1550/1550 [==============================] - 0s 277us/step - loss: 0.2477 - acc: 0.5348 - val_loss: 0.2341 - val_acc: 0.8567\n",
      "Epoch 2/10\n",
      "1550/1550 [==============================] - 0s 132us/step - loss: 0.2415 - acc: 0.6865 - val_loss: 0.2284 - val_acc: 0.8567\n",
      "Epoch 3/10\n",
      "1550/1550 [==============================] - 0s 151us/step - loss: 0.2324 - acc: 0.7277 - val_loss: 0.2198 - val_acc: 0.8567\n",
      "Epoch 4/10\n",
      "1550/1550 [==============================] - 0s 135us/step - loss: 0.2191 - acc: 0.7723 - val_loss: 0.2120 - val_acc: 0.8567\n",
      "Test...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      1.00      0.92       257\n",
      "           1       0.00      0.00      0.00        43\n",
      "\n",
      "    accuracy                           0.86       300\n",
      "   macro avg       0.43      0.50      0.46       300\n",
      "weighted avg       0.73      0.86      0.79       300\n",
      "\n",
      "Reporting Category:  Results and Side-Effects Observed\n",
      "Train on 1866 samples, validate on 300 samples\n",
      "Epoch 1/10\n",
      "1152/1866 [=================>............] - ETA: 0s - loss: 0.2776 - acc: 0.3186"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1866/1866 [==============================] - 0s 151us/step - loss: 0.2735 - acc: 0.3451 - val_loss: 0.2615 - val_acc: 0.1967\n",
      "Epoch 2/10\n",
      "1866/1866 [==============================] - 0s 144us/step - loss: 0.2560 - acc: 0.5000 - val_loss: 0.2554 - val_acc: 0.3767\n",
      "Epoch 3/10\n",
      "1866/1866 [==============================] - 0s 137us/step - loss: 0.2452 - acc: 0.5536 - val_loss: 0.2559 - val_acc: 0.3933\n",
      "Epoch 4/10\n",
      "1866/1866 [==============================] - 0s 121us/step - loss: 0.2356 - acc: 0.6399 - val_loss: 0.2514 - val_acc: 0.4500\n",
      "Epoch 5/10\n",
      "1866/1866 [==============================] - 0s 118us/step - loss: 0.2234 - acc: 0.6811 - val_loss: 0.2416 - val_acc: 0.4900\n",
      "Epoch 6/10\n",
      "1866/1866 [==============================] - 0s 115us/step - loss: 0.2079 - acc: 0.7444 - val_loss: 0.2125 - val_acc: 0.6067\n",
      "Epoch 7/10\n",
      "1866/1866 [==============================] - 0s 138us/step - loss: 0.1887 - acc: 0.8039 - val_loss: 0.2026 - val_acc: 0.6200\n",
      "Epoch 8/10\n",
      "1866/1866 [==============================] - 0s 121us/step - loss: 0.1689 - acc: 0.8365 - val_loss: 0.1667 - val_acc: 0.7433\n",
      "Epoch 9/10\n",
      "1866/1866 [==============================] - 0s 131us/step - loss: 0.1498 - acc: 0.8623 - val_loss: 0.1422 - val_acc: 0.8033\n",
      "Epoch 10/10\n",
      "1866/1866 [==============================] - 0s 131us/step - loss: 0.1335 - acc: 0.8821 - val_loss: 0.1584 - val_acc: 0.7267\n",
      "Test...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.02      0.67      0.05         3\n",
      "           1       1.00      0.73      0.84       297\n",
      "\n",
      "    accuracy                           0.73       300\n",
      "   macro avg       0.51      0.70      0.44       300\n",
      "weighted avg       0.99      0.73      0.83       300\n",
      "\n",
      "Reporting Category:  Medical Assistance\n",
      "Train on 1484 samples, validate on 300 samples\n",
      "Epoch 1/10\n",
      " 832/1484 [===============>..............] - ETA: 0s - loss: 0.3586 - acc: 0.3185"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1484/1484 [==============================] - 0s 144us/step - loss: 0.3498 - acc: 0.3363 - val_loss: 0.2870 - val_acc: 0.5600\n",
      "Epoch 2/10\n",
      "1484/1484 [==============================] - 0s 132us/step - loss: 0.2983 - acc: 0.4569 - val_loss: 0.2647 - val_acc: 0.5867\n",
      "Epoch 3/10\n",
      "1484/1484 [==============================] - 0s 130us/step - loss: 0.2674 - acc: 0.5290 - val_loss: 0.2123 - val_acc: 0.6800\n",
      "Epoch 4/10\n",
      "1484/1484 [==============================] - 0s 168us/step - loss: 0.2469 - acc: 0.5728 - val_loss: 0.2079 - val_acc: 0.6933\n",
      "Epoch 5/10\n",
      "1484/1484 [==============================] - 0s 147us/step - loss: 0.2309 - acc: 0.6193 - val_loss: 0.1944 - val_acc: 0.7167\n",
      "Epoch 6/10\n",
      "1484/1484 [==============================] - 0s 152us/step - loss: 0.2184 - acc: 0.6449 - val_loss: 0.1936 - val_acc: 0.7267\n",
      "Epoch 7/10\n",
      "1484/1484 [==============================] - 0s 138us/step - loss: 0.2060 - acc: 0.6651 - val_loss: 0.1889 - val_acc: 0.7433\n",
      "Epoch 8/10\n",
      "1484/1484 [==============================] - 0s 160us/step - loss: 0.1952 - acc: 0.6920 - val_loss: 0.1756 - val_acc: 0.7767\n",
      "Epoch 9/10\n",
      "1484/1484 [==============================] - 0s 152us/step - loss: 0.1847 - acc: 0.7197 - val_loss: 0.1802 - val_acc: 0.7767\n",
      "Epoch 10/10\n",
      "1484/1484 [==============================] - 0s 140us/step - loss: 0.1748 - acc: 0.7547 - val_loss: 0.1660 - val_acc: 0.8133\n",
      "Test...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.92      0.89       243\n",
      "           1       0.51      0.37      0.43        57\n",
      "\n",
      "    accuracy                           0.81       300\n",
      "   macro avg       0.69      0.64      0.66       300\n",
      "weighted avg       0.79      0.81      0.80       300\n",
      "\n",
      "Reporting Category:  Diet and Maintenance\n",
      "Train on 1668 samples, validate on 300 samples\n",
      "Epoch 1/10\n",
      "1216/1668 [====================>.........] - ETA: 0s - loss: 0.1853 - acc: 0.7401"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1668/1668 [==============================] - 0s 153us/step - loss: 0.1822 - acc: 0.7428 - val_loss: 0.1697 - val_acc: 0.8267\n",
      "Epoch 2/10\n",
      "1668/1668 [==============================] - 0s 156us/step - loss: 0.1607 - acc: 0.8028 - val_loss: 0.1544 - val_acc: 0.8667\n",
      "Epoch 3/10\n",
      "1668/1668 [==============================] - 0s 158us/step - loss: 0.1427 - acc: 0.8447 - val_loss: 0.1564 - val_acc: 0.8700\n",
      "Epoch 4/10\n",
      "1668/1668 [==============================] - 0s 154us/step - loss: 0.1275 - acc: 0.8897 - val_loss: 0.1348 - val_acc: 0.8733\n",
      "Epoch 5/10\n",
      "1668/1668 [==============================] - 0s 156us/step - loss: 0.1149 - acc: 0.8981 - val_loss: 0.1367 - val_acc: 0.8767\n",
      "Epoch 6/10\n",
      "1668/1668 [==============================] - 0s 127us/step - loss: 0.1059 - acc: 0.9089 - val_loss: 0.1297 - val_acc: 0.8800\n",
      "Epoch 7/10\n",
      "1668/1668 [==============================] - 0s 162us/step - loss: 0.0980 - acc: 0.9197 - val_loss: 0.1259 - val_acc: 0.8800\n",
      "Epoch 8/10\n",
      "1668/1668 [==============================] - 0s 160us/step - loss: 0.0918 - acc: 0.9209 - val_loss: 0.1199 - val_acc: 0.8800\n",
      "Epoch 9/10\n",
      "1668/1668 [==============================] - 0s 147us/step - loss: 0.0868 - acc: 0.9233 - val_loss: 0.1200 - val_acc: 0.8800\n",
      "Test...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.94       264\n",
      "           1       0.00      0.00      0.00        36\n",
      "\n",
      "    accuracy                           0.88       300\n",
      "   macro avg       0.44      0.50      0.47       300\n",
      "weighted avg       0.77      0.88      0.82       300\n",
      "\n",
      "Reporting Category:  Information Source\n",
      "Train on 1756 samples, validate on 300 samples\n",
      "Epoch 1/10\n",
      "1408/1756 [=======================>......] - ETA: 0s - loss: 0.0754 - acc: 0.9205"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1756/1756 [==============================] - 0s 122us/step - loss: 0.0772 - acc: 0.9174 - val_loss: 0.1000 - val_acc: 0.9200\n",
      "Epoch 2/10\n",
      "1756/1756 [==============================] - 0s 118us/step - loss: 0.0729 - acc: 0.9231 - val_loss: 0.0932 - val_acc: 0.9200\n",
      "Epoch 3/10\n",
      "1756/1756 [==============================] - 0s 131us/step - loss: 0.0698 - acc: 0.9265 - val_loss: 0.0953 - val_acc: 0.9200\n",
      "Epoch 4/10\n",
      "1756/1756 [==============================] - 0s 123us/step - loss: 0.0666 - acc: 0.9311 - val_loss: 0.1006 - val_acc: 0.9200\n",
      "Test...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96       276\n",
      "           1       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.92       300\n",
      "   macro avg       0.46      0.50      0.48       300\n",
      "weighted avg       0.85      0.92      0.88       300\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "def create_ngram_set(input_list, ngram_value=2):\n",
    "    \"\"\"\n",
    "    Extract a set of n-grams from a list of integers.\n",
    "    # >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=2)\n",
    "    {(4, 9), (4, 1), (1, 4), (9, 4)}\n",
    "    # >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=3)\n",
    "    [(1, 4, 9), (4, 9, 4), (9, 4, 1), (4, 1, 4)]\n",
    "    \"\"\"\n",
    "    return set(zip(*[input_list[i:] for i in range(ngram_value)]))\n",
    "\n",
    "\n",
    "def add_ngram(sequences, token_indice, ngram_range=2):\n",
    "    \"\"\"\n",
    "    Augment the input list of list (sequences) by appending n-grams values.\n",
    "    Example: adding bi-gram\n",
    "    # >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n",
    "    # >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017}\n",
    "    # >>> add_ngram(sequences, token\n",
    " [0.04929424 0.9507057 ]_indice, ngram_range=2)\n",
    "    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42]]\n",
    "    Example: adding tri-gram\n",
    "    # >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n",
    "    # >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017, (7, 9, 2): 2018}\n",
    "    # >>> add_ngram(sequences, token_indice, ngram_range=3)\n",
    "    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42, 2018]]\n",
    "    \"\"\"\n",
    "    new_sequences = []\n",
    "    for input_list in sequences:\n",
    "        new_list = input_list[:]\n",
    "        for ngram_value in range(2, ngram_range + 1):\n",
    "            for i in range(len(new_list) - ngram_value + 1):\n",
    "                ngram = tuple(new_list[i:i + ngram_value])\n",
    "                if ngram in token_indice:\n",
    "                    new_list.append(token_indice[ngram])\n",
    "        new_sequences.append(new_list)\n",
    "\n",
    "    return new_sequences\n",
    "\n",
    "\n",
    "# Set parameters:\n",
    "# ngram_range = 2 will add bi-grams features\n",
    "ngram_range = 1\n",
    "max_features = len(vocab)\n",
    "maxlen = max([len(item) for item in (x_train+x_test)])\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "epochs = 10\n",
    "\n",
    "print('Loading data...')\n",
    "\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "print('Average train sequence length: {}'.format(np.mean(list(map(len, x_train)), dtype=int)))\n",
    "print('Average test sequence length: {}'.format(np.mean(list(map(len, x_test)), dtype=int)))\n",
    "\n",
    "if ngram_range > 1:\n",
    "    print('Adding {}-gram features'.format(ngram_range))\n",
    "    # Create set of unique n-gram from the training set.\n",
    "    ngram_set = set()\n",
    "    for input_list in x_train:\n",
    "        for i in range(2, ngram_range + 1):\n",
    "            set_of_ngram = create_ngram_set(input_list, ngram_value=i)\n",
    "            ngram_set.update(set_of_ngram)\n",
    "\n",
    "    # Dictionary mapping n-gram token to a unique integer.\n",
    "    # Integer values are greater than max_features in order\n",
    "    # to avoid collision with existing features.\n",
    "    start_index = max_features + 1\n",
    "    token_indice = {v: k + start_index for k, v in enumerate(ngram_set)}\n",
    "    indice_token = {token_indice[k]: k for k in token_indice}\n",
    "\n",
    "    # max_features is the highest integer that could be found in the dataset.\n",
    "    max_features = np.max(list(indice_token.keys())) + 1\n",
    "\n",
    "    # Augmenting x_train and x_test with n-grams features\n",
    "    x_train = add_ngram(x_train, token_indice, ngram_range)\n",
    "    x_test = add_ngram(x_test, token_indice, ngram_range)\n",
    "    print('Average train sequence length: {}'.format(np.mean(list(map(len, x_train)), dtype=int)))\n",
    "    print('Average test sequence length: {}'.format(np.mean(list(map(len, x_test)), dtype=int)))\n",
    "\n",
    "print('Pad sequences (samples x time)...')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = FastText(maxlen, max_features, embedding_dims).get_model()\n",
    "model.compile('adam', 'mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3, mode='max')\n",
    "\n",
    "categories = ['Uncertainity of Post_Diagnosis', 'Results and Side-Effects Observed', 'Medical Assistance', 'Diet and Maintenance', 'Information Source']\n",
    "\n",
    "for category in categories:\n",
    "    print('Reporting Category: ', category)\n",
    "    y_test =  np.zeros((df_test.shape[0],2),dtype=int)\n",
    "\n",
    "    y_train_label = df_train[category].to_list()\n",
    "    y_test_label = df_test[category].to_list()\n",
    "\n",
    "\n",
    "    for rowid in range(y_test.shape[0]):\n",
    "      y_test[rowid, int(y_test_label[rowid])] = 1\n",
    "\n",
    "    x_train_res, y_train_res = sm.fit_resample(x_train,y_train_label)\n",
    "\n",
    "    y_train =  np.zeros((y_train_res.shape[0],2),dtype=int)\n",
    "    for rowid in range(y_train_res.shape[0]):\n",
    "      y_train[rowid, int(y_train_res[rowid])] = 1\n",
    "\n",
    "\n",
    "    model.fit(x_train_res,y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              callbacks=[early_stopping],\n",
    "              validation_data=(x_test, y_test))\n",
    "    print('Test...')\n",
    "    result = model.predict(x_test)\n",
    "    predictions = [np.argmax(x) for x in result]\n",
    "    print(classification_report(y_test_label,predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "SS3iLd-O88OV",
    "outputId": "708a02b9-8836-4b0a-cb01-7028743a1c23"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1550"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_res.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EGcrgMUD0wiD"
   },
   "source": [
    "#**TextCNN Deep Learning Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UHFJkLza07EY"
   },
   "outputs": [],
   "source": [
    "from keras import Input, Model\n",
    "from keras.layers import Embedding, Dense, Conv1D, GlobalMaxPooling1D, Concatenate, Dropout\n",
    "\n",
    "\n",
    "class TextCNN(object):\n",
    "    def __init__(self, maxlen, max_features, embedding_dims,\n",
    "                 class_num=2,\n",
    "                 last_activation='softmax'):\n",
    "        self.maxlen = maxlen\n",
    "        self.max_features = max_features\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.class_num = class_num\n",
    "        self.last_activation = last_activation\n",
    "\n",
    "    def get_model(self):\n",
    "        input = Input((self.maxlen,))\n",
    "\n",
    "        # Embedding part can try multichannel as same as origin paper\n",
    "        embedding = Embedding(self.max_features, self.embedding_dims, input_length=self.maxlen)(input)\n",
    "        convs = []\n",
    "        for kernel_size in [3, 4, 5]:\n",
    "            c = Conv1D(128, kernel_size, activation='relu')(embedding)\n",
    "            c = GlobalMaxPooling1D()(c)\n",
    "            convs.append(c)\n",
    "        x = Concatenate()(convs)\n",
    "\n",
    "        output = Dense(self.class_num, activation=self.last_activation)(x)\n",
    "        model = Model(inputs=input, outputs=output)\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "R0EJv8Ch1mES",
    "outputId": "918001fc-57f8-4397-d556-c16a990766b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "942 train sequences\n",
      "300 test sequences\n",
      "Pad sequences (samples x time)...\n",
      "x_train shape: (942, 707)\n",
      "x_test shape: (300, 707)\n",
      "Build model...\n",
      "Train...\n",
      "Reporting Category:  Uncertainity of Post_Diagnosis\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1550 samples, validate on 300 samples\n",
      "Epoch 1/10\n",
      "1550/1550 [==============================] - 6s 4ms/step - loss: 0.6085 - acc: 0.7716 - val_loss: 0.4814 - val_acc: 0.8533\n",
      "Epoch 2/10\n",
      "1550/1550 [==============================] - 1s 332us/step - loss: 0.3210 - acc: 0.8787 - val_loss: 0.4395 - val_acc: 0.8367\n",
      "Epoch 3/10\n",
      "1550/1550 [==============================] - 1s 337us/step - loss: 0.2108 - acc: 0.9065 - val_loss: 0.4019 - val_acc: 0.8567\n",
      "Epoch 4/10\n",
      "1550/1550 [==============================] - 1s 354us/step - loss: 0.1285 - acc: 0.9561 - val_loss: 0.4575 - val_acc: 0.7733\n",
      "Epoch 5/10\n",
      "1550/1550 [==============================] - 1s 348us/step - loss: 0.0712 - acc: 0.9781 - val_loss: 0.4541 - val_acc: 0.8100\n",
      "Epoch 6/10\n",
      "1550/1550 [==============================] - 1s 342us/step - loss: 0.0316 - acc: 0.9942 - val_loss: 0.4984 - val_acc: 0.8167\n",
      "Test...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.93      0.90       257\n",
      "           1       0.25      0.14      0.18        43\n",
      "\n",
      "    accuracy                           0.82       300\n",
      "   macro avg       0.56      0.53      0.54       300\n",
      "weighted avg       0.78      0.82      0.79       300\n",
      "\n",
      "Reporting Category:  Results and Side-Effects Observed\n",
      "Train on 1866 samples, validate on 300 samples\n",
      "Epoch 1/10\n",
      " 512/1866 [=======>......................] - ETA: 0s - loss: 2.6088 - acc: 0.2949"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1866/1866 [==============================] - 1s 362us/step - loss: 0.9741 - acc: 0.6977 - val_loss: 0.2221 - val_acc: 0.9167\n",
      "Epoch 2/10\n",
      "1866/1866 [==============================] - 1s 333us/step - loss: 0.0884 - acc: 0.9743 - val_loss: 0.1183 - val_acc: 0.9633\n",
      "Epoch 3/10\n",
      "1866/1866 [==============================] - 1s 333us/step - loss: 0.0408 - acc: 0.9887 - val_loss: 0.1140 - val_acc: 0.9667\n",
      "Epoch 4/10\n",
      "1866/1866 [==============================] - 1s 331us/step - loss: 0.0256 - acc: 0.9941 - val_loss: 0.1050 - val_acc: 0.9733\n",
      "Epoch 5/10\n",
      "1866/1866 [==============================] - 1s 323us/step - loss: 0.0172 - acc: 0.9973 - val_loss: 0.1048 - val_acc: 0.9767\n",
      "Epoch 6/10\n",
      "1866/1866 [==============================] - 1s 339us/step - loss: 0.0126 - acc: 0.9984 - val_loss: 0.1017 - val_acc: 0.9767\n",
      "Epoch 7/10\n",
      "1866/1866 [==============================] - 1s 330us/step - loss: 0.0095 - acc: 0.9984 - val_loss: 0.0968 - val_acc: 0.9733\n",
      "Epoch 8/10\n",
      "1866/1866 [==============================] - 1s 331us/step - loss: 0.0073 - acc: 0.9989 - val_loss: 0.0988 - val_acc: 0.9733\n",
      "Test...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         3\n",
      "           1       0.99      0.98      0.99       297\n",
      "\n",
      "    accuracy                           0.97       300\n",
      "   macro avg       0.49      0.49      0.49       300\n",
      "weighted avg       0.98      0.97      0.98       300\n",
      "\n",
      "Reporting Category:  Medical Assistance\n",
      "Train on 1484 samples, validate on 300 samples\n",
      "Epoch 1/10\n",
      " 384/1484 [======>.......................] - ETA: 0s - loss: 3.4078 - acc: 0.1589"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1484/1484 [==============================] - 1s 352us/step - loss: 1.3619 - acc: 0.5202 - val_loss: 0.6202 - val_acc: 0.6833\n",
      "Epoch 2/10\n",
      "1484/1484 [==============================] - 0s 334us/step - loss: 0.3510 - acc: 0.8565 - val_loss: 0.4664 - val_acc: 0.7933\n",
      "Epoch 3/10\n",
      "1484/1484 [==============================] - 0s 335us/step - loss: 0.2100 - acc: 0.9178 - val_loss: 0.4366 - val_acc: 0.7967\n",
      "Epoch 4/10\n",
      "1484/1484 [==============================] - 1s 345us/step - loss: 0.1239 - acc: 0.9690 - val_loss: 0.4483 - val_acc: 0.7800\n",
      "Epoch 5/10\n",
      "1484/1484 [==============================] - 1s 346us/step - loss: 0.0694 - acc: 0.9879 - val_loss: 0.4758 - val_acc: 0.7867\n",
      "Epoch 6/10\n",
      "1484/1484 [==============================] - 0s 330us/step - loss: 0.0372 - acc: 0.9946 - val_loss: 0.4964 - val_acc: 0.8100\n",
      "Epoch 7/10\n",
      "1484/1484 [==============================] - 0s 335us/step - loss: 0.0216 - acc: 0.9987 - val_loss: 0.5328 - val_acc: 0.7967\n",
      "Epoch 8/10\n",
      "1484/1484 [==============================] - 1s 353us/step - loss: 0.0142 - acc: 0.9980 - val_loss: 0.5609 - val_acc: 0.8000\n",
      "Epoch 9/10\n",
      "1484/1484 [==============================] - 1s 338us/step - loss: 0.0100 - acc: 0.9980 - val_loss: 0.5987 - val_acc: 0.7900\n",
      "Test...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.91      0.88       243\n",
      "           1       0.42      0.28      0.34        57\n",
      "\n",
      "    accuracy                           0.79       300\n",
      "   macro avg       0.63      0.60      0.61       300\n",
      "weighted avg       0.76      0.79      0.77       300\n",
      "\n",
      "Reporting Category:  Diet and Maintenance\n",
      "Train on 1668 samples, validate on 300 samples\n",
      "Epoch 1/10\n",
      " 512/1668 [========>.....................] - ETA: 0s - loss: 0.6701 - acc: 0.8477"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1668/1668 [==============================] - 1s 374us/step - loss: 0.4496 - acc: 0.8735 - val_loss: 0.3656 - val_acc: 0.8800\n",
      "Epoch 2/10\n",
      "1668/1668 [==============================] - 1s 337us/step - loss: 0.1571 - acc: 0.9406 - val_loss: 0.3267 - val_acc: 0.8933\n",
      "Epoch 3/10\n",
      "1668/1668 [==============================] - 1s 332us/step - loss: 0.0972 - acc: 0.9658 - val_loss: 0.3244 - val_acc: 0.8933\n",
      "Epoch 4/10\n",
      "1668/1668 [==============================] - 1s 328us/step - loss: 0.0602 - acc: 0.9808 - val_loss: 0.3089 - val_acc: 0.8867\n",
      "Epoch 5/10\n",
      "1668/1668 [==============================] - 1s 331us/step - loss: 0.0326 - acc: 0.9946 - val_loss: 0.3426 - val_acc: 0.8933\n",
      "Epoch 6/10\n",
      "1668/1668 [==============================] - 1s 333us/step - loss: 0.0191 - acc: 0.9982 - val_loss: 0.3309 - val_acc: 0.8900\n",
      "Test...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.96      0.94       264\n",
      "           1       0.57      0.36      0.44        36\n",
      "\n",
      "    accuracy                           0.89       300\n",
      "   macro avg       0.74      0.66      0.69       300\n",
      "weighted avg       0.87      0.89      0.88       300\n",
      "\n",
      "Reporting Category:  Information Source\n",
      "Train on 1756 samples, validate on 300 samples\n",
      "Epoch 1/10\n",
      " 544/1756 [========>.....................] - ETA: 0s - loss: 0.3483 - acc: 0.9118"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1756/1756 [==============================] - 1s 377us/step - loss: 0.2759 - acc: 0.9174 - val_loss: 0.2848 - val_acc: 0.9200\n",
      "Epoch 2/10\n",
      "1756/1756 [==============================] - 1s 336us/step - loss: 0.0946 - acc: 0.9692 - val_loss: 0.2255 - val_acc: 0.9133\n",
      "Epoch 3/10\n",
      "1756/1756 [==============================] - 1s 332us/step - loss: 0.0561 - acc: 0.9778 - val_loss: 0.2255 - val_acc: 0.9200\n",
      "Epoch 4/10\n",
      "1756/1756 [==============================] - 1s 332us/step - loss: 0.0298 - acc: 0.9920 - val_loss: 0.2180 - val_acc: 0.9133\n",
      "Epoch 5/10\n",
      "1756/1756 [==============================] - 1s 355us/step - loss: 0.0173 - acc: 0.9983 - val_loss: 0.2546 - val_acc: 0.9167\n",
      "Epoch 6/10\n",
      "1756/1756 [==============================] - 1s 335us/step - loss: 0.0106 - acc: 1.0000 - val_loss: 0.2494 - val_acc: 0.9167\n",
      "Test...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.99      0.96       276\n",
      "           1       0.40      0.08      0.14        24\n",
      "\n",
      "    accuracy                           0.92       300\n",
      "   macro avg       0.66      0.54      0.55       300\n",
      "weighted avg       0.88      0.92      0.89       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "ngram_range = 1\n",
    "max_features = len(vocab)\n",
    "maxlen = max([len(item) for item in (x_train+x_test)])\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "epochs = 10\n",
    "\n",
    "print('Loading data...')\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)...')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = TextCNN(maxlen, max_features, embedding_dims).get_model()\n",
    "model.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3, mode='max')\n",
    "\n",
    "for category in categories:\n",
    "    print('Reporting Category: ', category)\n",
    "\n",
    "    y_train_label = df_train[category].to_list()\n",
    "    y_test_label = df_test[category].to_list()\n",
    "\n",
    "    x_train_res, y_train_res = sm.fit_resample(x_train,y_train_label)\n",
    "\n",
    "    y_train =  to_categorical(y_train_res)\n",
    "    y_test =   to_categorical(y_test_label)\n",
    "\n",
    "    model.fit(x_train_res,y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              callbacks=[early_stopping],\n",
    "              validation_data=(x_test, y_test))\n",
    "    print('Test...')\n",
    "    result = model.predict(x_test)\n",
    "    predictions = [np.argmax(x) for x in result]\n",
    "    print(classification_report(y_test_label,predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K7f7tLXV7Jug"
   },
   "source": [
    "#**RCNN**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nKjPurZw7XsS"
   },
   "outputs": [],
   "source": [
    "from keras import Input, Model\n",
    "from keras import backend as K\n",
    "from keras.layers import Embedding, Dense, SimpleRNN, Lambda, Concatenate, Conv1D, GlobalMaxPooling1D\n",
    "\n",
    "\n",
    "class RCNN(object):\n",
    "    def __init__(self, maxlen, max_features, embedding_dims,\n",
    "                 class_num=2,\n",
    "                 last_activation='softmax'):\n",
    "        self.maxlen = maxlen\n",
    "        self.max_features = max_features\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.class_num = class_num\n",
    "        self.last_activation = last_activation\n",
    "\n",
    "    def get_model(self):\n",
    "        input_current = Input((self.maxlen,))\n",
    "        input_left = Input((self.maxlen,))\n",
    "        input_right = Input((self.maxlen,))\n",
    "\n",
    "        embedder = Embedding(self.max_features, self.embedding_dims, input_length=self.maxlen)\n",
    "        embedding_current = embedder(input_current)\n",
    "        embedding_left = embedder(input_left)\n",
    "        embedding_right = embedder(input_right)\n",
    "\n",
    "        x_left = SimpleRNN(128, return_sequences=True)(embedding_left)\n",
    "        x_right = SimpleRNN(128, return_sequences=True, go_backwards=True)(embedding_right)\n",
    "        x_right = Lambda(lambda x: K.reverse(x, axes=1))(x_right)\n",
    "        x = Concatenate(axis=2)([x_left, embedding_current, x_right])\n",
    "\n",
    "        x = Conv1D(64, kernel_size=1, activation='tanh')(x)\n",
    "        x = GlobalMaxPooling1D()(x)\n",
    "\n",
    "        output = Dense(self.class_num, activation=self.last_activation)(x)\n",
    "        model = Model(inputs=[input_current, input_left, input_right], outputs=output)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "5K3SJ7bz7lxD",
    "outputId": "090e4494-a8b9-42a9-9a13-503253d5304f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "942 train sequences\n",
      "300 test sequences\n",
      "Pad sequences (samples x time)...\n",
      "x_train shape: (942, 707)\n",
      "x_test shape: (300, 707)\n",
      "Prepare input for model...\n",
      "x_train_current shape: (942, 707)\n",
      "x_train_left shape: (942, 707)\n",
      "x_train_right shape: (942, 707)\n",
      "x_test_current shape: (300, 707)\n",
      "x_test_left shape: (300, 707)\n",
      "x_test_right shape: (300, 707)\n",
      "Build model...\n",
      "Reporting Category:  Uncertainity of Post_Diagnosis\n",
      "Train on 942 samples, validate on 300 samples\n",
      "Epoch 1/10\n",
      "942/942 [==============================] - 18s 19ms/step - loss: 0.4901 - acc: 0.8227 - val_loss: 0.4192 - val_acc: 0.8567\n",
      "Epoch 2/10\n",
      "942/942 [==============================] - 17s 18ms/step - loss: 0.4678 - acc: 0.8227 - val_loss: 0.4123 - val_acc: 0.8567\n",
      "Epoch 3/10\n",
      "942/942 [==============================] - 17s 18ms/step - loss: 0.4633 - acc: 0.8227 - val_loss: 0.4086 - val_acc: 0.8567\n",
      "Epoch 4/10\n",
      "942/942 [==============================] - 17s 18ms/step - loss: 0.4598 - acc: 0.8227 - val_loss: 0.4599 - val_acc: 0.8567\n",
      "Epoch 5/10\n",
      "942/942 [==============================] - 17s 18ms/step - loss: 0.4520 - acc: 0.8227 - val_loss: 0.4079 - val_acc: 0.8567\n",
      "Epoch 6/10\n",
      "942/942 [==============================] - 17s 18ms/step - loss: 0.4513 - acc: 0.8227 - val_loss: 0.4133 - val_acc: 0.8567\n",
      "Epoch 7/10\n",
      "942/942 [==============================] - 17s 18ms/step - loss: 0.4572 - acc: 0.8227 - val_loss: 0.3909 - val_acc: 0.8567\n",
      "Epoch 8/10\n",
      "942/942 [==============================] - 17s 18ms/step - loss: 0.4494 - acc: 0.8227 - val_loss: 0.3867 - val_acc: 0.8567\n",
      "Epoch 9/10\n",
      "942/942 [==============================] - 17s 18ms/step - loss: 0.4466 - acc: 0.8227 - val_loss: 0.4112 - val_acc: 0.8567\n",
      "Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      1.00      0.92       257\n",
      "           1       0.00      0.00      0.00        43\n",
      "\n",
      "    accuracy                           0.86       300\n",
      "   macro avg       0.43      0.50      0.46       300\n",
      "weighted avg       0.73      0.86      0.79       300\n",
      "\n",
      "Reporting Category:  Results and Side-Effects Observed\n",
      "Train on 942 samples, validate on 300 samples\n",
      "Epoch 1/10\n",
      "942/942 [==============================] - 17s 18ms/step - loss: 0.2365 - acc: 0.8811 - val_loss: 0.0564 - val_acc: 0.9900\n",
      "Epoch 2/10\n",
      "942/942 [==============================] - 17s 18ms/step - loss: 0.0548 - acc: 0.9904 - val_loss: 0.0562 - val_acc: 0.9900\n",
      "Epoch 3/10\n",
      "942/942 [==============================] - 17s 18ms/step - loss: 0.0541 - acc: 0.9904 - val_loss: 0.0560 - val_acc: 0.9900\n",
      "Epoch 4/10\n",
      "942/942 [==============================] - 17s 18ms/step - loss: 0.0541 - acc: 0.9904 - val_loss: 0.0560 - val_acc: 0.9900\n",
      "Epoch 5/10\n",
      "942/942 [==============================] - 17s 18ms/step - loss: 0.0543 - acc: 0.9904 - val_loss: 0.0561 - val_acc: 0.9900\n",
      "Epoch 6/10\n",
      "942/942 [==============================] - 17s 18ms/step - loss: 0.0543 - acc: 0.9904 - val_loss: 0.0560 - val_acc: 0.9900\n",
      "Epoch 7/10\n",
      "942/942 [==============================] - 17s 18ms/step - loss: 0.0546 - acc: 0.9904 - val_loss: 0.0560 - val_acc: 0.9900\n",
      "Epoch 8/10\n",
      "942/942 [==============================] - 18s 19ms/step - loss: 0.0542 - acc: 0.9904 - val_loss: 0.0560 - val_acc: 0.9900\n",
      "Epoch 9/10\n",
      "942/942 [==============================] - 17s 18ms/step - loss: 0.0542 - acc: 0.9904 - val_loss: 0.0561 - val_acc: 0.9900\n",
      "Test...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         3\n",
      "           1       0.99      1.00      0.99       297\n",
      "\n",
      "    accuracy                           0.99       300\n",
      "   macro avg       0.49      0.50      0.50       300\n",
      "weighted avg       0.98      0.99      0.99       300\n",
      "\n",
      "Reporting Category:  Medical Assistance\n",
      "Train on 942 samples, validate on 300 samples\n",
      "Epoch 1/10\n",
      "942/942 [==============================] - 17s 18ms/step - loss: 1.3038 - acc: 0.5520 - val_loss: 0.5975 - val_acc: 0.8100\n",
      "Epoch 2/10\n",
      "942/942 [==============================] - 17s 18ms/step - loss: 0.5513 - acc: 0.7877 - val_loss: 0.5084 - val_acc: 0.8100\n",
      "Epoch 3/10\n",
      "942/942 [==============================] - 17s 18ms/step - loss: 0.5239 - acc: 0.7877 - val_loss: 0.4867 - val_acc: 0.8100\n",
      "Epoch 4/10\n",
      "942/942 [==============================] - 17s 18ms/step - loss: 0.5199 - acc: 0.7877 - val_loss: 0.4879 - val_acc: 0.8100\n",
      "Epoch 5/10\n",
      "942/942 [==============================] - 17s 18ms/step - loss: 0.5188 - acc: 0.7877 - val_loss: 0.4869 - val_acc: 0.8100\n",
      "Epoch 6/10\n",
      "942/942 [==============================] - 17s 18ms/step - loss: 0.5189 - acc: 0.7877 - val_loss: 0.4918 - val_acc: 0.8100\n",
      "Epoch 7/10\n",
      "942/942 [==============================] - 17s 18ms/step - loss: 0.5180 - acc: 0.7877 - val_loss: 0.4867 - val_acc: 0.8100\n",
      "Epoch 8/10\n",
      "942/942 [==============================] - 17s 18ms/step - loss: 0.5175 - acc: 0.7877 - val_loss: 0.4913 - val_acc: 0.8100\n",
      "Epoch 9/10\n",
      "942/942 [==============================] - 17s 18ms/step - loss: 0.5197 - acc: 0.7877 - val_loss: 0.4868 - val_acc: 0.8100\n",
      "Test...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      1.00      0.90       243\n",
      "           1       0.00      0.00      0.00        57\n",
      "\n",
      "    accuracy                           0.81       300\n",
      "   macro avg       0.41      0.50      0.45       300\n",
      "weighted avg       0.66      0.81      0.72       300\n",
      "\n",
      "Reporting Category:  Diet and Maintenance\n",
      "Train on 942 samples, validate on 300 samples\n",
      "Epoch 1/10\n",
      "942/942 [==============================] - 17s 18ms/step - loss: 0.3639 - acc: 0.8854 - val_loss: 0.3676 - val_acc: 0.8800\n",
      "Epoch 2/10\n",
      "942/942 [==============================] - 17s 18ms/step - loss: 0.3584 - acc: 0.8854 - val_loss: 0.3669 - val_acc: 0.8800\n",
      "Epoch 3/10\n",
      "942/942 [==============================] - 17s 18ms/step - loss: 0.3568 - acc: 0.8854 - val_loss: 0.3672 - val_acc: 0.8800\n",
      "Epoch 4/10\n",
      "942/942 [==============================] - 17s 18ms/step - loss: 0.3594 - acc: 0.8854 - val_loss: 0.3684 - val_acc: 0.8800\n",
      "Epoch 5/10\n",
      "942/942 [==============================] - 17s 18ms/step - loss: 0.3565 - acc: 0.8854 - val_loss: 0.3670 - val_acc: 0.8800\n",
      "Epoch 6/10\n",
      "942/942 [==============================] - 17s 18ms/step - loss: 0.3566 - acc: 0.8854 - val_loss: 0.3669 - val_acc: 0.8800\n",
      "Epoch 7/10\n",
      "942/942 [==============================] - 17s 18ms/step - loss: 0.3563 - acc: 0.8854 - val_loss: 0.3668 - val_acc: 0.8800\n",
      "Epoch 8/10\n",
      "942/942 [==============================] - 17s 18ms/step - loss: 0.3565 - acc: 0.8854 - val_loss: 0.3664 - val_acc: 0.8800\n",
      "Epoch 9/10\n",
      "942/942 [==============================] - 17s 18ms/step - loss: 0.3551 - acc: 0.8854 - val_loss: 0.3647 - val_acc: 0.8800\n",
      "Test...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.94       264\n",
      "           1       0.00      0.00      0.00        36\n",
      "\n",
      "    accuracy                           0.88       300\n",
      "   macro avg       0.44      0.50      0.47       300\n",
      "weighted avg       0.77      0.88      0.82       300\n",
      "\n",
      "Reporting Category:  Information Source\n",
      "Train on 942 samples, validate on 300 samples\n",
      "Epoch 1/10\n",
      "942/942 [==============================] - 17s 18ms/step - loss: 0.2535 - acc: 0.9321 - val_loss: 0.2779 - val_acc: 0.9200\n",
      "Epoch 2/10\n",
      "942/942 [==============================] - 17s 18ms/step - loss: 0.2472 - acc: 0.9321 - val_loss: 0.2773 - val_acc: 0.9200\n",
      "Epoch 3/10\n",
      "942/942 [==============================] - 17s 18ms/step - loss: 0.2469 - acc: 0.9321 - val_loss: 0.2771 - val_acc: 0.9200\n",
      "Epoch 4/10\n",
      "942/942 [==============================] - 17s 18ms/step - loss: 0.2465 - acc: 0.9321 - val_loss: 0.2778 - val_acc: 0.9200\n",
      "Epoch 5/10\n",
      "942/942 [==============================] - 17s 18ms/step - loss: 0.2470 - acc: 0.9321 - val_loss: 0.2762 - val_acc: 0.9200\n",
      "Epoch 6/10\n",
      "942/942 [==============================] - 17s 18ms/step - loss: 0.2466 - acc: 0.9321 - val_loss: 0.2767 - val_acc: 0.9200\n",
      "Epoch 7/10\n",
      "942/942 [==============================] - 17s 18ms/step - loss: 0.2456 - acc: 0.9321 - val_loss: 0.2794 - val_acc: 0.9200\n",
      "Epoch 8/10\n",
      "942/942 [==============================] - 17s 18ms/step - loss: 0.2462 - acc: 0.9321 - val_loss: 0.2765 - val_acc: 0.9200\n",
      "Epoch 9/10\n",
      "942/942 [==============================] - 17s 18ms/step - loss: 0.2449 - acc: 0.9321 - val_loss: 0.2772 - val_acc: 0.9200\n",
      "Test...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96       276\n",
      "           1       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.92       300\n",
      "   macro avg       0.46      0.50      0.48       300\n",
      "weighted avg       0.85      0.92      0.88       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "max_features = len(vocab)\n",
    "maxlen = max([len(x) for x in (x_train+x_test)])\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "epochs = 10\n",
    "\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)...')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Prepare input for model...')\n",
    "x_train_current = x_train\n",
    "x_train_left = np.hstack([np.expand_dims(x_train[:, 0], axis=1), x_train[:, 0:-1]])\n",
    "x_train_right = np.hstack([x_train[:, 1:], np.expand_dims(x_train[:, -1], axis=1)])\n",
    "x_test_current = x_test\n",
    "x_test_left = np.hstack([np.expand_dims(x_test[:, 0], axis=1), x_test[:, 0:-1]])\n",
    "x_test_right = np.hstack([x_test[:, 1:], np.expand_dims(x_test[:, -1], axis=1)])\n",
    "print('x_train_current shape:', x_train_current.shape)\n",
    "print('x_train_left shape:', x_train_left.shape)\n",
    "print('x_train_right shape:', x_train_right.shape)\n",
    "print('x_test_current shape:', x_test_current.shape)\n",
    "print('x_test_left shape:', x_test_left.shape)\n",
    "print('x_test_right shape:', x_test_right.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = RCNN(maxlen, max_features, embedding_dims).get_model()\n",
    "model.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "categories = ['Uncertainity of Post_Diagnosis', 'Results and Side-Effects Observed', 'Medical Assistance', 'Diet and Maintenance', 'Information Source']\n",
    "\n",
    "for category in categories:\n",
    "    print('Reporting Category: ', category)\n",
    "    y_train =  np.zeros((df_train.shape[0],2),dtype=int)\n",
    "    y_test =  np.zeros((df_test.shape[0],2),dtype=int)\n",
    "\n",
    "    y_train_label = df_train[category].to_list()\n",
    "    y_test_label = df_test[category].to_list()\n",
    "\n",
    "    for rowid in range(y_train.shape[0]):\n",
    "      y_train[rowid, int(y_train_label[rowid])] = 1\n",
    "\n",
    "    for rowid in range(y_test.shape[0]):\n",
    "      y_test[rowid, int(y_test_label[rowid])] = 1\n",
    "\n",
    "    model.fit([x_train_current, x_train_left, x_train_right], y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=([x_test_current, x_test_left, x_test_right], y_test))\n",
    "    print('Test...')\n",
    "    result = model.predict([x_test_current, x_test_left, x_test_right])\n",
    "    predictions = [np.argmax(x) for x in result]\n",
    "    print(classification_report(y_test_label,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KI3Xn0RFA7mu"
   },
   "source": [
    "#**TextAttBiRNN**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7hy8PjX8BGrR"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            # 1\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "            # next add a Dense layer (for classification/regression) or whatever...\n",
    "            # 2\n",
    "            hidden = LSTM(64, return_sequences=True)(words)\n",
    "            sentence = Attention()(hidden)\n",
    "            # next add a Dense layer (for classification/regression) or whatever...\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        e = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))  # e = K.dot(x, self.W)\n",
    "        if self.bias:\n",
    "            e += self.b\n",
    "        e = K.tanh(e)\n",
    "\n",
    "        a = K.exp(e)\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        a = K.expand_dims(a)\n",
    "\n",
    "        c = K.sum(a * x, axis=1)\n",
    "        return c\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A-_KdpgsBTM1"
   },
   "outputs": [],
   "source": [
    "from keras import Input, Model\n",
    "from keras.layers import Embedding, Dense, Dropout, Bidirectional, CuDNNLSTM\n",
    "\n",
    "class TextAttBiRNN(object):\n",
    "    def __init__(self, maxlen, max_features, embedding_dims,\n",
    "                 class_num=2,\n",
    "                 last_activation='softmax'):\n",
    "        self.maxlen = maxlen\n",
    "        self.max_features = max_features\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.class_num = class_num\n",
    "        self.last_activation = last_activation\n",
    "\n",
    "    def get_model(self):\n",
    "        input = Input((self.maxlen,))\n",
    "\n",
    "        embedding = Embedding(self.max_features, self.embedding_dims, input_length=self.maxlen)(input)\n",
    "        x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(embedding)  # LSTM or GRU\n",
    "        x = Attention(self.maxlen)(x)\n",
    "\n",
    "        output = Dense(self.class_num, activation=self.last_activation)(x)\n",
    "        model = Model(inputs=input, outputs=output)\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "hQ9DkF39BeLk",
    "outputId": "ceb0b830-161f-45f0-90dd-9f79c875c93f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "942 train sequences\n",
      "300 test sequences\n",
      "Pad sequences (samples x time)...\n",
      "x_train shape: (942, 707)\n",
      "x_test shape: (300, 707)\n",
      "Build model...\n",
      "Train...\n",
      "Reporting Category:  Uncertainity of Post_Diagnosis\n",
      "Train on 942 samples, validate on 300 samples\n",
      "Epoch 1/10\n",
      "942/942 [==============================] - 3s 3ms/step - loss: 0.4933 - acc: 0.8227 - val_loss: 0.4025 - val_acc: 0.8567\n",
      "Epoch 2/10\n",
      "942/942 [==============================] - 2s 2ms/step - loss: 0.4462 - acc: 0.8227 - val_loss: 0.4004 - val_acc: 0.8567\n",
      "Epoch 3/10\n",
      "942/942 [==============================] - 2s 2ms/step - loss: 0.4102 - acc: 0.8206 - val_loss: 0.3777 - val_acc: 0.8567\n",
      "Epoch 4/10\n",
      "942/942 [==============================] - 2s 2ms/step - loss: 0.3493 - acc: 0.8195 - val_loss: 0.4316 - val_acc: 0.7667\n",
      "Test...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.87      0.86       257\n",
      "           1       0.15      0.14      0.15        43\n",
      "\n",
      "    accuracy                           0.77       300\n",
      "   macro avg       0.51      0.51      0.51       300\n",
      "weighted avg       0.76      0.77      0.76       300\n",
      "\n",
      "Reporting Category:  Results and Side-Effects Observed\n",
      "Train on 942 samples, validate on 300 samples\n",
      "Epoch 1/10\n",
      "942/942 [==============================] - 2s 2ms/step - loss: 0.3179 - acc: 0.8556 - val_loss: 0.0932 - val_acc: 0.9900\n",
      "Epoch 2/10\n",
      "942/942 [==============================] - 2s 2ms/step - loss: 0.0731 - acc: 0.9904 - val_loss: 0.0563 - val_acc: 0.9900\n",
      "Epoch 3/10\n",
      "942/942 [==============================] - 2s 2ms/step - loss: 0.0550 - acc: 0.9904 - val_loss: 0.0583 - val_acc: 0.9900\n",
      "Epoch 4/10\n",
      "942/942 [==============================] - 2s 2ms/step - loss: 0.0566 - acc: 0.9904 - val_loss: 0.0563 - val_acc: 0.9900\n",
      "Test...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         3\n",
      "           1       0.99      1.00      0.99       297\n",
      "\n",
      "    accuracy                           0.99       300\n",
      "   macro avg       0.49      0.50      0.50       300\n",
      "weighted avg       0.98      0.99      0.99       300\n",
      "\n",
      "Reporting Category:  Medical Assistance\n",
      "Train on 942 samples, validate on 300 samples\n",
      "Epoch 1/10\n",
      " 96/942 [==>...........................] - ETA: 1s - loss: 3.2528 - acc: 0.2083"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "942/942 [==============================] - 2s 2ms/step - loss: 0.9944 - acc: 0.6582 - val_loss: 0.4858 - val_acc: 0.8100\n",
      "Epoch 2/10\n",
      "942/942 [==============================] - 2s 2ms/step - loss: 0.5179 - acc: 0.7877 - val_loss: 0.4874 - val_acc: 0.8100\n",
      "Epoch 3/10\n",
      "942/942 [==============================] - 2s 2ms/step - loss: 0.5167 - acc: 0.7877 - val_loss: 0.4899 - val_acc: 0.8100\n",
      "Epoch 4/10\n",
      "942/942 [==============================] - 2s 2ms/step - loss: 0.5194 - acc: 0.7877 - val_loss: 0.4915 - val_acc: 0.8100\n",
      "Test...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      1.00      0.90       243\n",
      "           1       0.00      0.00      0.00        57\n",
      "\n",
      "    accuracy                           0.81       300\n",
      "   macro avg       0.41      0.50      0.45       300\n",
      "weighted avg       0.66      0.81      0.72       300\n",
      "\n",
      "Reporting Category:  Diet and Maintenance\n",
      "Train on 942 samples, validate on 300 samples\n",
      "Epoch 1/10\n",
      "942/942 [==============================] - 2s 2ms/step - loss: 0.3710 - acc: 0.8854 - val_loss: 0.3681 - val_acc: 0.8800\n",
      "Epoch 2/10\n",
      "942/942 [==============================] - 2s 2ms/step - loss: 0.3560 - acc: 0.8854 - val_loss: 0.3663 - val_acc: 0.8800\n",
      "Epoch 3/10\n",
      "942/942 [==============================] - 2s 2ms/step - loss: 0.3570 - acc: 0.8854 - val_loss: 0.3671 - val_acc: 0.8800\n",
      "Epoch 4/10\n",
      "942/942 [==============================] - 2s 2ms/step - loss: 0.3548 - acc: 0.8854 - val_loss: 0.3668 - val_acc: 0.8800\n",
      "Test...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.94       264\n",
      "           1       0.00      0.00      0.00        36\n",
      "\n",
      "    accuracy                           0.88       300\n",
      "   macro avg       0.44      0.50      0.47       300\n",
      "weighted avg       0.77      0.88      0.82       300\n",
      "\n",
      "Reporting Category:  Information Source\n",
      "Train on 942 samples, validate on 300 samples\n",
      "Epoch 1/10\n",
      "942/942 [==============================] - 2s 2ms/step - loss: 0.2561 - acc: 0.9321 - val_loss: 0.2798 - val_acc: 0.9200\n",
      "Epoch 2/10\n",
      "942/942 [==============================] - 2s 2ms/step - loss: 0.2487 - acc: 0.9321 - val_loss: 0.2783 - val_acc: 0.9200\n",
      "Epoch 3/10\n",
      "942/942 [==============================] - 2s 2ms/step - loss: 0.2475 - acc: 0.9321 - val_loss: 0.2795 - val_acc: 0.9200\n",
      "Epoch 4/10\n",
      "942/942 [==============================] - 2s 2ms/step - loss: 0.2486 - acc: 0.9321 - val_loss: 0.2788 - val_acc: 0.9200\n",
      "Test...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96       276\n",
      "           1       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.92       300\n",
      "   macro avg       0.46      0.50      0.48       300\n",
      "weighted avg       0.85      0.92      0.88       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing import sequence\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "max_features = len(vocab)\n",
    "maxlen = max([len(item) for item in (x_train+x_test)])\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "epochs = 10\n",
    "\n",
    "print('Loading data...')\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)...')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = TextAttBiRNN(maxlen, max_features, embedding_dims).get_model()\n",
    "model.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3, mode='max')\n",
    "categories = ['Uncertainity of Post_Diagnosis', 'Results and Side-Effects Observed', 'Medical Assistance', 'Diet and Maintenance', 'Information Source']\n",
    "\n",
    "for category in categories:\n",
    "    print('Reporting Category: ', category)\n",
    "    y_train =  np.zeros((df_train.shape[0],2),dtype=int)\n",
    "    y_test =  np.zeros((df_test.shape[0],2),dtype=int)\n",
    "\n",
    "    y_train_label = df_train[category].to_list()\n",
    "    y_test_label = df_test[category].to_list()\n",
    "\n",
    "    for rowid in range(y_train.shape[0]):\n",
    "      y_train[rowid, int(y_train_label[rowid])] = 1\n",
    "\n",
    "    for rowid in range(y_test.shape[0]):\n",
    "      y_test[rowid, int(y_test_label[rowid])] = 1\n",
    "\n",
    "    model.fit(x_train,y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              callbacks=[early_stopping],\n",
    "              validation_data=(x_test, y_test))\n",
    "    print('Test...')\n",
    "    result = model.predict(x_test)\n",
    "    predictions = [np.argmax(x) for x in result]\n",
    "    print(classification_report(y_test_label,predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IKrPf9d9FAxY"
   },
   "source": [
    "#**TextBiRNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BrFhwWkjGK2B"
   },
   "outputs": [],
   "source": [
    "from keras import Input, Model\n",
    "from keras.layers import Embedding, Dense, Dropout, Bidirectional, CuDNNLSTM\n",
    "\n",
    "\n",
    "class TextBiRNN(object):\n",
    "    def __init__(self, maxlen, max_features, embedding_dims,\n",
    "                 class_num=2,\n",
    "                 last_activation='softmax'):\n",
    "        self.maxlen = maxlen\n",
    "        self.max_features = max_features\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.class_num = class_num\n",
    "        self.last_activation = last_activation\n",
    "\n",
    "    def get_model(self):\n",
    "        input = Input((self.maxlen,))\n",
    "\n",
    "        embedding = Embedding(self.max_features, self.embedding_dims, input_length=self.maxlen)(input)\n",
    "        x = Bidirectional(CuDNNLSTM(128))(embedding)  # LSTM or GRU\n",
    "\n",
    "        output = Dense(self.class_num, activation=self.last_activation)(x)\n",
    "        model = Model(inputs=input, outputs=output)\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "3QsCCj-0FNYl",
    "outputId": "4051d7c5-4770-45cd-c476-f8e7434548c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)...\n",
      "x_train shape: (942, 707)\n",
      "x_test shape: (300, 707)\n",
      "Build model...\n",
      "Train...\n",
      "Reporting Category:  Uncertainity of Post_Diagnosis\n",
      "Train on 942 samples, validate on 300 samples\n",
      "Epoch 1/10\n",
      "942/942 [==============================] - 3s 3ms/step - loss: 0.5177 - acc: 0.8036 - val_loss: 0.4064 - val_acc: 0.8567\n",
      "Epoch 2/10\n",
      "942/942 [==============================] - 2s 2ms/step - loss: 0.4493 - acc: 0.8227 - val_loss: 0.4043 - val_acc: 0.8567\n",
      "Epoch 3/10\n",
      "942/942 [==============================] - 2s 2ms/step - loss: 0.3796 - acc: 0.8227 - val_loss: 0.4158 - val_acc: 0.8433\n",
      "Epoch 4/10\n",
      "942/942 [==============================] - 2s 2ms/step - loss: 0.2065 - acc: 0.9098 - val_loss: 0.4674 - val_acc: 0.8267\n",
      "Test...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.95      0.90       257\n",
      "           1       0.24      0.09      0.13        43\n",
      "\n",
      "    accuracy                           0.83       300\n",
      "   macro avg       0.55      0.52      0.52       300\n",
      "weighted avg       0.77      0.83      0.79       300\n",
      "\n",
      "Reporting Category:  Results and Side-Effects Observed\n",
      "Train on 942 samples, validate on 300 samples\n",
      "Epoch 1/10\n",
      "942/942 [==============================] - 2s 2ms/step - loss: 0.6337 - acc: 0.7909 - val_loss: 0.0560 - val_acc: 0.9900\n",
      "Epoch 2/10\n",
      "942/942 [==============================] - 2s 2ms/step - loss: 0.0567 - acc: 0.9904 - val_loss: 0.0609 - val_acc: 0.9900\n",
      "Epoch 3/10\n",
      "942/942 [==============================] - 2s 2ms/step - loss: 0.0549 - acc: 0.9904 - val_loss: 0.0564 - val_acc: 0.9900\n",
      "Epoch 4/10\n",
      "942/942 [==============================] - 2s 2ms/step - loss: 0.0535 - acc: 0.9904 - val_loss: 0.0567 - val_acc: 0.9900\n",
      "Test...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         3\n",
      "           1       0.99      1.00      0.99       297\n",
      "\n",
      "    accuracy                           0.99       300\n",
      "   macro avg       0.49      0.50      0.50       300\n",
      "weighted avg       0.98      0.99      0.99       300\n",
      "\n",
      "Reporting Category:  Medical Assistance\n",
      "Train on 942 samples, validate on 300 samples\n",
      "Epoch 1/10\n",
      " 96/942 [==>...........................] - ETA: 1s - loss: 3.6309 - acc: 0.1458"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "942/942 [==============================] - 2s 2ms/step - loss: 0.9816 - acc: 0.6624 - val_loss: 0.5301 - val_acc: 0.8100\n",
      "Epoch 2/10\n",
      "942/942 [==============================] - 2s 2ms/step - loss: 0.5165 - acc: 0.7877 - val_loss: 0.4870 - val_acc: 0.8100\n",
      "Epoch 3/10\n",
      "942/942 [==============================] - 2s 2ms/step - loss: 0.5006 - acc: 0.7877 - val_loss: 0.4873 - val_acc: 0.8100\n",
      "Epoch 4/10\n",
      "942/942 [==============================] - 2s 2ms/step - loss: 0.4835 - acc: 0.7877 - val_loss: 0.4945 - val_acc: 0.8100\n",
      "Test...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      1.00      0.90       243\n",
      "           1       0.00      0.00      0.00        57\n",
      "\n",
      "    accuracy                           0.81       300\n",
      "   macro avg       0.41      0.50      0.45       300\n",
      "weighted avg       0.66      0.81      0.72       300\n",
      "\n",
      "Reporting Category:  Diet and Maintenance\n",
      "Train on 942 samples, validate on 300 samples\n",
      "Epoch 1/10\n",
      "942/942 [==============================] - 2s 2ms/step - loss: 0.3701 - acc: 0.8854 - val_loss: 0.3828 - val_acc: 0.8800\n",
      "Epoch 2/10\n",
      "942/942 [==============================] - 2s 2ms/step - loss: 0.3466 - acc: 0.8854 - val_loss: 0.3791 - val_acc: 0.8800\n",
      "Epoch 3/10\n",
      "942/942 [==============================] - 2s 2ms/step - loss: 0.3316 - acc: 0.8854 - val_loss: 0.3801 - val_acc: 0.8800\n",
      "Epoch 4/10\n",
      "942/942 [==============================] - 2s 2ms/step - loss: 0.3182 - acc: 0.8854 - val_loss: 0.3816 - val_acc: 0.8800\n",
      "Test...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.94       264\n",
      "           1       0.00      0.00      0.00        36\n",
      "\n",
      "    accuracy                           0.88       300\n",
      "   macro avg       0.44      0.50      0.47       300\n",
      "weighted avg       0.77      0.88      0.82       300\n",
      "\n",
      "Reporting Category:  Information Source\n",
      "Train on 942 samples, validate on 300 samples\n",
      "Epoch 1/10\n",
      "942/942 [==============================] - 2s 2ms/step - loss: 0.2612 - acc: 0.9321 - val_loss: 0.2801 - val_acc: 0.9200\n",
      "Epoch 2/10\n",
      "942/942 [==============================] - 2s 2ms/step - loss: 0.2433 - acc: 0.9321 - val_loss: 0.2767 - val_acc: 0.9200\n",
      "Epoch 3/10\n",
      "942/942 [==============================] - 2s 2ms/step - loss: 0.2310 - acc: 0.9321 - val_loss: 0.2798 - val_acc: 0.9200\n",
      "Epoch 4/10\n",
      "942/942 [==============================] - 2s 2ms/step - loss: 0.2206 - acc: 0.9321 - val_loss: 0.2788 - val_acc: 0.9200\n",
      "Test...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96       276\n",
      "           1       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.92       300\n",
      "   macro avg       0.46      0.50      0.48       300\n",
      "weighted avg       0.85      0.92      0.88       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "max_features = len(vocab)\n",
    "maxlen = max([len(item) for item in (x_train+x_test)])\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "epochs = 10\n",
    "\n",
    "print('Pad sequences (samples x time)...')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = TextBiRNN(maxlen, max_features, embedding_dims).get_model()\n",
    "model.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3, mode='max')\n",
    "categories = ['Uncertainity of Post_Diagnosis', 'Results and Side-Effects Observed', 'Medical Assistance', 'Diet and Maintenance', 'Information Source']\n",
    "\n",
    "for category in categories:\n",
    "    print('Reporting Category: ', category)\n",
    "\n",
    "    y_train_label = df_train[category].to_list()\n",
    "    y_test_label = df_test[category].to_list()\n",
    "\n",
    "    y_train = to_categorical(y_train_label, num_classes=None)\n",
    "    y_test = to_categorical(y_test_label, num_classes=None)\n",
    "\n",
    "    model.fit(x_train,y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              callbacks=[early_stopping],\n",
    "              validation_data=(x_test, y_test))\n",
    "    print('Test...')\n",
    "    result = model.predict(x_test)\n",
    "    predictions = [np.argmax(x) for x in result]\n",
    "    print(classification_report(y_test_label,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-oH1P13CGyrj"
   },
   "source": [
    "#**RCNNVariant**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JUciUGJkHT12"
   },
   "outputs": [],
   "source": [
    "from keras import Input, Model\n",
    "from keras.layers import Embedding, Dense, Concatenate, Conv1D, Bidirectional, CuDNNLSTM, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "\n",
    "\n",
    "class RCNNVariant(object):\n",
    "    \"\"\"Variant of RCNN.\n",
    "\n",
    "        Base on structure of RCNN, we do some improvement:\n",
    "        1. Ignore the shift for left/right context.\n",
    "        2. Use Bidirectional LSTM/GRU to encode context.\n",
    "        3. Use Multi-CNN to represent the semantic vectors.\n",
    "        4. Use ReLU instead of Tanh.\n",
    "        5. Use both AveragePooling and MaxPooling.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, maxlen, max_features, embedding_dims,\n",
    "                 class_num=2,\n",
    "                 last_activation='softmax'):\n",
    "        self.maxlen = maxlen\n",
    "        self.max_features = max_features\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.class_num = class_num\n",
    "        self.last_activation = last_activation\n",
    "\n",
    "    def get_model(self):\n",
    "        input = Input((self.maxlen,))\n",
    "\n",
    "        embedding = Embedding(self.max_features, self.embedding_dims, input_length=self.maxlen)(input)\n",
    "\n",
    "        x_context = Bidirectional(CuDNNLSTM(128, return_sequences=True))(embedding)\n",
    "        x = Concatenate()([embedding, x_context])\n",
    "\n",
    "        convs = []\n",
    "        for kernel_size in range(1, 5):\n",
    "            conv = Conv1D(128, kernel_size, activation='relu')(x)\n",
    "            convs.append(conv)\n",
    "        poolings = [GlobalAveragePooling1D()(conv) for conv in convs] + [GlobalMaxPooling1D()(conv) for conv in convs]\n",
    "        x = Concatenate()(poolings)\n",
    "\n",
    "        output = Dense(self.class_num, activation=self.last_activation)(x)\n",
    "        model = Model(inputs=input, outputs=output)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "4ixibWBsG5lf",
    "outputId": "b1a5f198-cd2e-4448-aee1-05809fb7ae2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)...\n",
      "x_train shape: (942, 707)\n",
      "x_test shape: (300, 707)\n",
      "Build model...\n",
      "Train...\n",
      "Reporting Category:  Uncertainity of Post_Diagnosis\n",
      "Train on 942 samples, validate on 300 samples\n",
      "Epoch 1/10\n",
      "942/942 [==============================] - 5s 5ms/step - loss: 0.4852 - acc: 0.8227 - val_loss: 0.4197 - val_acc: 0.8567\n",
      "Epoch 2/10\n",
      "942/942 [==============================] - 3s 3ms/step - loss: 0.4541 - acc: 0.8227 - val_loss: 0.4038 - val_acc: 0.8567\n",
      "Epoch 3/10\n",
      "942/942 [==============================] - 3s 3ms/step - loss: 0.4285 - acc: 0.8238 - val_loss: 0.3906 - val_acc: 0.8567\n",
      "Epoch 4/10\n",
      "942/942 [==============================] - 3s 3ms/step - loss: 0.3157 - acc: 0.8503 - val_loss: 0.5010 - val_acc: 0.7367\n",
      "Test...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.80      0.84       257\n",
      "           1       0.24      0.37      0.29        43\n",
      "\n",
      "    accuracy                           0.74       300\n",
      "   macro avg       0.56      0.58      0.56       300\n",
      "weighted avg       0.79      0.74      0.76       300\n",
      "\n",
      "Reporting Category:  Results and Side-Effects Observed\n",
      "Train on 942 samples, validate on 300 samples\n",
      "Epoch 1/10\n",
      "942/942 [==============================] - 3s 3ms/step - loss: 0.2657 - acc: 0.9501 - val_loss: 0.1612 - val_acc: 0.9900\n",
      "Epoch 2/10\n",
      "942/942 [==============================] - 3s 3ms/step - loss: 0.1540 - acc: 0.9904 - val_loss: 0.1612 - val_acc: 0.9900\n",
      "Epoch 3/10\n",
      "942/942 [==============================] - 3s 3ms/step - loss: 0.1540 - acc: 0.9904 - val_loss: 0.1612 - val_acc: 0.9900\n",
      "Epoch 4/10\n",
      "942/942 [==============================] - 3s 3ms/step - loss: 0.1540 - acc: 0.9904 - val_loss: 0.1612 - val_acc: 0.9900\n",
      "Test...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         3\n",
      "           1       0.99      1.00      0.99       297\n",
      "\n",
      "    accuracy                           0.99       300\n",
      "   macro avg       0.49      0.50      0.50       300\n",
      "weighted avg       0.98      0.99      0.99       300\n",
      "\n",
      "Reporting Category:  Medical Assistance\n",
      "Train on 942 samples, validate on 300 samples\n",
      "Epoch 1/10\n",
      " 64/942 [=>............................] - ETA: 2s - loss: 13.0960 - acc: 0.1875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "942/942 [==============================] - 3s 3ms/step - loss: 12.6960 - acc: 0.2123 - val_loss: 13.0557 - val_acc: 0.1900\n",
      "Epoch 2/10\n",
      "942/942 [==============================] - 3s 3ms/step - loss: 12.6960 - acc: 0.2123 - val_loss: 13.0557 - val_acc: 0.1900\n",
      "Epoch 3/10\n",
      "942/942 [==============================] - 3s 3ms/step - loss: 12.6960 - acc: 0.2123 - val_loss: 13.0557 - val_acc: 0.1900\n",
      "Epoch 4/10\n",
      "942/942 [==============================] - 3s 3ms/step - loss: 12.6960 - acc: 0.2123 - val_loss: 13.0557 - val_acc: 0.1900\n",
      "Test...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       243\n",
      "           1       0.19      1.00      0.32        57\n",
      "\n",
      "    accuracy                           0.19       300\n",
      "   macro avg       0.10      0.50      0.16       300\n",
      "weighted avg       0.04      0.19      0.06       300\n",
      "\n",
      "Reporting Category:  Diet and Maintenance\n",
      "Train on 942 samples, validate on 300 samples\n",
      "Epoch 1/10\n",
      "942/942 [==============================] - 3s 3ms/step - loss: 14.2702 - acc: 0.1146 - val_loss: 14.1839 - val_acc: 0.1200\n",
      "Epoch 2/10\n",
      "942/942 [==============================] - 3s 3ms/step - loss: 14.2702 - acc: 0.1146 - val_loss: 14.1839 - val_acc: 0.1200\n",
      "Epoch 3/10\n",
      "942/942 [==============================] - 3s 3ms/step - loss: 14.2702 - acc: 0.1146 - val_loss: 14.1839 - val_acc: 0.1200\n",
      "Epoch 4/10\n",
      "942/942 [==============================] - 3s 3ms/step - loss: 14.2702 - acc: 0.1146 - val_loss: 14.1839 - val_acc: 0.1200\n",
      "Test...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       264\n",
      "           1       0.12      1.00      0.21        36\n",
      "\n",
      "    accuracy                           0.12       300\n",
      "   macro avg       0.06      0.50      0.11       300\n",
      "weighted avg       0.01      0.12      0.03       300\n",
      "\n",
      "Reporting Category:  Information Source\n",
      "Train on 942 samples, validate on 300 samples\n",
      "Epoch 1/10\n",
      "942/942 [==============================] - 3s 3ms/step - loss: 15.0230 - acc: 0.0679 - val_loss: 14.8286 - val_acc: 0.0800\n",
      "Epoch 2/10\n",
      "942/942 [==============================] - 3s 3ms/step - loss: 15.0230 - acc: 0.0679 - val_loss: 14.8286 - val_acc: 0.0800\n",
      "Epoch 3/10\n",
      "942/942 [==============================] - 3s 3ms/step - loss: 15.0230 - acc: 0.0679 - val_loss: 14.8286 - val_acc: 0.0800\n",
      "Epoch 4/10\n",
      "942/942 [==============================] - 3s 3ms/step - loss: 15.0230 - acc: 0.0679 - val_loss: 14.8286 - val_acc: 0.0800\n",
      "Test...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       276\n",
      "           1       0.08      1.00      0.15        24\n",
      "\n",
      "    accuracy                           0.08       300\n",
      "   macro avg       0.04      0.50      0.07       300\n",
      "weighted avg       0.01      0.08      0.01       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "max_features = len(vocab)\n",
    "maxlen = max([len(item) for item in (x_train+x_test)])\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "epochs = 10\n",
    "\n",
    "print('Pad sequences (samples x time)...')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = RCNNVariant(maxlen, max_features, embedding_dims).get_model()\n",
    "model.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3, mode='max')\n",
    "\n",
    "for category in categories:\n",
    "    print('Reporting Category: ', category)\n",
    "\n",
    "    y_train_label = df_train[category].to_list()\n",
    "    y_test_label = df_test[category].to_list()\n",
    "\n",
    "    y_train = to_categorical(y_train_label, num_classes=None)\n",
    "    y_test = to_categorical(y_test_label, num_classes=None)\n",
    "\n",
    "    model.fit(x_train,y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              callbacks=[early_stopping],\n",
    "              validation_data=(x_test, y_test))\n",
    "    print('Test...')\n",
    "    result = model.predict(x_test)\n",
    "    predictions = [np.argmax(x) for x in result]\n",
    "    print(classification_report(y_test_label,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AtWiv8xyJBQw"
   },
   "source": [
    "#**TextRNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JTCp9WXdIwly"
   },
   "outputs": [],
   "source": [
    "from keras import Input, Model\n",
    "from keras.layers import Embedding, Dense, Dropout, CuDNNLSTM\n",
    "\n",
    "\n",
    "class TextRNN(object):\n",
    "    def __init__(self, maxlen, max_features, embedding_dims,\n",
    "                 class_num=2,\n",
    "                 last_activation='softmax'):\n",
    "        self.maxlen = maxlen\n",
    "        self.max_features = max_features\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.class_num = class_num\n",
    "        self.last_activation = last_activation\n",
    "\n",
    "    def get_model(self):\n",
    "        input = Input((self.maxlen,))\n",
    "\n",
    "        embedding = Embedding(self.max_features, self.embedding_dims, input_length=self.maxlen)(input)\n",
    "        x = CuDNNLSTM(128)(embedding)  # LSTM or GRU\n",
    "\n",
    "        output = Dense(self.class_num, activation=self.last_activation)(x)\n",
    "        model = Model(inputs=input, outputs=output)\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "8pOHdg6PIyxG",
    "outputId": "d4ac58ba-ef03-486f-e8a5-b2a159315404"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)...\n",
      "x_train shape: (942, 707)\n",
      "x_test shape: (300, 707)\n",
      "Build model...\n",
      "Train...\n",
      "Reporting Category:  Uncertainity of Post_Diagnosis\n",
      "Train on 942 samples, validate on 300 samples\n",
      "Epoch 1/10\n",
      "942/942 [==============================] - 3s 3ms/step - loss: 0.5269 - acc: 0.8142 - val_loss: 0.3989 - val_acc: 0.8567\n",
      "Epoch 2/10\n",
      "942/942 [==============================] - 1s 1ms/step - loss: 0.4392 - acc: 0.8227 - val_loss: 0.3844 - val_acc: 0.8567\n",
      "Epoch 3/10\n",
      "942/942 [==============================] - 1s 1ms/step - loss: 0.3525 - acc: 0.8450 - val_loss: 0.3772 - val_acc: 0.8567\n",
      "Epoch 4/10\n",
      "942/942 [==============================] - 1s 1ms/step - loss: 0.2188 - acc: 0.9289 - val_loss: 0.3994 - val_acc: 0.8500\n",
      "Test...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.95      0.92       257\n",
      "           1       0.46      0.28      0.35        43\n",
      "\n",
      "    accuracy                           0.85       300\n",
      "   macro avg       0.67      0.61      0.63       300\n",
      "weighted avg       0.83      0.85      0.83       300\n",
      "\n",
      "Reporting Category:  Results and Side-Effects Observed\n",
      "Train on 942 samples, validate on 300 samples\n",
      "Epoch 1/10\n",
      "942/942 [==============================] - 1s 1ms/step - loss: 0.5894 - acc: 0.7654 - val_loss: 0.0585 - val_acc: 0.9900\n",
      "Epoch 2/10\n",
      "942/942 [==============================] - 1s 1ms/step - loss: 0.0547 - acc: 0.9904 - val_loss: 0.0565 - val_acc: 0.9900\n",
      "Epoch 3/10\n",
      "942/942 [==============================] - 1s 1ms/step - loss: 0.0539 - acc: 0.9904 - val_loss: 0.0574 - val_acc: 0.9900\n",
      "Epoch 4/10\n",
      "942/942 [==============================] - 1s 1ms/step - loss: 0.0529 - acc: 0.9904 - val_loss: 0.0617 - val_acc: 0.9900\n",
      "Test...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         3\n",
      "           1       0.99      1.00      0.99       297\n",
      "\n",
      "    accuracy                           0.99       300\n",
      "   macro avg       0.49      0.50      0.50       300\n",
      "weighted avg       0.98      0.99      0.99       300\n",
      "\n",
      "Reporting Category:  Medical Assistance\n",
      "Train on 942 samples, validate on 300 samples\n",
      "Epoch 1/10\n",
      "160/942 [====>.........................] - ETA: 0s - loss: 3.1842 - acc: 0.2250"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "942/942 [==============================] - 1s 1ms/step - loss: 1.0406 - acc: 0.6550 - val_loss: 0.4922 - val_acc: 0.8100\n",
      "Epoch 2/10\n",
      "942/942 [==============================] - 1s 1ms/step - loss: 0.5147 - acc: 0.7877 - val_loss: 0.4863 - val_acc: 0.8100\n",
      "Epoch 3/10\n",
      "942/942 [==============================] - 1s 1ms/step - loss: 0.5074 - acc: 0.7877 - val_loss: 0.4874 - val_acc: 0.8100\n",
      "Epoch 4/10\n",
      "942/942 [==============================] - 1s 1ms/step - loss: 0.4945 - acc: 0.7877 - val_loss: 0.4869 - val_acc: 0.8100\n",
      "Test...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      1.00      0.90       243\n",
      "           1       0.00      0.00      0.00        57\n",
      "\n",
      "    accuracy                           0.81       300\n",
      "   macro avg       0.41      0.50      0.45       300\n",
      "weighted avg       0.66      0.81      0.72       300\n",
      "\n",
      "Reporting Category:  Diet and Maintenance\n",
      "Train on 942 samples, validate on 300 samples\n",
      "Epoch 1/10\n",
      "942/942 [==============================] - 1s 1ms/step - loss: 0.3641 - acc: 0.8854 - val_loss: 0.3783 - val_acc: 0.8800\n",
      "Epoch 2/10\n",
      "942/942 [==============================] - 1s 1ms/step - loss: 0.3510 - acc: 0.8854 - val_loss: 0.3759 - val_acc: 0.8800\n",
      "Epoch 3/10\n",
      "942/942 [==============================] - 1s 1ms/step - loss: 0.3410 - acc: 0.8854 - val_loss: 0.3766 - val_acc: 0.8800\n",
      "Epoch 4/10\n",
      "942/942 [==============================] - 1s 1ms/step - loss: 0.3277 - acc: 0.8854 - val_loss: 0.3799 - val_acc: 0.8800\n",
      "Test...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.94       264\n",
      "           1       0.00      0.00      0.00        36\n",
      "\n",
      "    accuracy                           0.88       300\n",
      "   macro avg       0.44      0.50      0.47       300\n",
      "weighted avg       0.77      0.88      0.82       300\n",
      "\n",
      "Reporting Category:  Information Source\n",
      "Train on 942 samples, validate on 300 samples\n",
      "Epoch 1/10\n",
      "942/942 [==============================] - 1s 1ms/step - loss: 0.2519 - acc: 0.9321 - val_loss: 0.2864 - val_acc: 0.9200\n",
      "Epoch 2/10\n",
      "942/942 [==============================] - 1s 1ms/step - loss: 0.2445 - acc: 0.9321 - val_loss: 0.2798 - val_acc: 0.9200\n",
      "Epoch 3/10\n",
      "942/942 [==============================] - 1s 1ms/step - loss: 0.2328 - acc: 0.9321 - val_loss: 0.2798 - val_acc: 0.9200\n",
      "Epoch 4/10\n",
      "942/942 [==============================] - 1s 1ms/step - loss: 0.2207 - acc: 0.9321 - val_loss: 0.2809 - val_acc: 0.9200\n",
      "Test...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96       276\n",
      "           1       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.92       300\n",
      "   macro avg       0.46      0.50      0.48       300\n",
      "weighted avg       0.85      0.92      0.88       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "max_features = len(vocab)\n",
    "maxlen = max([len(item) for item in (x_train+x_test)])\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "epochs = 10\n",
    "\n",
    "print('Pad sequences (samples x time)...')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = TextRNN(maxlen, max_features, embedding_dims).get_model()\n",
    "model.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3, mode='max')\n",
    "\n",
    "for category in categories:\n",
    "    print('Reporting Category: ', category)\n",
    "\n",
    "    y_train_label = df_train[category].to_list()\n",
    "    y_test_label = df_test[category].to_list()\n",
    "\n",
    "    y_train = to_categorical(y_train_label, num_classes=None)\n",
    "    y_test = to_categorical(y_test_label, num_classes=None)\n",
    "\n",
    "    model.fit(x_train,y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              callbacks=[early_stopping],\n",
    "              validation_data=(x_test, y_test))\n",
    "    print('Test...')\n",
    "    result = model.predict(x_test)\n",
    "    predictions = [np.argmax(x) for x in result]\n",
    "    print(classification_report(y_test_label,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QYW-UiDBJ95o"
   },
   "source": [
    "#**HAN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "My5lrbrhKrSU"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            # 1\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "            # next add a Dense layer (for classification/regression) or whatever...\n",
    "            # 2\n",
    "            hidden = LSTM(64, return_sequences=True)(words)\n",
    "            sentence = Attention()(hidden)\n",
    "            # next add a Dense layer (for classification/regression) or whatever...\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        e = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))  # e = K.dot(x, self.W)\n",
    "        if self.bias:\n",
    "            e += self.b\n",
    "        e = K.tanh(e)\n",
    "\n",
    "        a = K.exp(e)\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        a = K.expand_dims(a)\n",
    "\n",
    "        c = K.sum(a * x, axis=1)\n",
    "        return c\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], self.features_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hJD8vxN2Krp_"
   },
   "outputs": [],
   "source": [
    "from keras import Input, Model\n",
    "from keras.layers import Embedding, Dense, Bidirectional, CuDNNLSTM, TimeDistributed\n",
    "\n",
    "class HAN(object):\n",
    "    def __init__(self, maxlen_sentence, maxlen_word, max_features, embedding_dims,\n",
    "                 class_num=2,\n",
    "                 last_activation='softmax'):\n",
    "        self.maxlen_sentence = maxlen_sentence\n",
    "        self.maxlen_word = maxlen_word\n",
    "        self.max_features = max_features\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.class_num = class_num\n",
    "        self.last_activation = last_activation\n",
    "\n",
    "    def get_model(self):\n",
    "        # Word part\n",
    "        input_word = Input(shape=(self.maxlen_word,))\n",
    "        x_word = Embedding(self.max_features, self.embedding_dims, input_length=self.maxlen_word)(input_word)\n",
    "        x_word = Bidirectional(CuDNNLSTM(128, return_sequences=True))(x_word)  # LSTM or GRU\n",
    "        x_word = Attention(self.maxlen_word)(x_word)\n",
    "        model_word = Model(input_word, x_word)\n",
    "\n",
    "        # Sentence part\n",
    "        input = Input(shape=(self.maxlen_sentence, self.maxlen_word))\n",
    "        x_sentence = TimeDistributed(model_word)(input)\n",
    "        x_sentence = Bidirectional(CuDNNLSTM(128, return_sequences=True))(x_sentence)  # LSTM or GRU\n",
    "        x_sentence = Attention(self.maxlen_sentence)(x_sentence)\n",
    "\n",
    "        output = Dense(self.class_num, activation=self.last_activation)(x_sentence)\n",
    "        model = Model(inputs=input, outputs=output)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "4HQ6GwkqJkiT",
    "outputId": "f002cc89-4bfc-4ddd-e48a-65366d64bc39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)...\n",
      "x_train shape: (942, 16, 25)\n",
      "x_test shape: (300, 16, 25)\n",
      "Build model...\n",
      "Train...\n",
      "Reporting Category:  Uncertainity of Post_Diagnosis\n",
      "Train on 942 samples, validate on 300 samples\n",
      "Epoch 1/10\n",
      "942/942 [==============================] - 3s 4ms/step - loss: 0.4840 - acc: 0.8068 - val_loss: 0.4001 - val_acc: 0.8567\n",
      "Epoch 2/10\n",
      "942/942 [==============================] - 1s 600us/step - loss: 0.4365 - acc: 0.8217 - val_loss: 0.4043 - val_acc: 0.8567\n",
      "Epoch 3/10\n",
      "942/942 [==============================] - 1s 599us/step - loss: 0.3900 - acc: 0.8503 - val_loss: 0.3714 - val_acc: 0.8567\n",
      "Epoch 4/10\n",
      "942/942 [==============================] - 1s 621us/step - loss: 0.2751 - acc: 0.8896 - val_loss: 0.6250 - val_acc: 0.8600\n",
      "Epoch 5/10\n",
      "942/942 [==============================] - 1s 600us/step - loss: 0.1427 - acc: 0.9490 - val_loss: 0.5545 - val_acc: 0.7700\n",
      "Epoch 6/10\n",
      "942/942 [==============================] - 1s 645us/step - loss: 0.0607 - acc: 0.9841 - val_loss: 0.5843 - val_acc: 0.8333\n",
      "Epoch 7/10\n",
      "942/942 [==============================] - 1s 640us/step - loss: 0.0329 - acc: 0.9926 - val_loss: 0.6334 - val_acc: 0.8333\n",
      "Test...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.93      0.91       257\n",
      "           1       0.37      0.23      0.29        43\n",
      "\n",
      "    accuracy                           0.83       300\n",
      "   macro avg       0.62      0.58      0.60       300\n",
      "weighted avg       0.81      0.83      0.82       300\n",
      "\n",
      "Reporting Category:  Results and Side-Effects Observed\n",
      "Train on 942 samples, validate on 300 samples\n",
      "Epoch 1/10\n",
      "942/942 [==============================] - 1s 599us/step - loss: 0.5360 - acc: 0.8875 - val_loss: 0.0576 - val_acc: 0.9900\n",
      "Epoch 2/10\n",
      "942/942 [==============================] - 1s 601us/step - loss: 0.0570 - acc: 0.9904 - val_loss: 0.0571 - val_acc: 0.9900\n",
      "Epoch 3/10\n",
      "942/942 [==============================] - 1s 620us/step - loss: 0.0554 - acc: 0.9904 - val_loss: 0.0584 - val_acc: 0.9900\n",
      "Epoch 4/10\n",
      "942/942 [==============================] - 1s 611us/step - loss: 0.0556 - acc: 0.9904 - val_loss: 0.0567 - val_acc: 0.9900\n",
      "Test...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         3\n",
      "           1       0.99      1.00      0.99       297\n",
      "\n",
      "    accuracy                           0.99       300\n",
      "   macro avg       0.49      0.50      0.50       300\n",
      "weighted avg       0.98      0.99      0.99       300\n",
      "\n",
      "Reporting Category:  Medical Assistance\n",
      "Train on 942 samples, validate on 300 samples\n",
      "Epoch 1/10\n",
      "320/942 [=========>....................] - ETA: 0s - loss: 1.4605 - acc: 0.5563"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "942/942 [==============================] - 1s 638us/step - loss: 0.8324 - acc: 0.7113 - val_loss: 0.5111 - val_acc: 0.8100\n",
      "Epoch 2/10\n",
      "942/942 [==============================] - 1s 656us/step - loss: 0.5090 - acc: 0.7877 - val_loss: 0.4681 - val_acc: 0.8100\n",
      "Epoch 3/10\n",
      "942/942 [==============================] - 1s 627us/step - loss: 0.4902 - acc: 0.7887 - val_loss: 0.4563 - val_acc: 0.8100\n",
      "Epoch 4/10\n",
      "942/942 [==============================] - 1s 625us/step - loss: 0.4719 - acc: 0.7909 - val_loss: 0.4570 - val_acc: 0.8267\n",
      "Epoch 5/10\n",
      "942/942 [==============================] - 1s 591us/step - loss: 0.4320 - acc: 0.8132 - val_loss: 0.4287 - val_acc: 0.8200\n",
      "Epoch 6/10\n",
      "942/942 [==============================] - 1s 616us/step - loss: 0.3451 - acc: 0.8705 - val_loss: 0.4430 - val_acc: 0.8167\n",
      "Epoch 7/10\n",
      "942/942 [==============================] - 1s 626us/step - loss: 0.2258 - acc: 0.9268 - val_loss: 0.4783 - val_acc: 0.8167\n",
      "Test...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.96      0.89       243\n",
      "           1       0.55      0.21      0.30        57\n",
      "\n",
      "    accuracy                           0.82       300\n",
      "   macro avg       0.69      0.58      0.60       300\n",
      "weighted avg       0.78      0.82      0.78       300\n",
      "\n",
      "Reporting Category:  Diet and Maintenance\n",
      "Train on 942 samples, validate on 300 samples\n",
      "Epoch 1/10\n",
      "942/942 [==============================] - 1s 632us/step - loss: 0.3842 - acc: 0.8726 - val_loss: 0.3603 - val_acc: 0.8800\n",
      "Epoch 2/10\n",
      "942/942 [==============================] - 1s 612us/step - loss: 0.3426 - acc: 0.8854 - val_loss: 0.3630 - val_acc: 0.8800\n",
      "Epoch 3/10\n",
      "942/942 [==============================] - 1s 607us/step - loss: 0.3327 - acc: 0.8854 - val_loss: 0.3555 - val_acc: 0.8800\n",
      "Epoch 4/10\n",
      "942/942 [==============================] - 1s 634us/step - loss: 0.3045 - acc: 0.8907 - val_loss: 0.3496 - val_acc: 0.8833\n",
      "Epoch 5/10\n",
      "942/942 [==============================] - 1s 635us/step - loss: 0.2578 - acc: 0.9023 - val_loss: 0.3283 - val_acc: 0.8900\n",
      "Epoch 6/10\n",
      "942/942 [==============================] - 1s 599us/step - loss: 0.1843 - acc: 0.9299 - val_loss: 0.3703 - val_acc: 0.8933\n",
      "Epoch 7/10\n",
      "942/942 [==============================] - 1s 639us/step - loss: 0.1087 - acc: 0.9618 - val_loss: 0.4012 - val_acc: 0.8933\n",
      "Epoch 8/10\n",
      "942/942 [==============================] - 1s 629us/step - loss: 0.0611 - acc: 0.9873 - val_loss: 0.4328 - val_acc: 0.9000\n",
      "Epoch 9/10\n",
      "942/942 [==============================] - 1s 600us/step - loss: 0.0584 - acc: 0.9841 - val_loss: 0.4526 - val_acc: 0.8933\n",
      "Epoch 10/10\n",
      "942/942 [==============================] - 1s 628us/step - loss: 0.0393 - acc: 0.9926 - val_loss: 0.4914 - val_acc: 0.8800\n",
      "Test...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.97      0.93       264\n",
      "           1       0.50      0.19      0.28        36\n",
      "\n",
      "    accuracy                           0.88       300\n",
      "   macro avg       0.70      0.58      0.61       300\n",
      "weighted avg       0.85      0.88      0.86       300\n",
      "\n",
      "Reporting Category:  Information Source\n",
      "Train on 942 samples, validate on 300 samples\n",
      "Epoch 1/10\n",
      "942/942 [==============================] - 1s 623us/step - loss: 0.4215 - acc: 0.8938 - val_loss: 0.2818 - val_acc: 0.9200\n",
      "Epoch 2/10\n",
      "942/942 [==============================] - 1s 633us/step - loss: 0.2501 - acc: 0.9331 - val_loss: 0.2750 - val_acc: 0.9200\n",
      "Epoch 3/10\n",
      "942/942 [==============================] - 1s 658us/step - loss: 0.2356 - acc: 0.9321 - val_loss: 0.2722 - val_acc: 0.9200\n",
      "Epoch 4/10\n",
      "942/942 [==============================] - 1s 650us/step - loss: 0.2306 - acc: 0.9342 - val_loss: 0.2704 - val_acc: 0.9200\n",
      "Test...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96       276\n",
      "           1       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.92       300\n",
      "   macro avg       0.46      0.50      0.48       300\n",
      "weighted avg       0.85      0.92      0.88       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "max_features = len(vocab)\n",
    "maxlen_sentence = 16\n",
    "maxlen_word = 25\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "epochs = 10\n",
    "\n",
    "print('Pad sequences (samples x time)...')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen_sentence * maxlen_word)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen_sentence * maxlen_word)\n",
    "x_train = x_train.reshape((len(x_train), maxlen_sentence, maxlen_word))\n",
    "x_test = x_test.reshape((len(x_test), maxlen_sentence, maxlen_word))\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "\n",
    "print('Build model...')\n",
    "model = HAN(maxlen_sentence, maxlen_word, max_features, embedding_dims).get_model()\n",
    "model.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3, mode='max')\n",
    "\n",
    "for category in categories:\n",
    "    print('Reporting Category: ', category)\n",
    "\n",
    "    y_train_label = df_train[category].to_list()\n",
    "    y_test_label = df_test[category].to_list()\n",
    "\n",
    "    y_train = to_categorical(y_train_label, num_classes=None)\n",
    "    y_test = to_categorical(y_test_label, num_classes=None)\n",
    "\n",
    "    model.fit(x_train,y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              callbacks=[early_stopping],\n",
    "              validation_data=(x_test, y_test))\n",
    "    print('Test...')\n",
    "    result = model.predict(x_test)\n",
    "    predictions = [np.argmax(x) for x in result]\n",
    "    print(classification_report(y_test_label,predictions))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "9744bUYH0hG3",
    "EGcrgMUD0wiD",
    "K7f7tLXV7Jug",
    "KI3Xn0RFA7mu",
    "IKrPf9d9FAxY",
    "-oH1P13CGyrj",
    "AtWiv8xyJBQw"
   ],
   "name": "TextClassificationDeepLearning",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
